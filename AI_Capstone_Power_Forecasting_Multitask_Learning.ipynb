{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "tiny_data = read_csv(\"1.csv\", usecols=[3, 4, 5, 6, 7])\n",
    "tiny_data = DataFrame(tiny_data.as_matrix(), columns=['high', 'low', 'average', 'humidity', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert target value to a ratio: power_today / power_yesterday to remove the time dependency between today's power consumption and yesterday's consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>humidity</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>75.000</td>\n",
       "      <td>901.094080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.2</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>1.7625</td>\n",
       "      <td>77.250</td>\n",
       "      <td>744.658412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.2375</td>\n",
       "      <td>72.750</td>\n",
       "      <td>743.560438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0375</td>\n",
       "      <td>65.875</td>\n",
       "      <td>784.915894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.9</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>1.8625</td>\n",
       "      <td>55.375</td>\n",
       "      <td>685.154914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   high  low  average  humidity      target\n",
       "0   1.9 -0.4   0.7875    75.000  901.094080\n",
       "1   6.2 -3.9   1.7625    77.250  744.658412\n",
       "2   7.8  2.0   4.2375    72.750  743.560438\n",
       "3   8.5 -1.2   3.0375    65.875  784.915894\n",
       "4   7.9 -3.6   1.8625    55.375  685.154914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the first target ratio to be 1, the scond value is day2 / day 1, util the last value\n",
    "def ratio(dataset):\n",
    "    target = dataset[['target']].as_matrix()\n",
    "    ratio = list()\n",
    "    ratio.append(1)\n",
    "    for i in range(1, len(target)):\n",
    "        value = target[i] / target[i - 1]\n",
    "        ratio.append(value[0])\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.826394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.055618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.872902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  1.000000\n",
       "1  0.826394\n",
       "2  0.998526\n",
       "3  1.055618\n",
       "4  0.872902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = DataFrame(ratio(tiny_data))\n",
    "target_np = ratio(tiny_data)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>humidity</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>75.000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.2</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>1.7625</td>\n",
       "      <td>77.250</td>\n",
       "      <td>0.826394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.2375</td>\n",
       "      <td>72.750</td>\n",
       "      <td>0.998526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0375</td>\n",
       "      <td>65.875</td>\n",
       "      <td>1.055618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.9</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>1.8625</td>\n",
       "      <td>55.375</td>\n",
       "      <td>0.872902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   high  low  average  humidity    target\n",
       "0   1.9 -0.4   0.7875    75.000  1.000000\n",
       "1   6.2 -3.9   1.7625    77.250  0.826394\n",
       "2   7.8  2.0   4.2375    72.750  0.998526\n",
       "3   8.5 -1.2   3.0375    65.875  1.055618\n",
       "4   7.9 -3.6   1.8625    55.375  0.872902"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratio_d = tiny_data[['high', 'low', 'average', 'humidity']] \n",
    "ratio_df = pd.concat([ratio_d, target], axis=1)\n",
    "ratio_df.columns = ['high', 'low', 'average', 'humidity', 'target']\n",
    "ratio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>humidity</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.974740</td>\n",
       "      <td>12.712976</td>\n",
       "      <td>16.707818</td>\n",
       "      <td>76.481134</td>\n",
       "      <td>1.004465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.218013</td>\n",
       "      <td>9.316424</td>\n",
       "      <td>9.087372</td>\n",
       "      <td>12.002461</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.900000</td>\n",
       "      <td>-7.800000</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>0.536749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.025000</td>\n",
       "      <td>4.925000</td>\n",
       "      <td>9.059375</td>\n",
       "      <td>69.156250</td>\n",
       "      <td>0.953329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.950000</td>\n",
       "      <td>13.950000</td>\n",
       "      <td>18.668750</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>1.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.200000</td>\n",
       "      <td>20.575000</td>\n",
       "      <td>23.956250</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1.057694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>38.100000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>32.575000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.454849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             high         low     average    humidity      target\n",
       "count  578.000000  578.000000  578.000000  578.000000  578.000000\n",
       "mean    20.974740   12.712976   16.707818   76.481134    1.004465\n",
       "std      9.218013    9.316424    9.087372   12.002461    0.106000\n",
       "min      0.000000  -10.900000   -7.800000   39.125000    0.536749\n",
       "25%     13.025000    4.925000    9.059375   69.156250    0.953329\n",
       "50%     22.950000   13.950000   18.668750   78.250000    1.000166\n",
       "75%     28.200000   20.575000   23.956250   85.000000    1.057694\n",
       "max     38.100000   28.600000   32.575000   99.000000    1.454849"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the statistics information\n",
    "ratio_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Next step is to handle temperature and humidity features\n",
    "\n",
    "具体来说，对每个feature做成一度的categoritcal，然后one－hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.9   ,  -0.4   ,   0.7875],\n",
       "       [  6.2   ,  -3.9   ,   1.7625],\n",
       "       [  7.8   ,   2.    ,   4.2375],\n",
       "       ..., \n",
       "       [ 29.9   ,  18.1   ,  23.7125],\n",
       "       [ 29.3   ,  16.9   ,  23.325 ],\n",
       "       [ 30.4   ,  18.6   ,  24.525 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ratio = ratio_df\n",
    "np_array = new_ratio[['high', 'low', 'average']].as_matrix()\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_num1 = []\n",
    "np_num2 = []\n",
    "np_num3 = []\n",
    "\n",
    "for num in np_array:\n",
    "    np_num1.append(num[0])\n",
    "    np_num2.append(num[1])\n",
    "    np_num3.append(num[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert high into categorical data\n",
    "\n",
    "# Temp:\n",
    "\"\"\"\n",
    "-11 ~ -10: 'h0'\n",
    "-10 ~ -9: 'h1'\n",
    "-9 ~ -8: 'h2'\n",
    "-8 ~ -7: 'h3'\n",
    "-7 ~ -6: 'h4'\n",
    "-6 ~ -5: 'h5'\n",
    "-5 ~ -4: 'h6'\n",
    "-4 ~ -3: 'h7'\n",
    "-3 ~ -2: 'h8'\n",
    "-2 ~ -1: 'h9'\n",
    "-1 ~ 0: 'h10'\n",
    "    \n",
    "0 - 1: 'h11'\n",
    "1 - 2: 'h12'\n",
    "2 - 3: 'h13'\n",
    "3 - 4: 'h14'\n",
    "5 - 6: 'h15'\n",
    "6 - 7: 'h16'\n",
    "7 - 8: 'h17'\n",
    "8 - 9: 'h18'\n",
    "9 - 10: 'h19'\n",
    "    \n",
    "10 - 11: 'h20'\n",
    "11 - 12: 'h21'\n",
    "12 - 13: 'h22'\n",
    "13 - 14: 'h23'\n",
    "15 - 16: 'h24'\n",
    "16 - 17: 'h25'\n",
    "17 - 18: 'h26'\n",
    "18 - 19: 'h27'\n",
    "19 - 20: 'h28'\n",
    "    \n",
    "20 - 21: 'h29'\n",
    "21 - 22: 'h30'\n",
    "22 - 23: 'h31'\n",
    "23 - 24: 'h32'\n",
    "25 - 26: 'h33'\n",
    "26 - 27: 'h34'\n",
    "27 - 28: 'h35'\n",
    "28 - 29: 'h36'\n",
    "29 - 30: 'h37'\n",
    "    \n",
    "30 - 31: 'h38'\n",
    "31 - 32: 'h39'\n",
    "32 - 33: 'h40'\n",
    "33 - 34: 'h41'\n",
    "35 - 36: 'h42'\n",
    "36 - 37: 'h43'\n",
    "37 - 38: 'h44'\n",
    "38 - 39: 'h45'\n",
    "\"\"\"\n",
    " \n",
    "nums = np_num3\n",
    "\n",
    "for i in range(len(nums)):\n",
    "    if nums[i] >= -11 and nums[i] < -10:\n",
    "        nums[i] = 'h0'\n",
    "    elif nums[i] >= -10 and nums[i] < -9:\n",
    "        nums[i] = 'h1'\n",
    "    elif nums[i] >= -9 and nums[i] < -8:\n",
    "        nums[i] = 'h2'\n",
    "    elif nums[i] >= -8 and nums[i] < -7:\n",
    "        nums[i] = 'h3'\n",
    "    elif nums[i] >= -7 and nums[i] < -6:\n",
    "        nums[i] = 'h4'\n",
    "    elif nums[i] >= -6 and nums[i] < -5:\n",
    "        nums[i] = 'h5'\n",
    "    elif nums[i] >= -5 and nums[i] < -4:\n",
    "        nums[i] = 'h6'\n",
    "    elif nums[i] >= -4 and nums[i] < -3:\n",
    "        nums[i] = 'h7'\n",
    "    elif nums[i] >= -3 and nums[i] < -2:\n",
    "        nums[i] = 'h8'\n",
    "    elif nums[i] >= -2 and nums[i] < -1:\n",
    "        nums[i] = 'h9'\n",
    "    elif nums[i] >= -1 and nums[i] < 0:\n",
    "        nums[i] = 'h10'\n",
    "    elif nums[i] >= 0 and nums[i] < 1:\n",
    "        nums[i] = 'h11'\n",
    "    elif nums[i] >= 1 and nums[i] < 2:\n",
    "        nums[i] = 'h12'\n",
    "    elif nums[i] >= 2 and nums[i] < 3:\n",
    "        nums[i] = 'h13'\n",
    "    elif nums[i] >= 3 and nums[i] < 4:\n",
    "        nums[i] = 'h14'\n",
    "    elif nums[i] >= 4 and nums[i] < 5:\n",
    "        nums[i] = 'h15'\n",
    "    elif nums[i] >= 5 and nums[i] < 6:\n",
    "        nums[i] = 'h16'\n",
    "    elif nums[i] >= 6 and nums[i] < 7:\n",
    "        nums[i] = 'h17'\n",
    "    elif nums[i] >= 7 and nums[i] < 8:\n",
    "        nums[i] = 'h18'\n",
    "    elif nums[i] >= 8 and nums[i] < 9:\n",
    "        nums[i] = 'h19'\n",
    "    elif nums[i] >= 9 and nums[i] < 10:\n",
    "        nums[i] = 'h20'\n",
    "    elif nums[i] >= 10 and nums[i] < 11:\n",
    "        nums[i] = 'h21'\n",
    "    elif nums[i] >= 11 and nums[i] < 12:\n",
    "        nums[i] = 'h22'\n",
    "    elif nums[i] >= 12 and nums[i] < 13:\n",
    "        nums[i] = 'h23'\n",
    "    elif nums[i] >= 13 and nums[i] < 14:\n",
    "        nums[i] = 'h24'\n",
    "    elif nums[i] >= 14 and nums[i] < 15:\n",
    "        nums[i] = 'h25'\n",
    "    elif nums[i] >= 15 and nums[i] < 16:\n",
    "        nums[i] = 'h26'\n",
    "    elif nums[i] >= 16 and nums[i] < 17:\n",
    "        nums[i] = 'h27'\n",
    "    elif nums[i] >= 17 and nums[i] < 18:\n",
    "        nums[i] = 'h28'\n",
    "    elif nums[i] >= 18 and nums[i] < 19:\n",
    "        nums[i] = 'h29'\n",
    "    elif nums[i] >= 19 and nums[i] < 20:\n",
    "        nums[i] = 'h30'\n",
    "    elif nums[i] >= 20 and nums[i] < 21:\n",
    "        nums[i] = 'h31'\n",
    "    elif nums[i] >= 21 and nums[i] < 22:\n",
    "        nums[i] = 'h32'\n",
    "    elif nums[i] >= 22 and nums[i] < 23:\n",
    "        nums[i] = 'h33'\n",
    "    elif nums[i] >= 23 and nums[i] < 24:\n",
    "        nums[i] = 'h34'\n",
    "    elif nums[i] >= 24 and nums[i] < 25:\n",
    "        nums[i] = 'h35'\n",
    "    elif nums[i] >= 25 and nums[i] < 26:\n",
    "        nums[i] = 'h36'\n",
    "    elif nums[i] >= 26 and nums[i] < 27:\n",
    "        nums[i] = 'h37'\n",
    "    elif nums[i] >= 27 and nums[i] < 28:\n",
    "        nums[i] = 'h38'\n",
    "    elif nums[i] >= 28 and nums[i] < 29:\n",
    "        nums[i] = 'h39'\n",
    "    elif nums[i] >= 29 and nums[i] < 30:\n",
    "        nums[i] = 'h40'\n",
    "    elif nums[i] >= 30 and nums[i] < 31:\n",
    "        nums[i] = 'h41'\n",
    "    elif nums[i] >= 31 and nums[i] < 32:\n",
    "        nums[i] = 'h42'\n",
    "    elif nums[i] >= 32 and nums[i] < 33:\n",
    "        nums[i] = 'h43'\n",
    "    elif nums[i] >= 33 and nums[i] < 34:\n",
    "        nums[i] = 'h44'\n",
    "    elif nums[i] >= 34 and nums[i] < 35:\n",
    "        nums[i] = 'h45'\n",
    "    elif nums[i] >= 35 and nums[i] < 36:\n",
    "        nums[i] = 'h46'\n",
    "    elif nums[i] >= 36 and nums[i] < 37:\n",
    "        nums[i] = 'h47'\n",
    "    elif nums[i] >= 37 and nums[i] < 38:\n",
    "        nums[i] = 'h48'\n",
    "    elif nums[i] >= 38 and nums[i] < 39:\n",
    "        nums[i] = 'h49'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  high\n",
       "0  h12\n",
       "1  h17\n",
       "2  h18\n",
       "3  h19\n",
       "4  h18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num1 = DataFrame(np_num1, columns=['high'])\n",
    "df_num2 = DataFrame(np_num2, columns=['low'])\n",
    "df_num3 = DataFrame(np_num3, columns=['average'])\n",
    "\n",
    "df_num1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_high = pd.get_dummies(df_num1, prefix=['high'])\n",
    "pd_low = pd.get_dummies(df_num2, prefix=['low'])\n",
    "pd_average = pd.get_dummies(df_num3, prefix=['aver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_h11</th>\n",
       "      <th>high_h12</th>\n",
       "      <th>high_h13</th>\n",
       "      <th>high_h14</th>\n",
       "      <th>high_h15</th>\n",
       "      <th>high_h16</th>\n",
       "      <th>high_h17</th>\n",
       "      <th>high_h18</th>\n",
       "      <th>high_h19</th>\n",
       "      <th>high_h20</th>\n",
       "      <th>...</th>\n",
       "      <th>aver_h37</th>\n",
       "      <th>aver_h38</th>\n",
       "      <th>aver_h39</th>\n",
       "      <th>aver_h40</th>\n",
       "      <th>aver_h41</th>\n",
       "      <th>aver_h42</th>\n",
       "      <th>aver_h43</th>\n",
       "      <th>aver_h6</th>\n",
       "      <th>aver_h8</th>\n",
       "      <th>aver_h9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   high_h11  high_h12  high_h13  high_h14  high_h15  high_h16  high_h17  \\\n",
       "0         0         1         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   high_h18  high_h19  high_h20   ...     aver_h37  aver_h38  aver_h39  \\\n",
       "0         0         0         0   ...            0         0         0   \n",
       "1         0         0         0   ...            0         0         0   \n",
       "2         1         0         0   ...            0         0         0   \n",
       "3         0         1         0   ...            0         0         0   \n",
       "4         1         0         0   ...            0         0         0   \n",
       "\n",
       "   aver_h40  aver_h41  aver_h42  aver_h43  aver_h6  aver_h8  aver_h9  \n",
       "0         0         0         0         0        0        0        0  \n",
       "1         0         0         0         0        0        0        0  \n",
       "2         0         0         0         0        0        0        0  \n",
       "3         0         0         0         0        0        0        0  \n",
       "4         0         0         0         0        0        0        0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.concat([pd_high, pd_low, pd_average], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalized the humidity feature and concatenate the features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.599165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.561587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.446764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.271399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   humidity\n",
       "0  0.599165\n",
       "1  0.636743\n",
       "2  0.561587\n",
       "3  0.446764\n",
       "4  0.271399"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize humidity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "humidity_scaled = min_max_scaler.fit_transform(ratio_df[['humidity']])\n",
    "humidity_scaled\n",
    "\n",
    "array_hum = []\n",
    "for num in humidity_scaled:\n",
    "    array_hum.append(num[0])\n",
    "    \n",
    "pd_hum = DataFrame(array_hum, columns=[['humidity']])\n",
    "pd_hum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_h11</th>\n",
       "      <th>high_h12</th>\n",
       "      <th>high_h13</th>\n",
       "      <th>high_h14</th>\n",
       "      <th>high_h15</th>\n",
       "      <th>high_h16</th>\n",
       "      <th>high_h17</th>\n",
       "      <th>high_h18</th>\n",
       "      <th>high_h19</th>\n",
       "      <th>high_h20</th>\n",
       "      <th>...</th>\n",
       "      <th>aver_h38</th>\n",
       "      <th>aver_h39</th>\n",
       "      <th>aver_h40</th>\n",
       "      <th>aver_h41</th>\n",
       "      <th>aver_h42</th>\n",
       "      <th>aver_h43</th>\n",
       "      <th>aver_h6</th>\n",
       "      <th>aver_h8</th>\n",
       "      <th>aver_h9</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   high_h11  high_h12  high_h13  high_h14  high_h15  high_h16  high_h17  \\\n",
       "0         0         1         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   high_h18  high_h19  high_h20    ...     aver_h38  aver_h39  aver_h40  \\\n",
       "0         0         0         0    ...            0         0         0   \n",
       "1         0         0         0    ...            0         0         0   \n",
       "2         1         0         0    ...            0         0         0   \n",
       "3         0         1         0    ...            0         0         0   \n",
       "4         1         0         0    ...            0         0         0   \n",
       "\n",
       "   aver_h41  aver_h42  aver_h43  aver_h6  aver_h8  aver_h9  humidity  \n",
       "0         0         0         0        0        0        0  0.599165  \n",
       "1         0         0         0        0        0        0  0.636743  \n",
       "2         0         0         0        0        0        0  0.561587  \n",
       "3         0         0         0        0        0        0  0.446764  \n",
       "4         0         0         0        0        0        0  0.271399  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ = pd.concat([features, pd_hum], axis=1)\n",
    "features_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. Add Holiday (weekday and weekend) as a feature - holiday\n",
    "holiday = [0] * 578\n",
    "holiday[0] = 1\n",
    "end = 1\n",
    "while end + 5 <= 577:\n",
    "    end += 5\n",
    "    holiday[end] = 1\n",
    "    \n",
    "    end += 1\n",
    "    holiday[end] = 1\n",
    "    \n",
    "    end += 1\n",
    "\n",
    "len(holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_h11</th>\n",
       "      <th>high_h12</th>\n",
       "      <th>high_h13</th>\n",
       "      <th>high_h14</th>\n",
       "      <th>high_h15</th>\n",
       "      <th>high_h16</th>\n",
       "      <th>high_h17</th>\n",
       "      <th>high_h18</th>\n",
       "      <th>high_h19</th>\n",
       "      <th>high_h20</th>\n",
       "      <th>...</th>\n",
       "      <th>aver_h39</th>\n",
       "      <th>aver_h40</th>\n",
       "      <th>aver_h41</th>\n",
       "      <th>aver_h42</th>\n",
       "      <th>aver_h43</th>\n",
       "      <th>aver_h6</th>\n",
       "      <th>aver_h8</th>\n",
       "      <th>aver_h9</th>\n",
       "      <th>humidity</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561587</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   high_h11  high_h12  high_h13  high_h14  high_h15  high_h16  high_h17  \\\n",
       "0         0         1         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   high_h18  high_h19  high_h20   ...     aver_h39  aver_h40  aver_h41  \\\n",
       "0         0         0         0   ...            0         0         0   \n",
       "1         0         0         0   ...            0         0         0   \n",
       "2         1         0         0   ...            0         0         0   \n",
       "3         0         1         0   ...            0         0         0   \n",
       "4         1         0         0   ...            0         0         0   \n",
       "\n",
       "   aver_h42  aver_h43  aver_h6  aver_h8  aver_h9  humidity  holiday  \n",
       "0         0         0        0        0        0  0.599165        1  \n",
       "1         0         0        0        0        0  0.636743        0  \n",
       "2         0         0        0        0        0  0.561587        0  \n",
       "3         0         0        0        0        0  0.446764        0  \n",
       "4         0         0        0        0        0  0.271399        0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features = pd.concat([features_, DataFrame(holiday, columns=['holiday'])], axis=1)\n",
    "final_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578, 118)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run SVM to get the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mguo/Downloads/ENTER/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.074992097748792397"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test using SVM\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, target_np, test_size=0.25)\n",
    "\n",
    "regressor = SVR(C=0.1, kernel='rbf', epsilon=0.01)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "mean_absolute_error(y_test, regressor.predict(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss function using error rate which is abs(predicted - target) / target\n",
    "def loss(y_test, y_pred):\n",
    "    error = 0\n",
    "    for y in zip(y_test, y_pred):\n",
    "        error += abs(y[0] - y[1]) / y[0]\n",
    "        \n",
    "    return error / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076737287600009321"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_test, regressor.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.074128625324297634"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = regressor.predict(final_features)\n",
    "loss(target_np, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Baselines: \n",
    "\n",
    "1. Linear Regressor\n",
    "2. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08045161221778746"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline 1: Linear Regressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "loss(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1f2de6d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PlotHistory(target_np, reg.predict(np_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08046609704482019"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline 2: Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "loss(y_test, reg.predict(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.075150809757019751"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Error for All Data using Decision Tree Regressor\n",
    "Y_pred = reg.predict(final_features)\n",
    "loss(target_np, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEoppYL6f8J+GcJebgIeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1f23a0090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PlotHistory(target_np, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Show the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlotHistory(y_test, y_pred):\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(y_test)\n",
    "    ax.plot(y_pred)\n",
    "    ## Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['test value ', 'predicted value '], loc = 0) \n",
    "    ax.set_title('Fitting Lines between Test Values and Predicted Values ')\n",
    "    ax.set_xlabel('counter')\n",
    "    ax.set_ylabel('value') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEoppYL6f8J+GcJebgIeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1df413ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PlotHistory(target_np, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning - Using Keras for building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define Neural model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=118, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Build the model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.08025 (0.02706) Error Rate\n"
     ]
    }
   ],
   "source": [
    "# 3.1. Feed the data and get the evaluation results (3 layers)\n",
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)\n",
    "\n",
    "X = np_features\n",
    "Y = np_target\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.5f (%.5f) Error Rate\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deeper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1  8-layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define Neural model\n",
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, input_dim=118, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Build the model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=deep_model, nb_epoch=50, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.07758 (0.02367) Error Rate\n"
     ]
    }
   ],
   "source": [
    "# 3. Feed the data and get the evaluation results (8 layers)\n",
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)\n",
    "\n",
    "X = np_features\n",
    "Y = np_target\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.5f (%.5f) Error Rate\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: 9-layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define Neural model\n",
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(100, input_dim=118, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Build the model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=deep_model, nb_epoch=50, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.07819 (0.02566) Error Rate\n"
     ]
    }
   ],
   "source": [
    "# 3. Feed the data and get the evaluation results (8 layers)\n",
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)\n",
    "\n",
    "X = np_features\n",
    "Y = np_target\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.5f (%.5f) Error Rate\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3: 10-layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define Neural model\n",
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, input_dim=118, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(7, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Build the model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=deep_model, nb_epoch=50, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.07829 (0.02331) Error Rate\n"
     ]
    }
   ],
   "source": [
    "# 3. Feed the data and get the evaluation results (8 layers)\n",
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)\n",
    "\n",
    "X = np_features\n",
    "Y = np_target\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.5f (%.5f) Error Rate\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7.4: 8-layers model with 100 training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define Neural model\n",
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, input_dim=118, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Build the model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=deep_model, nb_epoch=110, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.07758 (0.02367) Error Rate\n"
     ]
    }
   ],
   "source": [
    "# 3. Feed the data and get the evaluation results (8 layers)\n",
    "np_features = np.asarray(final_features)\n",
    "np_target = np.asarray(target_np)\n",
    "\n",
    "X = np_features\n",
    "Y = np_target\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.5f (%.5f) Error Rate\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary: the best and accurate error rate is 7.758% for site 1 using MLP models\n",
    "\n",
    "## In the next session, we will focus on the multitask learning so how it can improve the error rate (e.g. for the first observation site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Multitask Learning\n",
    "\n",
    "## Part 1: data preprocessing for all observation sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>humidity</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>...</th>\n",
       "      <th>target_37</th>\n",
       "      <th>target_38</th>\n",
       "      <th>target_39</th>\n",
       "      <th>target_40</th>\n",
       "      <th>target_41</th>\n",
       "      <th>target_42</th>\n",
       "      <th>target_43</th>\n",
       "      <th>target_44</th>\n",
       "      <th>target_45</th>\n",
       "      <th>target_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>75.000</td>\n",
       "      <td>901.094080</td>\n",
       "      <td>162.949446</td>\n",
       "      <td>727.585676</td>\n",
       "      <td>...</td>\n",
       "      <td>478.261740</td>\n",
       "      <td>612.896364</td>\n",
       "      <td>747.532996</td>\n",
       "      <td>965.241734</td>\n",
       "      <td>841.822074</td>\n",
       "      <td>428.932244</td>\n",
       "      <td>315.977223</td>\n",
       "      <td>610.669157</td>\n",
       "      <td>715.010485</td>\n",
       "      <td>464.724511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>1.7625</td>\n",
       "      <td>77.250</td>\n",
       "      <td>744.658412</td>\n",
       "      <td>144.802486</td>\n",
       "      <td>629.108444</td>\n",
       "      <td>...</td>\n",
       "      <td>397.217620</td>\n",
       "      <td>471.977703</td>\n",
       "      <td>613.450655</td>\n",
       "      <td>967.654491</td>\n",
       "      <td>678.675733</td>\n",
       "      <td>377.299306</td>\n",
       "      <td>304.421079</td>\n",
       "      <td>475.282134</td>\n",
       "      <td>654.681175</td>\n",
       "      <td>436.077693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.2375</td>\n",
       "      <td>72.750</td>\n",
       "      <td>743.560438</td>\n",
       "      <td>107.367718</td>\n",
       "      <td>633.000613</td>\n",
       "      <td>...</td>\n",
       "      <td>469.580215</td>\n",
       "      <td>468.818233</td>\n",
       "      <td>590.410559</td>\n",
       "      <td>918.974466</td>\n",
       "      <td>634.091766</td>\n",
       "      <td>423.005614</td>\n",
       "      <td>279.511575</td>\n",
       "      <td>475.837467</td>\n",
       "      <td>586.668844</td>\n",
       "      <td>450.792579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0375</td>\n",
       "      <td>65.875</td>\n",
       "      <td>784.915894</td>\n",
       "      <td>139.150299</td>\n",
       "      <td>631.792933</td>\n",
       "      <td>...</td>\n",
       "      <td>426.806403</td>\n",
       "      <td>406.796248</td>\n",
       "      <td>529.548238</td>\n",
       "      <td>799.568921</td>\n",
       "      <td>661.479923</td>\n",
       "      <td>368.000699</td>\n",
       "      <td>282.622732</td>\n",
       "      <td>465.404321</td>\n",
       "      <td>595.074607</td>\n",
       "      <td>420.071264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.9</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>1.8625</td>\n",
       "      <td>55.375</td>\n",
       "      <td>685.154914</td>\n",
       "      <td>90.278503</td>\n",
       "      <td>575.140527</td>\n",
       "      <td>...</td>\n",
       "      <td>413.930428</td>\n",
       "      <td>428.102990</td>\n",
       "      <td>510.400056</td>\n",
       "      <td>705.148753</td>\n",
       "      <td>567.459090</td>\n",
       "      <td>373.563940</td>\n",
       "      <td>263.720232</td>\n",
       "      <td>432.416071</td>\n",
       "      <td>589.593886</td>\n",
       "      <td>401.559982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  high  low  average  humidity    target_1    target_2  \\\n",
       "0  2015      2    1   1.9 -0.4   0.7875    75.000  901.094080  162.949446   \n",
       "1  2015      2    2   6.2 -3.9   1.7625    77.250  744.658412  144.802486   \n",
       "2  2015      2    3   7.8  2.0   4.2375    72.750  743.560438  107.367718   \n",
       "3  2015      2    4   8.5 -1.2   3.0375    65.875  784.915894  139.150299   \n",
       "4  2015      2    5   7.9 -3.6   1.8625    55.375  685.154914   90.278503   \n",
       "\n",
       "     target_3     ...       target_37   target_38   target_39   target_40  \\\n",
       "0  727.585676     ...      478.261740  612.896364  747.532996  965.241734   \n",
       "1  629.108444     ...      397.217620  471.977703  613.450655  967.654491   \n",
       "2  633.000613     ...      469.580215  468.818233  590.410559  918.974466   \n",
       "3  631.792933     ...      426.806403  406.796248  529.548238  799.568921   \n",
       "4  575.140527     ...      413.930428  428.102990  510.400056  705.148753   \n",
       "\n",
       "    target_41   target_42   target_43   target_44   target_45   target_46  \n",
       "0  841.822074  428.932244  315.977223  610.669157  715.010485  464.724511  \n",
       "1  678.675733  377.299306  304.421079  475.282134  654.681175  436.077693  \n",
       "2  634.091766  423.005614  279.511575  475.837467  586.668844  450.792579  \n",
       "3  661.479923  368.000699  282.622732  465.404321  595.074607  420.071264  \n",
       "4  567.459090  373.563940  263.720232  432.416071  589.593886  401.559982  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = read_csv(\"data.csv\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "      <th>target_6</th>\n",
       "      <th>target_7</th>\n",
       "      <th>target_8</th>\n",
       "      <th>target_9</th>\n",
       "      <th>target_10</th>\n",
       "      <th>...</th>\n",
       "      <th>target_37</th>\n",
       "      <th>target_38</th>\n",
       "      <th>target_39</th>\n",
       "      <th>target_40</th>\n",
       "      <th>target_41</th>\n",
       "      <th>target_42</th>\n",
       "      <th>target_43</th>\n",
       "      <th>target_44</th>\n",
       "      <th>target_45</th>\n",
       "      <th>target_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>901.094080</td>\n",
       "      <td>162.949446</td>\n",
       "      <td>727.585676</td>\n",
       "      <td>-11.153406</td>\n",
       "      <td>123.609260</td>\n",
       "      <td>472.958564</td>\n",
       "      <td>834.962039</td>\n",
       "      <td>-4.321780</td>\n",
       "      <td>428.579080</td>\n",
       "      <td>219.059225</td>\n",
       "      <td>...</td>\n",
       "      <td>478.261740</td>\n",
       "      <td>612.896364</td>\n",
       "      <td>747.532996</td>\n",
       "      <td>965.241734</td>\n",
       "      <td>841.822074</td>\n",
       "      <td>428.932244</td>\n",
       "      <td>315.977223</td>\n",
       "      <td>610.669157</td>\n",
       "      <td>715.010485</td>\n",
       "      <td>464.724511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>744.658412</td>\n",
       "      <td>144.802486</td>\n",
       "      <td>629.108444</td>\n",
       "      <td>3.139475</td>\n",
       "      <td>130.035425</td>\n",
       "      <td>446.337262</td>\n",
       "      <td>722.044629</td>\n",
       "      <td>13.753263</td>\n",
       "      <td>366.430302</td>\n",
       "      <td>262.458704</td>\n",
       "      <td>...</td>\n",
       "      <td>397.217620</td>\n",
       "      <td>471.977703</td>\n",
       "      <td>613.450655</td>\n",
       "      <td>967.654491</td>\n",
       "      <td>678.675733</td>\n",
       "      <td>377.299306</td>\n",
       "      <td>304.421079</td>\n",
       "      <td>475.282134</td>\n",
       "      <td>654.681175</td>\n",
       "      <td>436.077693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>743.560438</td>\n",
       "      <td>107.367718</td>\n",
       "      <td>633.000613</td>\n",
       "      <td>-0.738059</td>\n",
       "      <td>95.405784</td>\n",
       "      <td>433.472087</td>\n",
       "      <td>749.277117</td>\n",
       "      <td>10.625347</td>\n",
       "      <td>329.560499</td>\n",
       "      <td>261.630709</td>\n",
       "      <td>...</td>\n",
       "      <td>469.580215</td>\n",
       "      <td>468.818233</td>\n",
       "      <td>590.410559</td>\n",
       "      <td>918.974466</td>\n",
       "      <td>634.091766</td>\n",
       "      <td>423.005614</td>\n",
       "      <td>279.511575</td>\n",
       "      <td>475.837467</td>\n",
       "      <td>586.668844</td>\n",
       "      <td>450.792579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784.915894</td>\n",
       "      <td>139.150299</td>\n",
       "      <td>631.792933</td>\n",
       "      <td>-10.069709</td>\n",
       "      <td>101.578663</td>\n",
       "      <td>399.229170</td>\n",
       "      <td>645.384388</td>\n",
       "      <td>18.048176</td>\n",
       "      <td>322.957442</td>\n",
       "      <td>229.032526</td>\n",
       "      <td>...</td>\n",
       "      <td>426.806403</td>\n",
       "      <td>406.796248</td>\n",
       "      <td>529.548238</td>\n",
       "      <td>799.568921</td>\n",
       "      <td>661.479923</td>\n",
       "      <td>368.000699</td>\n",
       "      <td>282.622732</td>\n",
       "      <td>465.404321</td>\n",
       "      <td>595.074607</td>\n",
       "      <td>420.071264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>685.154914</td>\n",
       "      <td>90.278503</td>\n",
       "      <td>575.140527</td>\n",
       "      <td>-9.510255</td>\n",
       "      <td>108.534308</td>\n",
       "      <td>347.921127</td>\n",
       "      <td>621.195936</td>\n",
       "      <td>22.420897</td>\n",
       "      <td>327.327788</td>\n",
       "      <td>203.715167</td>\n",
       "      <td>...</td>\n",
       "      <td>413.930428</td>\n",
       "      <td>428.102990</td>\n",
       "      <td>510.400056</td>\n",
       "      <td>705.148753</td>\n",
       "      <td>567.459090</td>\n",
       "      <td>373.563940</td>\n",
       "      <td>263.720232</td>\n",
       "      <td>432.416071</td>\n",
       "      <td>589.593886</td>\n",
       "      <td>401.559982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target_1    target_2    target_3   target_4    target_5    target_6  \\\n",
       "0  901.094080  162.949446  727.585676 -11.153406  123.609260  472.958564   \n",
       "1  744.658412  144.802486  629.108444   3.139475  130.035425  446.337262   \n",
       "2  743.560438  107.367718  633.000613  -0.738059   95.405784  433.472087   \n",
       "3  784.915894  139.150299  631.792933 -10.069709  101.578663  399.229170   \n",
       "4  685.154914   90.278503  575.140527  -9.510255  108.534308  347.921127   \n",
       "\n",
       "     target_7   target_8    target_9   target_10     ...       target_37  \\\n",
       "0  834.962039  -4.321780  428.579080  219.059225     ...      478.261740   \n",
       "1  722.044629  13.753263  366.430302  262.458704     ...      397.217620   \n",
       "2  749.277117  10.625347  329.560499  261.630709     ...      469.580215   \n",
       "3  645.384388  18.048176  322.957442  229.032526     ...      426.806403   \n",
       "4  621.195936  22.420897  327.327788  203.715167     ...      413.930428   \n",
       "\n",
       "    target_38   target_39   target_40   target_41   target_42   target_43  \\\n",
       "0  612.896364  747.532996  965.241734  841.822074  428.932244  315.977223   \n",
       "1  471.977703  613.450655  967.654491  678.675733  377.299306  304.421079   \n",
       "2  468.818233  590.410559  918.974466  634.091766  423.005614  279.511575   \n",
       "3  406.796248  529.548238  799.568921  661.479923  368.000699  282.622732   \n",
       "4  428.102990  510.400056  705.148753  567.459090  373.563940  263.720232   \n",
       "\n",
       "    target_44   target_45   target_46  \n",
       "0  610.669157  715.010485  464.724511  \n",
       "1  475.282134  654.681175  436.077693  \n",
       "2  475.837467  586.668844  450.792579  \n",
       "3  465.404321  595.074607  420.071264  \n",
       "4  432.416071  589.593886  401.559982  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_data['year']\n",
    "del all_data['month']\n",
    "del all_data['day']\n",
    "del all_data['high']\n",
    "del all_data['low']\n",
    "del all_data['average']\n",
    "del all_data['humidity']\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "      <th>target_6</th>\n",
       "      <th>target_7</th>\n",
       "      <th>target_8</th>\n",
       "      <th>target_9</th>\n",
       "      <th>target_10</th>\n",
       "      <th>...</th>\n",
       "      <th>target_37</th>\n",
       "      <th>target_38</th>\n",
       "      <th>target_39</th>\n",
       "      <th>target_40</th>\n",
       "      <th>target_41</th>\n",
       "      <th>target_42</th>\n",
       "      <th>target_43</th>\n",
       "      <th>target_44</th>\n",
       "      <th>target_45</th>\n",
       "      <th>target_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.197709</td>\n",
       "      <td>285.046500</td>\n",
       "      <td>490.772663</td>\n",
       "      <td>156.048659</td>\n",
       "      <td>130.074646</td>\n",
       "      <td>312.434679</td>\n",
       "      <td>620.881838</td>\n",
       "      <td>114.019559</td>\n",
       "      <td>326.655980</td>\n",
       "      <td>215.219510</td>\n",
       "      <td>...</td>\n",
       "      <td>520.247194</td>\n",
       "      <td>418.178387</td>\n",
       "      <td>466.768330</td>\n",
       "      <td>552.271783</td>\n",
       "      <td>438.914756</td>\n",
       "      <td>287.312055</td>\n",
       "      <td>180.595186</td>\n",
       "      <td>410.572202</td>\n",
       "      <td>459.142460</td>\n",
       "      <td>371.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>286.091280</td>\n",
       "      <td>199.862399</td>\n",
       "      <td>265.355729</td>\n",
       "      <td>174.256129</td>\n",
       "      <td>93.745707</td>\n",
       "      <td>196.657463</td>\n",
       "      <td>253.733802</td>\n",
       "      <td>99.625866</td>\n",
       "      <td>143.891431</td>\n",
       "      <td>137.811505</td>\n",
       "      <td>...</td>\n",
       "      <td>307.429948</td>\n",
       "      <td>242.690925</td>\n",
       "      <td>221.037984</td>\n",
       "      <td>263.522635</td>\n",
       "      <td>226.066905</td>\n",
       "      <td>149.633136</td>\n",
       "      <td>117.846971</td>\n",
       "      <td>192.013776</td>\n",
       "      <td>223.947843</td>\n",
       "      <td>212.250247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>336.740122</td>\n",
       "      <td>21.150318</td>\n",
       "      <td>257.353534</td>\n",
       "      <td>-21.029919</td>\n",
       "      <td>35.995113</td>\n",
       "      <td>146.090480</td>\n",
       "      <td>393.617875</td>\n",
       "      <td>-14.804657</td>\n",
       "      <td>169.357817</td>\n",
       "      <td>104.390113</td>\n",
       "      <td>...</td>\n",
       "      <td>202.390508</td>\n",
       "      <td>230.925056</td>\n",
       "      <td>245.542608</td>\n",
       "      <td>291.913877</td>\n",
       "      <td>237.063207</td>\n",
       "      <td>158.577664</td>\n",
       "      <td>71.509404</td>\n",
       "      <td>237.953428</td>\n",
       "      <td>247.884440</td>\n",
       "      <td>210.482247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>410.332065</td>\n",
       "      <td>158.771288</td>\n",
       "      <td>332.233835</td>\n",
       "      <td>20.021201</td>\n",
       "      <td>79.094940</td>\n",
       "      <td>204.317552</td>\n",
       "      <td>469.745409</td>\n",
       "      <td>63.684747</td>\n",
       "      <td>236.494372</td>\n",
       "      <td>146.301021</td>\n",
       "      <td>...</td>\n",
       "      <td>352.041091</td>\n",
       "      <td>287.216682</td>\n",
       "      <td>339.478272</td>\n",
       "      <td>384.982720</td>\n",
       "      <td>305.145365</td>\n",
       "      <td>200.212154</td>\n",
       "      <td>109.130605</td>\n",
       "      <td>292.101183</td>\n",
       "      <td>314.714729</td>\n",
       "      <td>253.356933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>485.718435</td>\n",
       "      <td>241.455904</td>\n",
       "      <td>386.451650</td>\n",
       "      <td>122.507413</td>\n",
       "      <td>104.840755</td>\n",
       "      <td>240.559336</td>\n",
       "      <td>530.999904</td>\n",
       "      <td>92.559172</td>\n",
       "      <td>273.114090</td>\n",
       "      <td>163.698986</td>\n",
       "      <td>...</td>\n",
       "      <td>404.220756</td>\n",
       "      <td>329.574199</td>\n",
       "      <td>392.496892</td>\n",
       "      <td>459.141288</td>\n",
       "      <td>359.704440</td>\n",
       "      <td>235.196266</td>\n",
       "      <td>136.467067</td>\n",
       "      <td>339.309297</td>\n",
       "      <td>367.066054</td>\n",
       "      <td>296.224312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>645.223982</td>\n",
       "      <td>330.669537</td>\n",
       "      <td>524.913644</td>\n",
       "      <td>202.174512</td>\n",
       "      <td>149.374484</td>\n",
       "      <td>340.166474</td>\n",
       "      <td>654.929093</td>\n",
       "      <td>124.456267</td>\n",
       "      <td>351.386976</td>\n",
       "      <td>218.649802</td>\n",
       "      <td>...</td>\n",
       "      <td>564.388729</td>\n",
       "      <td>428.072956</td>\n",
       "      <td>501.345673</td>\n",
       "      <td>621.606897</td>\n",
       "      <td>464.333773</td>\n",
       "      <td>306.749532</td>\n",
       "      <td>209.058687</td>\n",
       "      <td>455.204775</td>\n",
       "      <td>513.154812</td>\n",
       "      <td>381.400179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1957.846210</td>\n",
       "      <td>1207.689996</td>\n",
       "      <td>1735.690175</td>\n",
       "      <td>983.147059</td>\n",
       "      <td>660.409882</td>\n",
       "      <td>1271.516219</td>\n",
       "      <td>1849.143185</td>\n",
       "      <td>601.685769</td>\n",
       "      <td>936.294368</td>\n",
       "      <td>931.763994</td>\n",
       "      <td>...</td>\n",
       "      <td>2035.097195</td>\n",
       "      <td>1586.692112</td>\n",
       "      <td>1573.637670</td>\n",
       "      <td>1723.167363</td>\n",
       "      <td>1537.294793</td>\n",
       "      <td>1041.672818</td>\n",
       "      <td>756.588113</td>\n",
       "      <td>1267.950940</td>\n",
       "      <td>1479.794424</td>\n",
       "      <td>1432.856096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target_1     target_2     target_3    target_4    target_5  \\\n",
       "count   578.000000   578.000000   578.000000  578.000000  578.000000   \n",
       "mean    586.197709   285.046500   490.772663  156.048659  130.074646   \n",
       "std     286.091280   199.862399   265.355729  174.256129   93.745707   \n",
       "min     336.740122    21.150318   257.353534  -21.029919   35.995113   \n",
       "25%     410.332065   158.771288   332.233835   20.021201   79.094940   \n",
       "50%     485.718435   241.455904   386.451650  122.507413  104.840755   \n",
       "75%     645.223982   330.669537   524.913644  202.174512  149.374484   \n",
       "max    1957.846210  1207.689996  1735.690175  983.147059  660.409882   \n",
       "\n",
       "          target_6     target_7    target_8    target_9   target_10  \\\n",
       "count   578.000000   578.000000  578.000000  578.000000  578.000000   \n",
       "mean    312.434679   620.881838  114.019559  326.655980  215.219510   \n",
       "std     196.657463   253.733802   99.625866  143.891431  137.811505   \n",
       "min     146.090480   393.617875  -14.804657  169.357817  104.390113   \n",
       "25%     204.317552   469.745409   63.684747  236.494372  146.301021   \n",
       "50%     240.559336   530.999904   92.559172  273.114090  163.698986   \n",
       "75%     340.166474   654.929093  124.456267  351.386976  218.649802   \n",
       "max    1271.516219  1849.143185  601.685769  936.294368  931.763994   \n",
       "\n",
       "          ...         target_37    target_38    target_39    target_40  \\\n",
       "count     ...        578.000000   578.000000   578.000000   578.000000   \n",
       "mean      ...        520.247194   418.178387   466.768330   552.271783   \n",
       "std       ...        307.429948   242.690925   221.037984   263.522635   \n",
       "min       ...        202.390508   230.925056   245.542608   291.913877   \n",
       "25%       ...        352.041091   287.216682   339.478272   384.982720   \n",
       "50%       ...        404.220756   329.574199   392.496892   459.141288   \n",
       "75%       ...        564.388729   428.072956   501.345673   621.606897   \n",
       "max       ...       2035.097195  1586.692112  1573.637670  1723.167363   \n",
       "\n",
       "         target_41    target_42   target_43    target_44    target_45  \\\n",
       "count   578.000000   578.000000  578.000000   578.000000   578.000000   \n",
       "mean    438.914756   287.312055  180.595186   410.572202   459.142460   \n",
       "std     226.066905   149.633136  117.846971   192.013776   223.947843   \n",
       "min     237.063207   158.577664   71.509404   237.953428   247.884440   \n",
       "25%     305.145365   200.212154  109.130605   292.101183   314.714729   \n",
       "50%     359.704440   235.196266  136.467067   339.309297   367.066054   \n",
       "75%     464.333773   306.749532  209.058687   455.204775   513.154812   \n",
       "max    1537.294793  1041.672818  756.588113  1267.950940  1479.794424   \n",
       "\n",
       "         target_46  \n",
       "count   578.000000  \n",
       "mean    371.664700  \n",
       "std     212.250247  \n",
       "min     210.482247  \n",
       "25%     253.356933  \n",
       "50%     296.224312  \n",
       "75%     381.400179  \n",
       "max    1432.856096  \n",
       "\n",
       "[8 rows x 46 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle Abnormal Data - Replace the value 0 and negative value to be 1 (for computing ratio)\n",
    "def remove_abnormal(all_data):\n",
    "    for index, row in all_data.iterrows():\n",
    "        for i in range(len(row)):\n",
    "            if row[i] <= 0:\n",
    "                row[i] = 1\n",
    "    \n",
    "remove_abnormal(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "      <th>target_6</th>\n",
       "      <th>target_7</th>\n",
       "      <th>target_8</th>\n",
       "      <th>target_9</th>\n",
       "      <th>target_10</th>\n",
       "      <th>...</th>\n",
       "      <th>target_37</th>\n",
       "      <th>target_38</th>\n",
       "      <th>target_39</th>\n",
       "      <th>target_40</th>\n",
       "      <th>target_41</th>\n",
       "      <th>target_42</th>\n",
       "      <th>target_43</th>\n",
       "      <th>target_44</th>\n",
       "      <th>target_45</th>\n",
       "      <th>target_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.197709</td>\n",
       "      <td>285.046500</td>\n",
       "      <td>490.772663</td>\n",
       "      <td>156.752025</td>\n",
       "      <td>130.074646</td>\n",
       "      <td>312.434679</td>\n",
       "      <td>620.881838</td>\n",
       "      <td>114.081615</td>\n",
       "      <td>326.655980</td>\n",
       "      <td>215.219510</td>\n",
       "      <td>...</td>\n",
       "      <td>520.247194</td>\n",
       "      <td>418.178387</td>\n",
       "      <td>466.768330</td>\n",
       "      <td>552.271783</td>\n",
       "      <td>438.914756</td>\n",
       "      <td>287.312055</td>\n",
       "      <td>180.595186</td>\n",
       "      <td>410.572202</td>\n",
       "      <td>459.142460</td>\n",
       "      <td>371.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>286.091280</td>\n",
       "      <td>199.862399</td>\n",
       "      <td>265.355729</td>\n",
       "      <td>173.604069</td>\n",
       "      <td>93.745707</td>\n",
       "      <td>196.657463</td>\n",
       "      <td>253.733802</td>\n",
       "      <td>99.551351</td>\n",
       "      <td>143.891431</td>\n",
       "      <td>137.811505</td>\n",
       "      <td>...</td>\n",
       "      <td>307.429948</td>\n",
       "      <td>242.690925</td>\n",
       "      <td>221.037984</td>\n",
       "      <td>263.522635</td>\n",
       "      <td>226.066905</td>\n",
       "      <td>149.633136</td>\n",
       "      <td>117.846971</td>\n",
       "      <td>192.013776</td>\n",
       "      <td>223.947843</td>\n",
       "      <td>212.250247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>336.740122</td>\n",
       "      <td>21.150318</td>\n",
       "      <td>257.353534</td>\n",
       "      <td>0.152396</td>\n",
       "      <td>35.995113</td>\n",
       "      <td>146.090480</td>\n",
       "      <td>393.617875</td>\n",
       "      <td>0.497008</td>\n",
       "      <td>169.357817</td>\n",
       "      <td>104.390113</td>\n",
       "      <td>...</td>\n",
       "      <td>202.390508</td>\n",
       "      <td>230.925056</td>\n",
       "      <td>245.542608</td>\n",
       "      <td>291.913877</td>\n",
       "      <td>237.063207</td>\n",
       "      <td>158.577664</td>\n",
       "      <td>71.509404</td>\n",
       "      <td>237.953428</td>\n",
       "      <td>247.884440</td>\n",
       "      <td>210.482247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>410.332065</td>\n",
       "      <td>158.771288</td>\n",
       "      <td>332.233835</td>\n",
       "      <td>20.021201</td>\n",
       "      <td>79.094940</td>\n",
       "      <td>204.317552</td>\n",
       "      <td>469.745409</td>\n",
       "      <td>63.684747</td>\n",
       "      <td>236.494372</td>\n",
       "      <td>146.301021</td>\n",
       "      <td>...</td>\n",
       "      <td>352.041091</td>\n",
       "      <td>287.216682</td>\n",
       "      <td>339.478272</td>\n",
       "      <td>384.982720</td>\n",
       "      <td>305.145365</td>\n",
       "      <td>200.212154</td>\n",
       "      <td>109.130605</td>\n",
       "      <td>292.101183</td>\n",
       "      <td>314.714729</td>\n",
       "      <td>253.356933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>485.718435</td>\n",
       "      <td>241.455904</td>\n",
       "      <td>386.451650</td>\n",
       "      <td>122.507413</td>\n",
       "      <td>104.840755</td>\n",
       "      <td>240.559336</td>\n",
       "      <td>530.999904</td>\n",
       "      <td>92.559172</td>\n",
       "      <td>273.114090</td>\n",
       "      <td>163.698986</td>\n",
       "      <td>...</td>\n",
       "      <td>404.220756</td>\n",
       "      <td>329.574199</td>\n",
       "      <td>392.496892</td>\n",
       "      <td>459.141288</td>\n",
       "      <td>359.704440</td>\n",
       "      <td>235.196266</td>\n",
       "      <td>136.467067</td>\n",
       "      <td>339.309297</td>\n",
       "      <td>367.066054</td>\n",
       "      <td>296.224312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>645.223982</td>\n",
       "      <td>330.669537</td>\n",
       "      <td>524.913644</td>\n",
       "      <td>202.174512</td>\n",
       "      <td>149.374484</td>\n",
       "      <td>340.166474</td>\n",
       "      <td>654.929093</td>\n",
       "      <td>124.456267</td>\n",
       "      <td>351.386976</td>\n",
       "      <td>218.649802</td>\n",
       "      <td>...</td>\n",
       "      <td>564.388729</td>\n",
       "      <td>428.072956</td>\n",
       "      <td>501.345673</td>\n",
       "      <td>621.606897</td>\n",
       "      <td>464.333773</td>\n",
       "      <td>306.749532</td>\n",
       "      <td>209.058687</td>\n",
       "      <td>455.204775</td>\n",
       "      <td>513.154812</td>\n",
       "      <td>381.400179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1957.846210</td>\n",
       "      <td>1207.689996</td>\n",
       "      <td>1735.690175</td>\n",
       "      <td>983.147059</td>\n",
       "      <td>660.409882</td>\n",
       "      <td>1271.516219</td>\n",
       "      <td>1849.143185</td>\n",
       "      <td>601.685769</td>\n",
       "      <td>936.294368</td>\n",
       "      <td>931.763994</td>\n",
       "      <td>...</td>\n",
       "      <td>2035.097195</td>\n",
       "      <td>1586.692112</td>\n",
       "      <td>1573.637670</td>\n",
       "      <td>1723.167363</td>\n",
       "      <td>1537.294793</td>\n",
       "      <td>1041.672818</td>\n",
       "      <td>756.588113</td>\n",
       "      <td>1267.950940</td>\n",
       "      <td>1479.794424</td>\n",
       "      <td>1432.856096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target_1     target_2     target_3    target_4    target_5  \\\n",
       "count   578.000000   578.000000   578.000000  578.000000  578.000000   \n",
       "mean    586.197709   285.046500   490.772663  156.752025  130.074646   \n",
       "std     286.091280   199.862399   265.355729  173.604069   93.745707   \n",
       "min     336.740122    21.150318   257.353534    0.152396   35.995113   \n",
       "25%     410.332065   158.771288   332.233835   20.021201   79.094940   \n",
       "50%     485.718435   241.455904   386.451650  122.507413  104.840755   \n",
       "75%     645.223982   330.669537   524.913644  202.174512  149.374484   \n",
       "max    1957.846210  1207.689996  1735.690175  983.147059  660.409882   \n",
       "\n",
       "          target_6     target_7    target_8    target_9   target_10  \\\n",
       "count   578.000000   578.000000  578.000000  578.000000  578.000000   \n",
       "mean    312.434679   620.881838  114.081615  326.655980  215.219510   \n",
       "std     196.657463   253.733802   99.551351  143.891431  137.811505   \n",
       "min     146.090480   393.617875    0.497008  169.357817  104.390113   \n",
       "25%     204.317552   469.745409   63.684747  236.494372  146.301021   \n",
       "50%     240.559336   530.999904   92.559172  273.114090  163.698986   \n",
       "75%     340.166474   654.929093  124.456267  351.386976  218.649802   \n",
       "max    1271.516219  1849.143185  601.685769  936.294368  931.763994   \n",
       "\n",
       "          ...         target_37    target_38    target_39    target_40  \\\n",
       "count     ...        578.000000   578.000000   578.000000   578.000000   \n",
       "mean      ...        520.247194   418.178387   466.768330   552.271783   \n",
       "std       ...        307.429948   242.690925   221.037984   263.522635   \n",
       "min       ...        202.390508   230.925056   245.542608   291.913877   \n",
       "25%       ...        352.041091   287.216682   339.478272   384.982720   \n",
       "50%       ...        404.220756   329.574199   392.496892   459.141288   \n",
       "75%       ...        564.388729   428.072956   501.345673   621.606897   \n",
       "max       ...       2035.097195  1586.692112  1573.637670  1723.167363   \n",
       "\n",
       "         target_41    target_42   target_43    target_44    target_45  \\\n",
       "count   578.000000   578.000000  578.000000   578.000000   578.000000   \n",
       "mean    438.914756   287.312055  180.595186   410.572202   459.142460   \n",
       "std     226.066905   149.633136  117.846971   192.013776   223.947843   \n",
       "min     237.063207   158.577664   71.509404   237.953428   247.884440   \n",
       "25%     305.145365   200.212154  109.130605   292.101183   314.714729   \n",
       "50%     359.704440   235.196266  136.467067   339.309297   367.066054   \n",
       "75%     464.333773   306.749532  209.058687   455.204775   513.154812   \n",
       "max    1537.294793  1041.672818  756.588113  1267.950940  1479.794424   \n",
       "\n",
       "         target_46  \n",
       "count   578.000000  \n",
       "mean    371.664700  \n",
       "std     212.250247  \n",
       "min     210.482247  \n",
       "25%     253.356933  \n",
       "50%     296.224312  \n",
       "75%     381.400179  \n",
       "max    1432.856096  \n",
       "\n",
       "[8 rows x 46 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578, 46)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the first target ratio to be 1, the scond value is day2 / day 1, util the last value for each columns\n",
    "def ratio(dataset):\n",
    "    result = []\n",
    "    for target in all_data.columns:\n",
    "        target = dataset[[target]].as_matrix()\n",
    "        ratio = []\n",
    "        ratio.append(1.0)\n",
    "        for i in range(1, len(target)):\n",
    "            value = target[i] / target[i - 1]\n",
    "            ratio.append(value[0])\n",
    "        result.append(ratio)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ratio = ratio(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the ratio data from each column into DataFrame\n",
    "\n",
    "def concatenate(all_ratio):\n",
    "    temp_first = DataFrame(all_ratio[0], columns=['target1'])\n",
    "    i = 1\n",
    "    for ratio in all_ratio[1:]:\n",
    "        i += 1\n",
    "        temp_second = DataFrame(ratio, columns=['target' + str(i)])\n",
    "        temp_first = pd.concat([temp_first, temp_second], axis=1)\n",
    "    return temp_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578, 46)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratio = concatenate(all_ratio)\n",
    "df_ratio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>target4</th>\n",
       "      <th>target5</th>\n",
       "      <th>target6</th>\n",
       "      <th>target7</th>\n",
       "      <th>target8</th>\n",
       "      <th>target9</th>\n",
       "      <th>target10</th>\n",
       "      <th>...</th>\n",
       "      <th>target37</th>\n",
       "      <th>target38</th>\n",
       "      <th>target39</th>\n",
       "      <th>target40</th>\n",
       "      <th>target41</th>\n",
       "      <th>target42</th>\n",
       "      <th>target43</th>\n",
       "      <th>target44</th>\n",
       "      <th>target45</th>\n",
       "      <th>target46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.826394</td>\n",
       "      <td>0.888634</td>\n",
       "      <td>0.864652</td>\n",
       "      <td>3.139475</td>\n",
       "      <td>1.051988</td>\n",
       "      <td>0.943713</td>\n",
       "      <td>0.864763</td>\n",
       "      <td>13.753263</td>\n",
       "      <td>0.854989</td>\n",
       "      <td>1.198118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830544</td>\n",
       "      <td>0.770078</td>\n",
       "      <td>0.820634</td>\n",
       "      <td>1.002500</td>\n",
       "      <td>0.806199</td>\n",
       "      <td>0.879624</td>\n",
       "      <td>0.963427</td>\n",
       "      <td>0.778297</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.938357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998526</td>\n",
       "      <td>0.741477</td>\n",
       "      <td>1.006187</td>\n",
       "      <td>0.318525</td>\n",
       "      <td>0.733691</td>\n",
       "      <td>0.971176</td>\n",
       "      <td>1.037716</td>\n",
       "      <td>0.772569</td>\n",
       "      <td>0.899381</td>\n",
       "      <td>0.996845</td>\n",
       "      <td>...</td>\n",
       "      <td>1.182174</td>\n",
       "      <td>0.993306</td>\n",
       "      <td>0.962442</td>\n",
       "      <td>0.949693</td>\n",
       "      <td>0.934307</td>\n",
       "      <td>1.121141</td>\n",
       "      <td>0.918174</td>\n",
       "      <td>1.001168</td>\n",
       "      <td>0.896114</td>\n",
       "      <td>1.033744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.055618</td>\n",
       "      <td>1.296016</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.064701</td>\n",
       "      <td>0.921003</td>\n",
       "      <td>0.861343</td>\n",
       "      <td>1.698596</td>\n",
       "      <td>0.979964</td>\n",
       "      <td>0.875404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908911</td>\n",
       "      <td>0.867706</td>\n",
       "      <td>0.896915</td>\n",
       "      <td>0.870067</td>\n",
       "      <td>1.043193</td>\n",
       "      <td>0.869966</td>\n",
       "      <td>1.011131</td>\n",
       "      <td>0.978074</td>\n",
       "      <td>1.014328</td>\n",
       "      <td>0.931850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.872902</td>\n",
       "      <td>0.648784</td>\n",
       "      <td>0.910331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.068475</td>\n",
       "      <td>0.871482</td>\n",
       "      <td>0.962521</td>\n",
       "      <td>1.242280</td>\n",
       "      <td>1.013532</td>\n",
       "      <td>0.889460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969832</td>\n",
       "      <td>1.052377</td>\n",
       "      <td>0.963841</td>\n",
       "      <td>0.881911</td>\n",
       "      <td>0.857863</td>\n",
       "      <td>1.015117</td>\n",
       "      <td>0.933118</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>0.990790</td>\n",
       "      <td>0.955933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    target1   target2   target3   target4   target5   target6   target7  \\\n",
       "0  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1  0.826394  0.888634  0.864652  3.139475  1.051988  0.943713  0.864763   \n",
       "2  0.998526  0.741477  1.006187  0.318525  0.733691  0.971176  1.037716   \n",
       "3  1.055618  1.296016  0.998092  1.000000  1.064701  0.921003  0.861343   \n",
       "4  0.872902  0.648784  0.910331  1.000000  1.068475  0.871482  0.962521   \n",
       "\n",
       "     target8   target9  target10    ...     target37  target38  target39  \\\n",
       "0   1.000000  1.000000  1.000000    ...     1.000000  1.000000  1.000000   \n",
       "1  13.753263  0.854989  1.198118    ...     0.830544  0.770078  0.820634   \n",
       "2   0.772569  0.899381  0.996845    ...     1.182174  0.993306  0.962442   \n",
       "3   1.698596  0.979964  0.875404    ...     0.908911  0.867706  0.896915   \n",
       "4   1.242280  1.013532  0.889460    ...     0.969832  1.052377  0.963841   \n",
       "\n",
       "   target40  target41  target42  target43  target44  target45  target46  \n",
       "0  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "1  1.002500  0.806199  0.879624  0.963427  0.778297  0.915625  0.938357  \n",
       "2  0.949693  0.934307  1.121141  0.918174  1.001168  0.896114  1.033744  \n",
       "3  0.870067  1.043193  0.869966  1.011131  0.978074  1.014328  0.931850  \n",
       "4  0.881911  0.857863  1.015117  0.933118  0.929119  0.990790  0.955933  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_matrix = []\n",
    "for column in df_ratio.columns:\n",
    "    temp = df_ratio[column].as_matrix()\n",
    "    target_matrix.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: the 'target_matrix' is the label matrix for all sites, Our Target Data for all the 46 Observation Sites  are ready!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Next We will build multi-task network architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "578/578 [==============================] - 2s - loss: 41.5586 - output1_loss: 0.7680 - output2_loss: 0.5286 - output3_loss: 0.5518 - output4_loss: 1.8510 - output5_loss: 0.9475 - output6_loss: 0.8898 - output7_loss: 0.6873 - output8_loss: 0.8248 - output9_loss: 0.9124 - output10_loss: 0.5130 - output11_loss: 0.8495 - output12_loss: 0.9242 - output13_loss: 0.8408 - output14_loss: 0.6463 - output15_loss: 0.5695 - output16_loss: 1.1200 - output17_loss: 0.5514 - output18_loss: 1.1007 - output19_loss: 0.6420 - output20_loss: 0.8799 - output21_loss: 1.0026 - output22_loss: 0.5080 - output23_loss: 0.7182 - output24_loss: 1.0777 - output25_loss: 0.9078 - output26_loss: 0.9430 - output27_loss: 1.0552 - output28_loss: 0.7856 - output29_loss: 1.0009 - output30_loss: 0.8558 - output31_loss: 0.4946 - output32_loss: 0.8394 - output33_loss: 0.5479 - output34_loss: 4.0981 - output35_loss: 0.7969 - output36_loss: 1.0441 - output37_loss: 0.7984 - output38_loss: 1.2578 - output39_loss: 0.5882 - output40_loss: 1.0424 - output41_loss: 0.4941 - output42_loss: 0.8773 - output43_loss: 0.8134 - output44_loss: 1.2606 - output45_loss: 0.5167 - output46_loss: 0.6350     \n",
      "Epoch 2/100\n",
      "578/578 [==============================] - 0s - loss: 27.6201 - output1_loss: 0.4200 - output2_loss: 0.4213 - output3_loss: 0.4259 - output4_loss: 1.4852 - output5_loss: 0.4996 - output6_loss: 0.4217 - output7_loss: 0.4089 - output8_loss: 0.6128 - output9_loss: 0.4503 - output10_loss: 0.3758 - output11_loss: 0.4291 - output12_loss: 0.4871 - output13_loss: 0.4274 - output14_loss: 0.3933 - output15_loss: 0.3809 - output16_loss: 0.9044 - output17_loss: 0.3933 - output18_loss: 0.7284 - output19_loss: 0.3527 - output20_loss: 0.4685 - output21_loss: 0.6627 - output22_loss: 0.4082 - output23_loss: 0.4618 - output24_loss: 0.7057 - output25_loss: 0.4948 - output26_loss: 0.5506 - output27_loss: 0.7204 - output28_loss: 0.4333 - output29_loss: 0.5669 - output30_loss: 0.4498 - output31_loss: 0.4031 - output32_loss: 0.3732 - output33_loss: 0.3807 - output34_loss: 3.8374 - output35_loss: 0.4702 - output36_loss: 0.5807 - output37_loss: 0.3779 - output38_loss: 1.0223 - output39_loss: 0.4120 - output40_loss: 0.6468 - output41_loss: 0.3594 - output42_loss: 0.5522 - output43_loss: 0.4282 - output44_loss: 1.1100 - output45_loss: 0.3616 - output46_loss: 0.3636     \n",
      "Epoch 3/100\n",
      "578/578 [==============================] - 0s - loss: 21.3463 - output1_loss: 0.3030 - output2_loss: 0.3430 - output3_loss: 0.3467 - output4_loss: 1.3221 - output5_loss: 0.3620 - output6_loss: 0.3321 - output7_loss: 0.3426 - output8_loss: 0.5693 - output9_loss: 0.3811 - output10_loss: 0.3139 - output11_loss: 0.3349 - output12_loss: 0.3587 - output13_loss: 0.3387 - output14_loss: 0.3324 - output15_loss: 0.3096 - output16_loss: 0.5350 - output17_loss: 0.3344 - output18_loss: 0.3805 - output19_loss: 0.3062 - output20_loss: 0.3139 - output21_loss: 0.3414 - output22_loss: 0.3173 - output23_loss: 0.4037 - output24_loss: 0.4319 - output25_loss: 0.3840 - output26_loss: 0.4929 - output27_loss: 0.3673 - output28_loss: 0.3614 - output29_loss: 0.3824 - output30_loss: 0.3556 - output31_loss: 0.3135 - output32_loss: 0.2896 - output33_loss: 0.2905 - output34_loss: 3.7202 - output35_loss: 0.3401 - output36_loss: 0.3400 - output37_loss: 0.3131 - output38_loss: 0.5585 - output39_loss: 0.3661 - output40_loss: 0.3999 - output41_loss: 0.2980 - output42_loss: 0.3759 - output43_loss: 0.3410 - output44_loss: 0.8067 - output45_loss: 0.2856 - output46_loss: 0.3092     \n",
      "Epoch 4/100\n",
      "578/578 [==============================] - 0s - loss: 18.0305 - output1_loss: 0.2471 - output2_loss: 0.3015 - output3_loss: 0.2758 - output4_loss: 1.2781 - output5_loss: 0.2999 - output6_loss: 0.2886 - output7_loss: 0.2833 - output8_loss: 0.5259 - output9_loss: 0.3115 - output10_loss: 0.2569 - output11_loss: 0.2823 - output12_loss: 0.2735 - output13_loss: 0.2829 - output14_loss: 0.2752 - output15_loss: 0.2565 - output16_loss: 0.3142 - output17_loss: 0.2733 - output18_loss: 0.2945 - output19_loss: 0.2621 - output20_loss: 0.2568 - output21_loss: 0.2589 - output22_loss: 0.2634 - output23_loss: 0.3070 - output24_loss: 0.3521 - output25_loss: 0.3127 - output26_loss: 0.4479 - output27_loss: 0.2541 - output28_loss: 0.3054 - output29_loss: 0.3080 - output30_loss: 0.3018 - output31_loss: 0.2577 - output32_loss: 0.2532 - output33_loss: 0.2510 - output34_loss: 3.6957 - output35_loss: 0.2674 - output36_loss: 0.2887 - output37_loss: 0.2605 - output38_loss: 0.3844 - output39_loss: 0.2880 - output40_loss: 0.3153 - output41_loss: 0.2515 - output42_loss: 0.3192 - output43_loss: 0.2975 - output44_loss: 0.4610 - output45_loss: 0.2428 - output46_loss: 0.2456     \n",
      "Epoch 5/100\n",
      "578/578 [==============================] - 0s - loss: 16.1597 - output1_loss: 0.2021 - output2_loss: 0.2690 - output3_loss: 0.2492 - output4_loss: 1.2503 - output5_loss: 0.2692 - output6_loss: 0.2408 - output7_loss: 0.2449 - output8_loss: 0.4929 - output9_loss: 0.2668 - output10_loss: 0.2293 - output11_loss: 0.2606 - output12_loss: 0.2361 - output13_loss: 0.2527 - output14_loss: 0.2578 - output15_loss: 0.2140 - output16_loss: 0.2471 - output17_loss: 0.2342 - output18_loss: 0.2304 - output19_loss: 0.2211 - output20_loss: 0.2385 - output21_loss: 0.2371 - output22_loss: 0.2267 - output23_loss: 0.2547 - output24_loss: 0.3014 - output25_loss: 0.2776 - output26_loss: 0.4167 - output27_loss: 0.2219 - output28_loss: 0.2594 - output29_loss: 0.2600 - output30_loss: 0.2652 - output31_loss: 0.2254 - output32_loss: 0.2199 - output33_loss: 0.2226 - output34_loss: 3.6744 - output35_loss: 0.2492 - output36_loss: 0.2424 - output37_loss: 0.2249 - output38_loss: 0.3101 - output39_loss: 0.2538 - output40_loss: 0.2579 - output41_loss: 0.2258 - output42_loss: 0.2513 - output43_loss: 0.2734 - output44_loss: 0.2514 - output45_loss: 0.2233 - output46_loss: 0.2260     \n",
      "Epoch 6/100\n",
      "578/578 [==============================] - 0s - loss: 14.7740 - output1_loss: 0.1821 - output2_loss: 0.2271 - output3_loss: 0.2237 - output4_loss: 1.2322 - output5_loss: 0.2418 - output6_loss: 0.2159 - output7_loss: 0.2192 - output8_loss: 0.4724 - output9_loss: 0.2490 - output10_loss: 0.1977 - output11_loss: 0.2234 - output12_loss: 0.2165 - output13_loss: 0.2243 - output14_loss: 0.2212 - output15_loss: 0.1831 - output16_loss: 0.2226 - output17_loss: 0.1982 - output18_loss: 0.1893 - output19_loss: 0.1965 - output20_loss: 0.2167 - output21_loss: 0.2200 - output22_loss: 0.1968 - output23_loss: 0.2146 - output24_loss: 0.2721 - output25_loss: 0.2394 - output26_loss: 0.3975 - output27_loss: 0.2124 - output28_loss: 0.2195 - output29_loss: 0.2360 - output30_loss: 0.2225 - output31_loss: 0.1960 - output32_loss: 0.2014 - output33_loss: 0.1911 - output34_loss: 3.6672 - output35_loss: 0.2211 - output36_loss: 0.2147 - output37_loss: 0.1883 - output38_loss: 0.2463 - output39_loss: 0.2033 - output40_loss: 0.2424 - output41_loss: 0.2040 - output42_loss: 0.2041 - output43_loss: 0.2358 - output44_loss: 0.1937 - output45_loss: 0.1858 - output46_loss: 0.1951     \n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 13.5454 - output1_loss: 0.1579 - output2_loss: 0.2060 - output3_loss: 0.1791 - output4_loss: 1.2108 - output5_loss: 0.2254 - output6_loss: 0.1804 - output7_loss: 0.1835 - output8_loss: 0.4683 - output9_loss: 0.2118 - output10_loss: 0.1817 - output11_loss: 0.2002 - output12_loss: 0.1866 - output13_loss: 0.2010 - output14_loss: 0.1946 - output15_loss: 0.1707 - output16_loss: 0.1905 - output17_loss: 0.1784 - output18_loss: 0.1628 - output19_loss: 0.1708 - output20_loss: 0.1872 - output21_loss: 0.1869 - output22_loss: 0.1591 - output23_loss: 0.1722 - output24_loss: 0.2228 - output25_loss: 0.2117 - output26_loss: 0.3774 - output27_loss: 0.1868 - output28_loss: 0.1942 - output29_loss: 0.1964 - output30_loss: 0.1970 - output31_loss: 0.1642 - output32_loss: 0.1757 - output33_loss: 0.1698 - output34_loss: 3.6646 - output35_loss: 0.1911 - output36_loss: 0.1875 - output37_loss: 0.1602 - output38_loss: 0.2265 - output39_loss: 0.1750 - output40_loss: 0.2039 - output41_loss: 0.1720 - output42_loss: 0.1833 - output43_loss: 0.2093 - output44_loss: 0.1789 - output45_loss: 0.1657 - output46_loss: 0.1659     \n",
      "Epoch 8/100\n",
      "578/578 [==============================] - 0s - loss: 12.5663 - output1_loss: 0.1388 - output2_loss: 0.1904 - output3_loss: 0.1566 - output4_loss: 1.2088 - output5_loss: 0.2054 - output6_loss: 0.1630 - output7_loss: 0.1587 - output8_loss: 0.4400 - output9_loss: 0.1833 - output10_loss: 0.1604 - output11_loss: 0.1766 - output12_loss: 0.1676 - output13_loss: 0.1711 - output14_loss: 0.1703 - output15_loss: 0.1441 - output16_loss: 0.1829 - output17_loss: 0.1598 - output18_loss: 0.1369 - output19_loss: 0.1448 - output20_loss: 0.1735 - output21_loss: 0.1551 - output22_loss: 0.1383 - output23_loss: 0.1460 - output24_loss: 0.1905 - output25_loss: 0.1899 - output26_loss: 0.3542 - output27_loss: 0.1608 - output28_loss: 0.1764 - output29_loss: 0.1766 - output30_loss: 0.1776 - output31_loss: 0.1499 - output32_loss: 0.1654 - output33_loss: 0.1459 - output34_loss: 3.6631 - output35_loss: 0.1663 - output36_loss: 0.1566 - output37_loss: 0.1474 - output38_loss: 0.1875 - output39_loss: 0.1439 - output40_loss: 0.1864 - output41_loss: 0.1549 - output42_loss: 0.1594 - output43_loss: 0.1881 - output44_loss: 0.1564 - output45_loss: 0.1477 - output46_loss: 0.1487     \n",
      "Epoch 9/100\n",
      "578/578 [==============================] - 0s - loss: 12.0974 - output1_loss: 0.1275 - output2_loss: 0.1807 - output3_loss: 0.1414 - output4_loss: 1.1985 - output5_loss: 0.1924 - output6_loss: 0.1483 - output7_loss: 0.1520 - output8_loss: 0.4455 - output9_loss: 0.1666 - output10_loss: 0.1540 - output11_loss: 0.1701 - output12_loss: 0.1588 - output13_loss: 0.1639 - output14_loss: 0.1600 - output15_loss: 0.1396 - output16_loss: 0.1680 - output17_loss: 0.1504 - output18_loss: 0.1278 - output19_loss: 0.1327 - output20_loss: 0.1632 - output21_loss: 0.1424 - output22_loss: 0.1225 - output23_loss: 0.1386 - output24_loss: 0.1778 - output25_loss: 0.1772 - output26_loss: 0.3513 - output27_loss: 0.1539 - output28_loss: 0.1597 - output29_loss: 0.1603 - output30_loss: 0.1640 - output31_loss: 0.1460 - output32_loss: 0.1529 - output33_loss: 0.1429 - output34_loss: 3.6509 - output35_loss: 0.1528 - output36_loss: 0.1498 - output37_loss: 0.1367 - output38_loss: 0.1755 - output39_loss: 0.1289 - output40_loss: 0.1588 - output41_loss: 0.1405 - output42_loss: 0.1473 - output43_loss: 0.1968 - output44_loss: 0.1480 - output45_loss: 0.1422 - output46_loss: 0.1380     \n",
      "Epoch 10/100\n",
      "578/578 [==============================] - 0s - loss: 11.2240 - output1_loss: 0.1117 - output2_loss: 0.1581 - output3_loss: 0.1219 - output4_loss: 1.1790 - output5_loss: 0.1780 - output6_loss: 0.1297 - output7_loss: 0.1312 - output8_loss: 0.4416 - output9_loss: 0.1433 - output10_loss: 0.1372 - output11_loss: 0.1444 - output12_loss: 0.1351 - output13_loss: 0.1423 - output14_loss: 0.1329 - output15_loss: 0.1159 - output16_loss: 0.1496 - output17_loss: 0.1244 - output18_loss: 0.1091 - output19_loss: 0.1253 - output20_loss: 0.1415 - output21_loss: 0.1303 - output22_loss: 0.1043 - output23_loss: 0.1204 - output24_loss: 0.1504 - output25_loss: 0.1503 - output26_loss: 0.3447 - output27_loss: 0.1392 - output28_loss: 0.1347 - output29_loss: 0.1438 - output30_loss: 0.1413 - output31_loss: 0.1147 - output32_loss: 0.1367 - output33_loss: 0.1146 - output34_loss: 3.6628 - output35_loss: 0.1335 - output36_loss: 0.1283 - output37_loss: 0.1146 - output38_loss: 0.1486 - output39_loss: 0.1085 - output40_loss: 0.1449 - output41_loss: 0.1324 - output42_loss: 0.1314 - output43_loss: 0.1736 - output44_loss: 0.1260 - output45_loss: 0.1234 - output46_loss: 0.1188     \n",
      "Epoch 11/100\n",
      "578/578 [==============================] - 0s - loss: 10.8168 - output1_loss: 0.1059 - output2_loss: 0.1532 - output3_loss: 0.1055 - output4_loss: 1.1695 - output5_loss: 0.1723 - output6_loss: 0.1246 - output7_loss: 0.1133 - output8_loss: 0.4299 - output9_loss: 0.1379 - output10_loss: 0.1351 - output11_loss: 0.1359 - output12_loss: 0.1282 - output13_loss: 0.1360 - output14_loss: 0.1278 - output15_loss: 0.1126 - output16_loss: 0.1471 - output17_loss: 0.1142 - output18_loss: 0.1031 - output19_loss: 0.1095 - output20_loss: 0.1336 - output21_loss: 0.1126 - output22_loss: 0.0942 - output23_loss: 0.1082 - output24_loss: 0.1421 - output25_loss: 0.1401 - output26_loss: 0.3381 - output27_loss: 0.1286 - output28_loss: 0.1226 - output29_loss: 0.1329 - output30_loss: 0.1333 - output31_loss: 0.1069 - output32_loss: 0.1257 - output33_loss: 0.1075 - output34_loss: 3.6507 - output35_loss: 0.1256 - output36_loss: 0.1199 - output37_loss: 0.1041 - output38_loss: 0.1364 - output39_loss: 0.1067 - output40_loss: 0.1331 - output41_loss: 0.1153 - output42_loss: 0.1205 - output43_loss: 0.1627 - output44_loss: 0.1220 - output45_loss: 0.1206 - output46_loss: 0.1111     \n",
      "Epoch 12/100\n",
      "578/578 [==============================] - 0s - loss: 10.4310 - output1_loss: 0.0960 - output2_loss: 0.1458 - output3_loss: 0.0939 - output4_loss: 1.1674 - output5_loss: 0.1711 - output6_loss: 0.1159 - output7_loss: 0.0980 - output8_loss: 0.4263 - output9_loss: 0.1262 - output10_loss: 0.1285 - output11_loss: 0.1247 - output12_loss: 0.1175 - output13_loss: 0.1208 - output14_loss: 0.1128 - output15_loss: 0.0996 - output16_loss: 0.1335 - output17_loss: 0.1111 - output18_loss: 0.0932 - output19_loss: 0.1050 - output20_loss: 0.1244 - output21_loss: 0.1068 - output22_loss: 0.0894 - output23_loss: 0.1063 - output24_loss: 0.1305 - output25_loss: 0.1308 - output26_loss: 0.3347 - output27_loss: 0.1217 - output28_loss: 0.1178 - output29_loss: 0.1157 - output30_loss: 0.1289 - output31_loss: 0.0980 - output32_loss: 0.1173 - output33_loss: 0.0961 - output34_loss: 3.6469 - output35_loss: 0.1153 - output36_loss: 0.1143 - output37_loss: 0.0955 - output38_loss: 0.1256 - output39_loss: 0.0980 - output40_loss: 0.1201 - output41_loss: 0.1078 - output42_loss: 0.1153 - output43_loss: 0.1578 - output44_loss: 0.1127 - output45_loss: 0.1103 - output46_loss: 0.1058     \n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 10.2431 - output1_loss: 0.0927 - output2_loss: 0.1407 - output3_loss: 0.0954 - output4_loss: 1.1730 - output5_loss: 0.1691 - output6_loss: 0.1092 - output7_loss: 0.0959 - output8_loss: 0.4219 - output9_loss: 0.1215 - output10_loss: 0.1275 - output11_loss: 0.1230 - output12_loss: 0.1149 - output13_loss: 0.1169 - output14_loss: 0.1108 - output15_loss: 0.1007 - output16_loss: 0.1303 - output17_loss: 0.1027 - output18_loss: 0.0908 - output19_loss: 0.1003 - output20_loss: 0.1151 - output21_loss: 0.1030 - output22_loss: 0.0850 - output23_loss: 0.0987 - output24_loss: 0.1201 - output25_loss: 0.1228 - output26_loss: 0.3316 - output27_loss: 0.1156 - output28_loss: 0.1059 - output29_loss: 0.1124 - output30_loss: 0.1213 - output31_loss: 0.0941 - output32_loss: 0.1148 - output33_loss: 0.0966 - output34_loss: 3.6480 - output35_loss: 0.1129 - output36_loss: 0.1132 - output37_loss: 0.0945 - output38_loss: 0.1164 - output39_loss: 0.0923 - output40_loss: 0.1137 - output41_loss: 0.1024 - output42_loss: 0.1110 - output43_loss: 0.1528 - output44_loss: 0.1064 - output45_loss: 0.1038 - output46_loss: 0.1017     \n",
      "Epoch 14/100\n",
      "578/578 [==============================] - 0s - loss: 10.2213 - output1_loss: 0.0894 - output2_loss: 0.1431 - output3_loss: 0.0944 - output4_loss: 1.1706 - output5_loss: 0.1716 - output6_loss: 0.1103 - output7_loss: 0.0908 - output8_loss: 0.4155 - output9_loss: 0.1149 - output10_loss: 0.1243 - output11_loss: 0.1223 - output12_loss: 0.1106 - output13_loss: 0.1158 - output14_loss: 0.1110 - output15_loss: 0.0995 - output16_loss: 0.1354 - output17_loss: 0.1043 - output18_loss: 0.0882 - output19_loss: 0.0986 - output20_loss: 0.1177 - output21_loss: 0.0950 - output22_loss: 0.0848 - output23_loss: 0.1024 - output24_loss: 0.1177 - output25_loss: 0.1284 - output26_loss: 0.3310 - output27_loss: 0.1177 - output28_loss: 0.1059 - output29_loss: 0.1152 - output30_loss: 0.1176 - output31_loss: 0.0942 - output32_loss: 0.1117 - output33_loss: 0.0971 - output34_loss: 3.6572 - output35_loss: 0.1147 - output36_loss: 0.1088 - output37_loss: 0.0932 - output38_loss: 0.1151 - output39_loss: 0.0932 - output40_loss: 0.1093 - output41_loss: 0.1064 - output42_loss: 0.1093 - output43_loss: 0.1574 - output44_loss: 0.1084 - output45_loss: 0.1011 - output46_loss: 0.1003     \n",
      "Epoch 15/100\n",
      "578/578 [==============================] - 0s - loss: 9.8233 - output1_loss: 0.0863 - output2_loss: 0.1332 - output3_loss: 0.0849 - output4_loss: 1.1641 - output5_loss: 0.1604 - output6_loss: 0.1022 - output7_loss: 0.0803 - output8_loss: 0.4125 - output9_loss: 0.1082 - output10_loss: 0.1220 - output11_loss: 0.1173 - output12_loss: 0.1049 - output13_loss: 0.1020 - output14_loss: 0.0980 - output15_loss: 0.0915 - output16_loss: 0.1212 - output17_loss: 0.0946 - output18_loss: 0.0853 - output19_loss: 0.0996 - output20_loss: 0.1118 - output21_loss: 0.0885 - output22_loss: 0.0758 - output23_loss: 0.0939 - output24_loss: 0.1071 - output25_loss: 0.1091 - output26_loss: 0.3222 - output27_loss: 0.1068 - output28_loss: 0.0954 - output29_loss: 0.1059 - output30_loss: 0.1080 - output31_loss: 0.0881 - output32_loss: 0.1055 - output33_loss: 0.0825 - output34_loss: 3.6440 - output35_loss: 0.1044 - output36_loss: 0.1024 - output37_loss: 0.0913 - output38_loss: 0.1026 - output39_loss: 0.0861 - output40_loss: 0.1008 - output41_loss: 0.0940 - output42_loss: 0.1019 - output43_loss: 0.1441 - output44_loss: 0.0962 - output45_loss: 0.0907 - output46_loss: 0.0959     \n",
      "Epoch 16/100\n",
      "578/578 [==============================] - 0s - loss: 9.7333 - output1_loss: 0.0824 - output2_loss: 0.1303 - output3_loss: 0.0866 - output4_loss: 1.1577 - output5_loss: 0.1631 - output6_loss: 0.1027 - output7_loss: 0.0781 - output8_loss: 0.4155 - output9_loss: 0.1022 - output10_loss: 0.1205 - output11_loss: 0.1134 - output12_loss: 0.1010 - output13_loss: 0.1053 - output14_loss: 0.0927 - output15_loss: 0.0891 - output16_loss: 0.1208 - output17_loss: 0.0912 - output18_loss: 0.0810 - output19_loss: 0.0949 - output20_loss: 0.1086 - output21_loss: 0.0842 - output22_loss: 0.0747 - output23_loss: 0.0923 - output24_loss: 0.1027 - output25_loss: 0.1065 - output26_loss: 0.3209 - output27_loss: 0.1104 - output28_loss: 0.0960 - output29_loss: 0.1049 - output30_loss: 0.1045 - output31_loss: 0.0862 - output32_loss: 0.0995 - output33_loss: 0.0830 - output34_loss: 3.6415 - output35_loss: 0.1010 - output36_loss: 0.1032 - output37_loss: 0.0892 - output38_loss: 0.0945 - output39_loss: 0.0815 - output40_loss: 0.0976 - output41_loss: 0.0959 - output42_loss: 0.1028 - output43_loss: 0.1454 - output44_loss: 0.0935 - output45_loss: 0.0927 - output46_loss: 0.0916     \n",
      "Epoch 17/100\n",
      "578/578 [==============================] - 0s - loss: 9.7623 - output1_loss: 0.0817 - output2_loss: 0.1342 - output3_loss: 0.0851 - output4_loss: 1.1584 - output5_loss: 0.1658 - output6_loss: 0.1015 - output7_loss: 0.0793 - output8_loss: 0.4149 - output9_loss: 0.1057 - output10_loss: 0.1206 - output11_loss: 0.1118 - output12_loss: 0.1003 - output13_loss: 0.1077 - output14_loss: 0.0964 - output15_loss: 0.0900 - output16_loss: 0.1219 - output17_loss: 0.0907 - output18_loss: 0.0828 - output19_loss: 0.0938 - output20_loss: 0.1063 - output21_loss: 0.0847 - output22_loss: 0.0739 - output23_loss: 0.0933 - output24_loss: 0.1036 - output25_loss: 0.1047 - output26_loss: 0.3249 - output27_loss: 0.1007 - output28_loss: 0.0939 - output29_loss: 0.1036 - output30_loss: 0.1062 - output31_loss: 0.0879 - output32_loss: 0.1032 - output33_loss: 0.0829 - output34_loss: 3.6466 - output35_loss: 0.1031 - output36_loss: 0.1033 - output37_loss: 0.0889 - output38_loss: 0.0975 - output39_loss: 0.0842 - output40_loss: 0.1010 - output41_loss: 0.0939 - output42_loss: 0.1038 - output43_loss: 0.1490 - output44_loss: 0.0953 - output45_loss: 0.0898 - output46_loss: 0.0938     \n",
      "Epoch 18/100\n",
      "578/578 [==============================] - 0s - loss: 9.7051 - output1_loss: 0.0847 - output2_loss: 0.1355 - output3_loss: 0.0821 - output4_loss: 1.1594 - output5_loss: 0.1617 - output6_loss: 0.0989 - output7_loss: 0.0761 - output8_loss: 0.4152 - output9_loss: 0.1011 - output10_loss: 0.1200 - output11_loss: 0.1119 - output12_loss: 0.0965 - output13_loss: 0.1065 - output14_loss: 0.0904 - output15_loss: 0.0890 - output16_loss: 0.1232 - output17_loss: 0.0883 - output18_loss: 0.0830 - output19_loss: 0.0928 - output20_loss: 0.1095 - output21_loss: 0.0843 - output22_loss: 0.0742 - output23_loss: 0.0925 - output24_loss: 0.1027 - output25_loss: 0.1070 - output26_loss: 0.3193 - output27_loss: 0.1013 - output28_loss: 0.0901 - output29_loss: 0.1013 - output30_loss: 0.1026 - output31_loss: 0.0838 - output32_loss: 0.1029 - output33_loss: 0.0814 - output34_loss: 3.6479 - output35_loss: 0.1005 - output36_loss: 0.1013 - output37_loss: 0.0887 - output38_loss: 0.0940 - output39_loss: 0.0832 - output40_loss: 0.1016 - output41_loss: 0.0935 - output42_loss: 0.1022 - output43_loss: 0.1463 - output44_loss: 0.0970 - output45_loss: 0.0888 - output46_loss: 0.0911     \n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.5133 - output1_loss: 0.0792 - output2_loss: 0.1321 - output3_loss: 0.0774 - output4_loss: 1.1568 - output5_loss: 0.1593 - output6_loss: 0.1000 - output7_loss: 0.0703 - output8_loss: 0.4115 - output9_loss: 0.0950 - output10_loss: 0.1166 - output11_loss: 0.1055 - output12_loss: 0.0942 - output13_loss: 0.0980 - output14_loss: 0.0852 - output15_loss: 0.0861 - output16_loss: 0.1165 - output17_loss: 0.0873 - output18_loss: 0.0778 - output19_loss: 0.0916 - output20_loss: 0.1030 - output21_loss: 0.0766 - output22_loss: 0.0721 - output23_loss: 0.0910 - output24_loss: 0.0902 - output25_loss: 0.1030 - output26_loss: 0.3203 - output27_loss: 0.0979 - output28_loss: 0.0855 - output29_loss: 0.0964 - output30_loss: 0.1045 - output31_loss: 0.0842 - output32_loss: 0.0962 - output33_loss: 0.0753 - output34_loss: 3.6459 - output35_loss: 0.0996 - output36_loss: 0.0963 - output37_loss: 0.0848 - output38_loss: 0.0903 - output39_loss: 0.0811 - output40_loss: 0.0924 - output41_loss: 0.0896 - output42_loss: 0.0961 - output43_loss: 0.1421 - output44_loss: 0.0888 - output45_loss: 0.0817 - output46_loss: 0.0879     \n",
      "Epoch 20/100\n",
      "578/578 [==============================] - 0s - loss: 9.5216 - output1_loss: 0.0788 - output2_loss: 0.1296 - output3_loss: 0.0785 - output4_loss: 1.1562 - output5_loss: 0.1620 - output6_loss: 0.0985 - output7_loss: 0.0712 - output8_loss: 0.4154 - output9_loss: 0.0969 - output10_loss: 0.1207 - output11_loss: 0.1043 - output12_loss: 0.0933 - output13_loss: 0.1025 - output14_loss: 0.0873 - output15_loss: 0.0857 - output16_loss: 0.1164 - output17_loss: 0.0860 - output18_loss: 0.0773 - output19_loss: 0.0927 - output20_loss: 0.1000 - output21_loss: 0.0759 - output22_loss: 0.0704 - output23_loss: 0.0908 - output24_loss: 0.0936 - output25_loss: 0.0987 - output26_loss: 0.3190 - output27_loss: 0.0994 - output28_loss: 0.0866 - output29_loss: 0.0948 - output30_loss: 0.1023 - output31_loss: 0.0844 - output32_loss: 0.0986 - output33_loss: 0.0768 - output34_loss: 3.6483 - output35_loss: 0.0973 - output36_loss: 0.0954 - output37_loss: 0.0843 - output38_loss: 0.0854 - output39_loss: 0.0804 - output40_loss: 0.0904 - output41_loss: 0.0895 - output42_loss: 0.1008 - output43_loss: 0.1409 - output44_loss: 0.0930 - output45_loss: 0.0829 - output46_loss: 0.0883     \n",
      "Epoch 21/100\n",
      "578/578 [==============================] - 0s - loss: 9.4401 - output1_loss: 0.0764 - output2_loss: 0.1313 - output3_loss: 0.0784 - output4_loss: 1.1571 - output5_loss: 0.1576 - output6_loss: 0.0965 - output7_loss: 0.0701 - output8_loss: 0.4113 - output9_loss: 0.0926 - output10_loss: 0.1181 - output11_loss: 0.1095 - output12_loss: 0.0934 - output13_loss: 0.0999 - output14_loss: 0.0847 - output15_loss: 0.0835 - output16_loss: 0.1140 - output17_loss: 0.0856 - output18_loss: 0.0761 - output19_loss: 0.0913 - output20_loss: 0.0973 - output21_loss: 0.0739 - output22_loss: 0.0695 - output23_loss: 0.0886 - output24_loss: 0.0883 - output25_loss: 0.0968 - output26_loss: 0.3188 - output27_loss: 0.0940 - output28_loss: 0.0796 - output29_loss: 0.0957 - output30_loss: 0.0988 - output31_loss: 0.0803 - output32_loss: 0.0946 - output33_loss: 0.0750 - output34_loss: 3.6459 - output35_loss: 0.0938 - output36_loss: 0.0967 - output37_loss: 0.0862 - output38_loss: 0.0867 - output39_loss: 0.0794 - output40_loss: 0.0873 - output41_loss: 0.0865 - output42_loss: 0.0964 - output43_loss: 0.1434 - output44_loss: 0.0886 - output45_loss: 0.0833 - output46_loss: 0.0871     \n",
      "Epoch 22/100\n",
      "578/578 [==============================] - 0s - loss: 9.3472 - output1_loss: 0.0731 - output2_loss: 0.1276 - output3_loss: 0.0778 - output4_loss: 1.1542 - output5_loss: 0.1583 - output6_loss: 0.0961 - output7_loss: 0.0669 - output8_loss: 0.4079 - output9_loss: 0.0879 - output10_loss: 0.1159 - output11_loss: 0.0982 - output12_loss: 0.0885 - output13_loss: 0.0960 - output14_loss: 0.0789 - output15_loss: 0.0826 - output16_loss: 0.1089 - output17_loss: 0.0860 - output18_loss: 0.0745 - output19_loss: 0.0895 - output20_loss: 0.0934 - output21_loss: 0.0719 - output22_loss: 0.0691 - output23_loss: 0.0862 - output24_loss: 0.0851 - output25_loss: 0.0933 - output26_loss: 0.3159 - output27_loss: 0.0947 - output28_loss: 0.0802 - output29_loss: 0.0936 - output30_loss: 0.0989 - output31_loss: 0.0799 - output32_loss: 0.0933 - output33_loss: 0.0732 - output34_loss: 3.6482 - output35_loss: 0.0959 - output36_loss: 0.0964 - output37_loss: 0.0854 - output38_loss: 0.0845 - output39_loss: 0.0800 - output40_loss: 0.0851 - output41_loss: 0.0832 - output42_loss: 0.0960 - output43_loss: 0.1412 - output44_loss: 0.0862 - output45_loss: 0.0815 - output46_loss: 0.0859     \n",
      "Epoch 23/100\n",
      "578/578 [==============================] - 0s - loss: 9.3516 - output1_loss: 0.0773 - output2_loss: 0.1260 - output3_loss: 0.0772 - output4_loss: 1.1520 - output5_loss: 0.1571 - output6_loss: 0.0968 - output7_loss: 0.0672 - output8_loss: 0.4088 - output9_loss: 0.0899 - output10_loss: 0.1175 - output11_loss: 0.1017 - output12_loss: 0.0899 - output13_loss: 0.0962 - output14_loss: 0.0806 - output15_loss: 0.0849 - output16_loss: 0.1107 - output17_loss: 0.0850 - output18_loss: 0.0757 - output19_loss: 0.0905 - output20_loss: 0.0934 - output21_loss: 0.0699 - output22_loss: 0.0691 - output23_loss: 0.0869 - output24_loss: 0.0864 - output25_loss: 0.0944 - output26_loss: 0.3159 - output27_loss: 0.0917 - output28_loss: 0.0793 - output29_loss: 0.0961 - output30_loss: 0.0999 - output31_loss: 0.0812 - output32_loss: 0.0927 - output33_loss: 0.0731 - output34_loss: 3.6434 - output35_loss: 0.0960 - output36_loss: 0.0930 - output37_loss: 0.0838 - output38_loss: 0.0828 - output39_loss: 0.0797 - output40_loss: 0.0836 - output41_loss: 0.0858 - output42_loss: 0.0981 - output43_loss: 0.1394 - output44_loss: 0.0861 - output45_loss: 0.0777 - output46_loss: 0.0872     \n",
      "Epoch 24/100\n",
      "578/578 [==============================] - 0s - loss: 9.3387 - output1_loss: 0.0748 - output2_loss: 0.1286 - output3_loss: 0.0771 - output4_loss: 1.1553 - output5_loss: 0.1564 - output6_loss: 0.0950 - output7_loss: 0.0673 - output8_loss: 0.4095 - output9_loss: 0.0911 - output10_loss: 0.1163 - output11_loss: 0.1010 - output12_loss: 0.0880 - output13_loss: 0.0932 - output14_loss: 0.0793 - output15_loss: 0.0840 - output16_loss: 0.1075 - output17_loss: 0.0860 - output18_loss: 0.0761 - output19_loss: 0.0890 - output20_loss: 0.0945 - output21_loss: 0.0685 - output22_loss: 0.0690 - output23_loss: 0.0873 - output24_loss: 0.0857 - output25_loss: 0.0965 - output26_loss: 0.3149 - output27_loss: 0.0927 - output28_loss: 0.0769 - output29_loss: 0.0961 - output30_loss: 0.1003 - output31_loss: 0.0792 - output32_loss: 0.0920 - output33_loss: 0.0731 - output34_loss: 3.6427 - output35_loss: 0.0971 - output36_loss: 0.0932 - output37_loss: 0.0824 - output38_loss: 0.0845 - output39_loss: 0.0793 - output40_loss: 0.0832 - output41_loss: 0.0835 - output42_loss: 0.0975 - output43_loss: 0.1410 - output44_loss: 0.0868 - output45_loss: 0.0796 - output46_loss: 0.0857     \n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.3163 - output1_loss: 0.0746 - output2_loss: 0.1269 - output3_loss: 0.0758 - output4_loss: 1.1530 - output5_loss: 0.1611 - output6_loss: 0.0965 - output7_loss: 0.0657 - output8_loss: 0.4075 - output9_loss: 0.0904 - output10_loss: 0.1155 - output11_loss: 0.1005 - output12_loss: 0.0874 - output13_loss: 0.0911 - output14_loss: 0.0787 - output15_loss: 0.0841 - output16_loss: 0.1065 - output17_loss: 0.0851 - output18_loss: 0.0750 - output19_loss: 0.0916 - output20_loss: 0.0923 - output21_loss: 0.0703 - output22_loss: 0.0692 - output23_loss: 0.0856 - output24_loss: 0.0828 - output25_loss: 0.0928 - output26_loss: 0.3142 - output27_loss: 0.0920 - output28_loss: 0.0794 - output29_loss: 0.0931 - output30_loss: 0.0983 - output31_loss: 0.0816 - output32_loss: 0.0919 - output33_loss: 0.0728 - output34_loss: 3.6416 - output35_loss: 0.0963 - output36_loss: 0.0953 - output37_loss: 0.0863 - output38_loss: 0.0841 - output39_loss: 0.0785 - output40_loss: 0.0809 - output41_loss: 0.0851 - output42_loss: 0.0944 - output43_loss: 0.1398 - output44_loss: 0.0844 - output45_loss: 0.0793 - output46_loss: 0.0867     \n",
      "Epoch 26/100\n",
      "578/578 [==============================] - 0s - loss: 9.2898 - output1_loss: 0.0754 - output2_loss: 0.1279 - output3_loss: 0.0755 - output4_loss: 1.1472 - output5_loss: 0.1572 - output6_loss: 0.0942 - output7_loss: 0.0655 - output8_loss: 0.4080 - output9_loss: 0.0889 - output10_loss: 0.1132 - output11_loss: 0.1017 - output12_loss: 0.0876 - output13_loss: 0.0956 - output14_loss: 0.0797 - output15_loss: 0.0828 - output16_loss: 0.1056 - output17_loss: 0.0844 - output18_loss: 0.0736 - output19_loss: 0.0897 - output20_loss: 0.0939 - output21_loss: 0.0680 - output22_loss: 0.0683 - output23_loss: 0.0874 - output24_loss: 0.0842 - output25_loss: 0.0913 - output26_loss: 0.3139 - output27_loss: 0.0902 - output28_loss: 0.0772 - output29_loss: 0.0930 - output30_loss: 0.0975 - output31_loss: 0.0793 - output32_loss: 0.0899 - output33_loss: 0.0720 - output34_loss: 3.6448 - output35_loss: 0.0956 - output36_loss: 0.0951 - output37_loss: 0.0831 - output38_loss: 0.0820 - output39_loss: 0.0785 - output40_loss: 0.0820 - output41_loss: 0.0838 - output42_loss: 0.0967 - output43_loss: 0.1386 - output44_loss: 0.0862 - output45_loss: 0.0778 - output46_loss: 0.0857     \n",
      "Epoch 27/100\n",
      "578/578 [==============================] - 0s - loss: 9.2326 - output1_loss: 0.0752 - output2_loss: 0.1279 - output3_loss: 0.0762 - output4_loss: 1.1453 - output5_loss: 0.1558 - output6_loss: 0.0940 - output7_loss: 0.0657 - output8_loss: 0.4063 - output9_loss: 0.0882 - output10_loss: 0.1151 - output11_loss: 0.1007 - output12_loss: 0.0861 - output13_loss: 0.0900 - output14_loss: 0.0792 - output15_loss: 0.0814 - output16_loss: 0.1069 - output17_loss: 0.0839 - output18_loss: 0.0748 - output19_loss: 0.0885 - output20_loss: 0.0896 - output21_loss: 0.0694 - output22_loss: 0.0680 - output23_loss: 0.0850 - output24_loss: 0.0799 - output25_loss: 0.0906 - output26_loss: 0.3144 - output27_loss: 0.0876 - output28_loss: 0.0749 - output29_loss: 0.0911 - output30_loss: 0.0981 - output31_loss: 0.0790 - output32_loss: 0.0886 - output33_loss: 0.0705 - output34_loss: 3.6440 - output35_loss: 0.0949 - output36_loss: 0.0921 - output37_loss: 0.0834 - output38_loss: 0.0794 - output39_loss: 0.0761 - output40_loss: 0.0770 - output41_loss: 0.0815 - output42_loss: 0.0918 - output43_loss: 0.1380 - output44_loss: 0.0839 - output45_loss: 0.0774 - output46_loss: 0.0852     \n",
      "Epoch 28/100\n",
      "578/578 [==============================] - 0s - loss: 9.2158 - output1_loss: 0.0738 - output2_loss: 0.1263 - output3_loss: 0.0763 - output4_loss: 1.1457 - output5_loss: 0.1551 - output6_loss: 0.0945 - output7_loss: 0.0646 - output8_loss: 0.4101 - output9_loss: 0.0864 - output10_loss: 0.1150 - output11_loss: 0.0990 - output12_loss: 0.0856 - output13_loss: 0.0910 - output14_loss: 0.0780 - output15_loss: 0.0832 - output16_loss: 0.1040 - output17_loss: 0.0823 - output18_loss: 0.0742 - output19_loss: 0.0909 - output20_loss: 0.0911 - output21_loss: 0.0665 - output22_loss: 0.0672 - output23_loss: 0.0834 - output24_loss: 0.0793 - output25_loss: 0.0892 - output26_loss: 0.3156 - output27_loss: 0.0883 - output28_loss: 0.0757 - output29_loss: 0.0899 - output30_loss: 0.0955 - output31_loss: 0.0786 - output32_loss: 0.0895 - output33_loss: 0.0713 - output34_loss: 3.6402 - output35_loss: 0.0942 - output36_loss: 0.0913 - output37_loss: 0.0824 - output38_loss: 0.0786 - output39_loss: 0.0765 - output40_loss: 0.0774 - output41_loss: 0.0838 - output42_loss: 0.0927 - output43_loss: 0.1390 - output44_loss: 0.0823 - output45_loss: 0.0769 - output46_loss: 0.0832     \n",
      "Epoch 29/100\n",
      "578/578 [==============================] - 0s - loss: 9.2565 - output1_loss: 0.0744 - output2_loss: 0.1279 - output3_loss: 0.0745 - output4_loss: 1.1482 - output5_loss: 0.1545 - output6_loss: 0.0957 - output7_loss: 0.0649 - output8_loss: 0.4109 - output9_loss: 0.0877 - output10_loss: 0.1150 - output11_loss: 0.0994 - output12_loss: 0.0871 - output13_loss: 0.0919 - output14_loss: 0.0779 - output15_loss: 0.0828 - output16_loss: 0.1066 - output17_loss: 0.0863 - output18_loss: 0.0753 - output19_loss: 0.0886 - output20_loss: 0.0917 - output21_loss: 0.0690 - output22_loss: 0.0688 - output23_loss: 0.0848 - output24_loss: 0.0805 - output25_loss: 0.0906 - output26_loss: 0.3140 - output27_loss: 0.0890 - output28_loss: 0.0752 - output29_loss: 0.0917 - output30_loss: 0.0965 - output31_loss: 0.0800 - output32_loss: 0.0916 - output33_loss: 0.0720 - output34_loss: 3.6406 - output35_loss: 0.0962 - output36_loss: 0.0927 - output37_loss: 0.0851 - output38_loss: 0.0805 - output39_loss: 0.0787 - output40_loss: 0.0778 - output41_loss: 0.0834 - output42_loss: 0.0951 - output43_loss: 0.1376 - output44_loss: 0.0816 - output45_loss: 0.0771 - output46_loss: 0.0850     \n",
      "Epoch 30/100\n",
      "578/578 [==============================] - 0s - loss: 9.2316 - output1_loss: 0.0744 - output2_loss: 0.1259 - output3_loss: 0.0749 - output4_loss: 1.1468 - output5_loss: 0.1566 - output6_loss: 0.0942 - output7_loss: 0.0651 - output8_loss: 0.4107 - output9_loss: 0.0877 - output10_loss: 0.1163 - output11_loss: 0.1007 - output12_loss: 0.0841 - output13_loss: 0.0906 - output14_loss: 0.0794 - output15_loss: 0.0818 - output16_loss: 0.1068 - output17_loss: 0.0836 - output18_loss: 0.0741 - output19_loss: 0.0889 - output20_loss: 0.0890 - output21_loss: 0.0682 - output22_loss: 0.0675 - output23_loss: 0.0859 - output24_loss: 0.0797 - output25_loss: 0.0892 - output26_loss: 0.3156 - output27_loss: 0.0875 - output28_loss: 0.0769 - output29_loss: 0.0907 - output30_loss: 0.0969 - output31_loss: 0.0783 - output32_loss: 0.0889 - output33_loss: 0.0702 - output34_loss: 3.6410 - output35_loss: 0.0936 - output36_loss: 0.0927 - output37_loss: 0.0836 - output38_loss: 0.0796 - output39_loss: 0.0791 - output40_loss: 0.0753 - output41_loss: 0.0823 - output42_loss: 0.0954 - output43_loss: 0.1367 - output44_loss: 0.0825 - output45_loss: 0.0771 - output46_loss: 0.0855     \n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.2219 - output1_loss: 0.0746 - output2_loss: 0.1273 - output3_loss: 0.0755 - output4_loss: 1.1473 - output5_loss: 0.1550 - output6_loss: 0.0958 - output7_loss: 0.0655 - output8_loss: 0.4111 - output9_loss: 0.0867 - output10_loss: 0.1150 - output11_loss: 0.0988 - output12_loss: 0.0859 - output13_loss: 0.0899 - output14_loss: 0.0781 - output15_loss: 0.0826 - output16_loss: 0.1039 - output17_loss: 0.0854 - output18_loss: 0.0743 - output19_loss: 0.0908 - output20_loss: 0.0878 - output21_loss: 0.0677 - output22_loss: 0.0683 - output23_loss: 0.0838 - output24_loss: 0.0797 - output25_loss: 0.0900 - output26_loss: 0.3148 - output27_loss: 0.0862 - output28_loss: 0.0750 - output29_loss: 0.0907 - output30_loss: 0.0978 - output31_loss: 0.0789 - output32_loss: 0.0898 - output33_loss: 0.0700 - output34_loss: 3.6378 - output35_loss: 0.0952 - output36_loss: 0.0912 - output37_loss: 0.0839 - output38_loss: 0.0793 - output39_loss: 0.0781 - output40_loss: 0.0768 - output41_loss: 0.0842 - output42_loss: 0.0941 - output43_loss: 0.1379 - output44_loss: 0.0790 - output45_loss: 0.0762 - output46_loss: 0.0843     \n",
      "Epoch 32/100\n",
      "578/578 [==============================] - 0s - loss: 9.1579 - output1_loss: 0.0714 - output2_loss: 0.1250 - output3_loss: 0.0737 - output4_loss: 1.1460 - output5_loss: 0.1527 - output6_loss: 0.0929 - output7_loss: 0.0621 - output8_loss: 0.4075 - output9_loss: 0.0853 - output10_loss: 0.1142 - output11_loss: 0.0983 - output12_loss: 0.0825 - output13_loss: 0.0872 - output14_loss: 0.0767 - output15_loss: 0.0798 - output16_loss: 0.1034 - output17_loss: 0.0824 - output18_loss: 0.0720 - output19_loss: 0.0887 - output20_loss: 0.0878 - output21_loss: 0.0646 - output22_loss: 0.0674 - output23_loss: 0.0847 - output24_loss: 0.0755 - output25_loss: 0.0874 - output26_loss: 0.3140 - output27_loss: 0.0861 - output28_loss: 0.0732 - output29_loss: 0.0907 - output30_loss: 0.0978 - output31_loss: 0.0793 - output32_loss: 0.0874 - output33_loss: 0.0689 - output34_loss: 3.6423 - output35_loss: 0.0940 - output36_loss: 0.0892 - output37_loss: 0.0827 - output38_loss: 0.0784 - output39_loss: 0.0770 - output40_loss: 0.0740 - output41_loss: 0.0807 - output42_loss: 0.0925 - output43_loss: 0.1380 - output44_loss: 0.0808 - output45_loss: 0.0770 - output46_loss: 0.0846     \n",
      "Epoch 33/100\n",
      "578/578 [==============================] - 0s - loss: 9.2008 - output1_loss: 0.0734 - output2_loss: 0.1250 - output3_loss: 0.0742 - output4_loss: 1.1473 - output5_loss: 0.1548 - output6_loss: 0.0941 - output7_loss: 0.0637 - output8_loss: 0.4089 - output9_loss: 0.0889 - output10_loss: 0.1152 - output11_loss: 0.0979 - output12_loss: 0.0835 - output13_loss: 0.0898 - output14_loss: 0.0776 - output15_loss: 0.0830 - output16_loss: 0.1048 - output17_loss: 0.0830 - output18_loss: 0.0732 - output19_loss: 0.0875 - output20_loss: 0.0886 - output21_loss: 0.0673 - output22_loss: 0.0669 - output23_loss: 0.0837 - output24_loss: 0.0765 - output25_loss: 0.0891 - output26_loss: 0.3145 - output27_loss: 0.0898 - output28_loss: 0.0729 - output29_loss: 0.0928 - output30_loss: 0.0968 - output31_loss: 0.0800 - output32_loss: 0.0894 - output33_loss: 0.0702 - output34_loss: 3.6441 - output35_loss: 0.0940 - output36_loss: 0.0930 - output37_loss: 0.0835 - output38_loss: 0.0789 - output39_loss: 0.0774 - output40_loss: 0.0766 - output41_loss: 0.0809 - output42_loss: 0.0932 - output43_loss: 0.1363 - output44_loss: 0.0791 - output45_loss: 0.0756 - output46_loss: 0.0840     \n",
      "Epoch 34/100\n",
      "578/578 [==============================] - 0s - loss: 9.1633 - output1_loss: 0.0732 - output2_loss: 0.1246 - output3_loss: 0.0736 - output4_loss: 1.1477 - output5_loss: 0.1560 - output6_loss: 0.0936 - output7_loss: 0.0638 - output8_loss: 0.4066 - output9_loss: 0.0864 - output10_loss: 0.1144 - output11_loss: 0.0947 - output12_loss: 0.0842 - output13_loss: 0.0888 - output14_loss: 0.0760 - output15_loss: 0.0814 - output16_loss: 0.1004 - output17_loss: 0.0824 - output18_loss: 0.0726 - output19_loss: 0.0877 - output20_loss: 0.0862 - output21_loss: 0.0657 - output22_loss: 0.0670 - output23_loss: 0.0860 - output24_loss: 0.0789 - output25_loss: 0.0875 - output26_loss: 0.3136 - output27_loss: 0.0858 - output28_loss: 0.0739 - output29_loss: 0.0905 - output30_loss: 0.0973 - output31_loss: 0.0766 - output32_loss: 0.0870 - output33_loss: 0.0696 - output34_loss: 3.6433 - output35_loss: 0.0918 - output36_loss: 0.0914 - output37_loss: 0.0814 - output38_loss: 0.0803 - output39_loss: 0.0764 - output40_loss: 0.0731 - output41_loss: 0.0804 - output42_loss: 0.0933 - output43_loss: 0.1376 - output44_loss: 0.0785 - output45_loss: 0.0762 - output46_loss: 0.0860     \n",
      "Epoch 35/100\n",
      "578/578 [==============================] - 0s - loss: 9.1737 - output1_loss: 0.0741 - output2_loss: 0.1279 - output3_loss: 0.0739 - output4_loss: 1.1460 - output5_loss: 0.1568 - output6_loss: 0.0936 - output7_loss: 0.0635 - output8_loss: 0.4082 - output9_loss: 0.0851 - output10_loss: 0.1163 - output11_loss: 0.0960 - output12_loss: 0.0853 - output13_loss: 0.0893 - output14_loss: 0.0764 - output15_loss: 0.0816 - output16_loss: 0.1023 - output17_loss: 0.0836 - output18_loss: 0.0736 - output19_loss: 0.0871 - output20_loss: 0.0862 - output21_loss: 0.0671 - output22_loss: 0.0679 - output23_loss: 0.0818 - output24_loss: 0.0764 - output25_loss: 0.0876 - output26_loss: 0.3131 - output27_loss: 0.0850 - output28_loss: 0.0741 - output29_loss: 0.0906 - output30_loss: 0.0976 - output31_loss: 0.0786 - output32_loss: 0.0897 - output33_loss: 0.0712 - output34_loss: 3.6367 - output35_loss: 0.0936 - output36_loss: 0.0898 - output37_loss: 0.0826 - output38_loss: 0.0804 - output39_loss: 0.0781 - output40_loss: 0.0758 - output41_loss: 0.0814 - output42_loss: 0.0951 - output43_loss: 0.1355 - output44_loss: 0.0764 - output45_loss: 0.0761 - output46_loss: 0.0848     \n",
      "Epoch 36/100\n",
      "578/578 [==============================] - 0s - loss: 9.1344 - output1_loss: 0.0722 - output2_loss: 0.1255 - output3_loss: 0.0736 - output4_loss: 1.1448 - output5_loss: 0.1523 - output6_loss: 0.0922 - output7_loss: 0.0633 - output8_loss: 0.4074 - output9_loss: 0.0841 - output10_loss: 0.1127 - output11_loss: 0.0944 - output12_loss: 0.0839 - output13_loss: 0.0873 - output14_loss: 0.0760 - output15_loss: 0.0810 - output16_loss: 0.1017 - output17_loss: 0.0826 - output18_loss: 0.0723 - output19_loss: 0.0872 - output20_loss: 0.0866 - output21_loss: 0.0665 - output22_loss: 0.0662 - output23_loss: 0.0843 - output24_loss: 0.0766 - output25_loss: 0.0866 - output26_loss: 0.3133 - output27_loss: 0.0850 - output28_loss: 0.0750 - output29_loss: 0.0910 - output30_loss: 0.0955 - output31_loss: 0.0764 - output32_loss: 0.0859 - output33_loss: 0.0697 - output34_loss: 3.6391 - output35_loss: 0.0924 - output36_loss: 0.0893 - output37_loss: 0.0831 - output38_loss: 0.0790 - output39_loss: 0.0769 - output40_loss: 0.0751 - output41_loss: 0.0810 - output42_loss: 0.0926 - output43_loss: 0.1369 - output44_loss: 0.0777 - output45_loss: 0.0744 - output46_loss: 0.0838     \n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.1166 - output1_loss: 0.0719 - output2_loss: 0.1246 - output3_loss: 0.0733 - output4_loss: 1.1475 - output5_loss: 0.1532 - output6_loss: 0.0929 - output7_loss: 0.0618 - output8_loss: 0.4086 - output9_loss: 0.0828 - output10_loss: 0.1121 - output11_loss: 0.0941 - output12_loss: 0.0826 - output13_loss: 0.0865 - output14_loss: 0.0757 - output15_loss: 0.0806 - output16_loss: 0.0998 - output17_loss: 0.0812 - output18_loss: 0.0707 - output19_loss: 0.0872 - output20_loss: 0.0858 - output21_loss: 0.0648 - output22_loss: 0.0658 - output23_loss: 0.0840 - output24_loss: 0.0761 - output25_loss: 0.0852 - output26_loss: 0.3153 - output27_loss: 0.0839 - output28_loss: 0.0739 - output29_loss: 0.0900 - output30_loss: 0.0952 - output31_loss: 0.0759 - output32_loss: 0.0871 - output33_loss: 0.0681 - output34_loss: 3.6399 - output35_loss: 0.0924 - output36_loss: 0.0907 - output37_loss: 0.0806 - output38_loss: 0.0796 - output39_loss: 0.0759 - output40_loss: 0.0748 - output41_loss: 0.0808 - output42_loss: 0.0919 - output43_loss: 0.1358 - output44_loss: 0.0790 - output45_loss: 0.0747 - output46_loss: 0.0826     \n",
      "Epoch 38/100\n",
      "578/578 [==============================] - 0s - loss: 9.1439 - output1_loss: 0.0728 - output2_loss: 0.1253 - output3_loss: 0.0741 - output4_loss: 1.1448 - output5_loss: 0.1554 - output6_loss: 0.0925 - output7_loss: 0.0633 - output8_loss: 0.4090 - output9_loss: 0.0863 - output10_loss: 0.1127 - output11_loss: 0.0951 - output12_loss: 0.0833 - output13_loss: 0.0882 - output14_loss: 0.0757 - output15_loss: 0.0822 - output16_loss: 0.1004 - output17_loss: 0.0834 - output18_loss: 0.0717 - output19_loss: 0.0874 - output20_loss: 0.0868 - output21_loss: 0.0656 - output22_loss: 0.0660 - output23_loss: 0.0824 - output24_loss: 0.0761 - output25_loss: 0.0885 - output26_loss: 0.3120 - output27_loss: 0.0844 - output28_loss: 0.0741 - output29_loss: 0.0887 - output30_loss: 0.0954 - output31_loss: 0.0775 - output32_loss: 0.0870 - output33_loss: 0.0702 - output34_loss: 3.6434 - output35_loss: 0.0931 - output36_loss: 0.0926 - output37_loss: 0.0814 - output38_loss: 0.0780 - output39_loss: 0.0771 - output40_loss: 0.0746 - output41_loss: 0.0815 - output42_loss: 0.0908 - output43_loss: 0.1353 - output44_loss: 0.0786 - output45_loss: 0.0757 - output46_loss: 0.0836     \n",
      "Epoch 39/100\n",
      "578/578 [==============================] - 0s - loss: 9.1418 - output1_loss: 0.0723 - output2_loss: 0.1259 - output3_loss: 0.0725 - output4_loss: 1.1443 - output5_loss: 0.1538 - output6_loss: 0.0940 - output7_loss: 0.0635 - output8_loss: 0.4061 - output9_loss: 0.0861 - output10_loss: 0.1140 - output11_loss: 0.0963 - output12_loss: 0.0835 - output13_loss: 0.0889 - output14_loss: 0.0753 - output15_loss: 0.0805 - output16_loss: 0.1026 - output17_loss: 0.0815 - output18_loss: 0.0717 - output19_loss: 0.0909 - output20_loss: 0.0858 - output21_loss: 0.0650 - output22_loss: 0.0653 - output23_loss: 0.0842 - output24_loss: 0.0763 - output25_loss: 0.0894 - output26_loss: 0.3138 - output27_loss: 0.0845 - output28_loss: 0.0740 - output29_loss: 0.0890 - output30_loss: 0.0955 - output31_loss: 0.0767 - output32_loss: 0.0872 - output33_loss: 0.0680 - output34_loss: 3.6394 - output35_loss: 0.0937 - output36_loss: 0.0947 - output37_loss: 0.0827 - output38_loss: 0.0775 - output39_loss: 0.0765 - output40_loss: 0.0718 - output41_loss: 0.0803 - output42_loss: 0.0937 - output43_loss: 0.1362 - output44_loss: 0.0791 - output45_loss: 0.0754 - output46_loss: 0.0823     \n",
      "Epoch 40/100\n",
      "578/578 [==============================] - 0s - loss: 9.0816 - output1_loss: 0.0701 - output2_loss: 0.1260 - output3_loss: 0.0721 - output4_loss: 1.1439 - output5_loss: 0.1539 - output6_loss: 0.0910 - output7_loss: 0.0613 - output8_loss: 0.4086 - output9_loss: 0.0832 - output10_loss: 0.1110 - output11_loss: 0.0958 - output12_loss: 0.0833 - output13_loss: 0.0871 - output14_loss: 0.0755 - output15_loss: 0.0797 - output16_loss: 0.0986 - output17_loss: 0.0824 - output18_loss: 0.0697 - output19_loss: 0.0859 - output20_loss: 0.0826 - output21_loss: 0.0648 - output22_loss: 0.0652 - output23_loss: 0.0824 - output24_loss: 0.0747 - output25_loss: 0.0857 - output26_loss: 0.3128 - output27_loss: 0.0837 - output28_loss: 0.0716 - output29_loss: 0.0877 - output30_loss: 0.0941 - output31_loss: 0.0761 - output32_loss: 0.0872 - output33_loss: 0.0681 - output34_loss: 3.6385 - output35_loss: 0.0909 - output36_loss: 0.0898 - output37_loss: 0.0811 - output38_loss: 0.0780 - output39_loss: 0.0767 - output40_loss: 0.0718 - output41_loss: 0.0797 - output42_loss: 0.0927 - output43_loss: 0.1343 - output44_loss: 0.0753 - output45_loss: 0.0759 - output46_loss: 0.0811     \n",
      "Epoch 41/100\n",
      "578/578 [==============================] - 0s - loss: 9.1475 - output1_loss: 0.0732 - output2_loss: 0.1258 - output3_loss: 0.0743 - output4_loss: 1.1417 - output5_loss: 0.1535 - output6_loss: 0.0925 - output7_loss: 0.0639 - output8_loss: 0.4076 - output9_loss: 0.0867 - output10_loss: 0.1140 - output11_loss: 0.0970 - output12_loss: 0.0840 - output13_loss: 0.0884 - output14_loss: 0.0757 - output15_loss: 0.0815 - output16_loss: 0.1024 - output17_loss: 0.0827 - output18_loss: 0.0710 - output19_loss: 0.0886 - output20_loss: 0.0845 - output21_loss: 0.0662 - output22_loss: 0.0651 - output23_loss: 0.0821 - output24_loss: 0.0770 - output25_loss: 0.0876 - output26_loss: 0.3132 - output27_loss: 0.0853 - output28_loss: 0.0735 - output29_loss: 0.0904 - output30_loss: 0.0965 - output31_loss: 0.0781 - output32_loss: 0.0871 - output33_loss: 0.0689 - output34_loss: 3.6453 - output35_loss: 0.0955 - output36_loss: 0.0890 - output37_loss: 0.0820 - output38_loss: 0.0800 - output39_loss: 0.0760 - output40_loss: 0.0740 - output41_loss: 0.0809 - output42_loss: 0.0921 - output43_loss: 0.1359 - output44_loss: 0.0781 - output45_loss: 0.0760 - output46_loss: 0.0832     \n",
      "Epoch 42/100\n",
      "578/578 [==============================] - 0s - loss: 9.1277 - output1_loss: 0.0711 - output2_loss: 0.1269 - output3_loss: 0.0735 - output4_loss: 1.1453 - output5_loss: 0.1539 - output6_loss: 0.0922 - output7_loss: 0.0634 - output8_loss: 0.4076 - output9_loss: 0.0858 - output10_loss: 0.1135 - output11_loss: 0.0956 - output12_loss: 0.0847 - output13_loss: 0.0866 - output14_loss: 0.0756 - output15_loss: 0.0817 - output16_loss: 0.1009 - output17_loss: 0.0825 - output18_loss: 0.0713 - output19_loss: 0.0876 - output20_loss: 0.0860 - output21_loss: 0.0663 - output22_loss: 0.0660 - output23_loss: 0.0840 - output24_loss: 0.0762 - output25_loss: 0.0860 - output26_loss: 0.3147 - output27_loss: 0.0852 - output28_loss: 0.0738 - output29_loss: 0.0893 - output30_loss: 0.0963 - output31_loss: 0.0769 - output32_loss: 0.0864 - output33_loss: 0.0694 - output34_loss: 3.6396 - output35_loss: 0.0922 - output36_loss: 0.0896 - output37_loss: 0.0830 - output38_loss: 0.0768 - output39_loss: 0.0743 - output40_loss: 0.0734 - output41_loss: 0.0797 - output42_loss: 0.0917 - output43_loss: 0.1359 - output44_loss: 0.0782 - output45_loss: 0.0752 - output46_loss: 0.0820     \n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.1104 - output1_loss: 0.0700 - output2_loss: 0.1253 - output3_loss: 0.0727 - output4_loss: 1.1466 - output5_loss: 0.1545 - output6_loss: 0.0930 - output7_loss: 0.0629 - output8_loss: 0.4079 - output9_loss: 0.0848 - output10_loss: 0.1124 - output11_loss: 0.0959 - output12_loss: 0.0831 - output13_loss: 0.0876 - output14_loss: 0.0743 - output15_loss: 0.0828 - output16_loss: 0.1003 - output17_loss: 0.0826 - output18_loss: 0.0705 - output19_loss: 0.0872 - output20_loss: 0.0848 - output21_loss: 0.0648 - output22_loss: 0.0649 - output23_loss: 0.0819 - output24_loss: 0.0747 - output25_loss: 0.0869 - output26_loss: 0.3138 - output27_loss: 0.0851 - output28_loss: 0.0730 - output29_loss: 0.0890 - output30_loss: 0.0972 - output31_loss: 0.0777 - output32_loss: 0.0861 - output33_loss: 0.0671 - output34_loss: 3.6415 - output35_loss: 0.0909 - output36_loss: 0.0891 - output37_loss: 0.0823 - output38_loss: 0.0749 - output39_loss: 0.0754 - output40_loss: 0.0725 - output41_loss: 0.0785 - output42_loss: 0.0913 - output43_loss: 0.1383 - output44_loss: 0.0790 - output45_loss: 0.0736 - output46_loss: 0.0815     \n",
      "Epoch 44/100\n",
      "578/578 [==============================] - 0s - loss: 9.0518 - output1_loss: 0.0689 - output2_loss: 0.1242 - output3_loss: 0.0709 - output4_loss: 1.1434 - output5_loss: 0.1523 - output6_loss: 0.0911 - output7_loss: 0.0614 - output8_loss: 0.4092 - output9_loss: 0.0835 - output10_loss: 0.1122 - output11_loss: 0.0943 - output12_loss: 0.0837 - output13_loss: 0.0863 - output14_loss: 0.0739 - output15_loss: 0.0804 - output16_loss: 0.0987 - output17_loss: 0.0813 - output18_loss: 0.0703 - output19_loss: 0.0836 - output20_loss: 0.0811 - output21_loss: 0.0643 - output22_loss: 0.0644 - output23_loss: 0.0802 - output24_loss: 0.0741 - output25_loss: 0.0843 - output26_loss: 0.3123 - output27_loss: 0.0846 - output28_loss: 0.0715 - output29_loss: 0.0889 - output30_loss: 0.0958 - output31_loss: 0.0742 - output32_loss: 0.0844 - output33_loss: 0.0659 - output34_loss: 3.6400 - output35_loss: 0.0910 - output36_loss: 0.0879 - output37_loss: 0.0803 - output38_loss: 0.0754 - output39_loss: 0.0737 - output40_loss: 0.0705 - output41_loss: 0.0774 - output42_loss: 0.0900 - output43_loss: 0.1359 - output44_loss: 0.0772 - output45_loss: 0.0749 - output46_loss: 0.0818     \n",
      "Epoch 45/100\n",
      "578/578 [==============================] - 0s - loss: 9.0973 - output1_loss: 0.0723 - output2_loss: 0.1244 - output3_loss: 0.0732 - output4_loss: 1.1439 - output5_loss: 0.1515 - output6_loss: 0.0904 - output7_loss: 0.0620 - output8_loss: 0.4081 - output9_loss: 0.0846 - output10_loss: 0.1123 - output11_loss: 0.0945 - output12_loss: 0.0835 - output13_loss: 0.0864 - output14_loss: 0.0742 - output15_loss: 0.0801 - output16_loss: 0.1024 - output17_loss: 0.0829 - output18_loss: 0.0717 - output19_loss: 0.0871 - output20_loss: 0.0832 - output21_loss: 0.0648 - output22_loss: 0.0650 - output23_loss: 0.0834 - output24_loss: 0.0743 - output25_loss: 0.0855 - output26_loss: 0.3136 - output27_loss: 0.0852 - output28_loss: 0.0722 - output29_loss: 0.0885 - output30_loss: 0.0950 - output31_loss: 0.0775 - output32_loss: 0.0866 - output33_loss: 0.0677 - output34_loss: 3.6394 - output35_loss: 0.0923 - output36_loss: 0.0890 - output37_loss: 0.0814 - output38_loss: 0.0792 - output39_loss: 0.0756 - output40_loss: 0.0710 - output41_loss: 0.0798 - output42_loss: 0.0912 - output43_loss: 0.1350 - output44_loss: 0.0776 - output45_loss: 0.0748 - output46_loss: 0.0830     \n",
      "Epoch 46/100\n",
      "578/578 [==============================] - 0s - loss: 9.0576 - output1_loss: 0.0698 - output2_loss: 0.1245 - output3_loss: 0.0705 - output4_loss: 1.1430 - output5_loss: 0.1535 - output6_loss: 0.0914 - output7_loss: 0.0618 - output8_loss: 0.4086 - output9_loss: 0.0844 - output10_loss: 0.1109 - output11_loss: 0.0945 - output12_loss: 0.0819 - output13_loss: 0.0865 - output14_loss: 0.0742 - output15_loss: 0.0800 - output16_loss: 0.0982 - output17_loss: 0.0800 - output18_loss: 0.0702 - output19_loss: 0.0842 - output20_loss: 0.0823 - output21_loss: 0.0629 - output22_loss: 0.0649 - output23_loss: 0.0825 - output24_loss: 0.0743 - output25_loss: 0.0862 - output26_loss: 0.3116 - output27_loss: 0.0850 - output28_loss: 0.0709 - output29_loss: 0.0875 - output30_loss: 0.0955 - output31_loss: 0.0747 - output32_loss: 0.0858 - output33_loss: 0.0672 - output34_loss: 3.6383 - output35_loss: 0.0906 - output36_loss: 0.0884 - output37_loss: 0.0798 - output38_loss: 0.0758 - output39_loss: 0.0743 - output40_loss: 0.0721 - output41_loss: 0.0786 - output42_loss: 0.0921 - output43_loss: 0.1360 - output44_loss: 0.0774 - output45_loss: 0.0746 - output46_loss: 0.0805     \n",
      "Epoch 47/100\n",
      "578/578 [==============================] - 0s - loss: 9.0097 - output1_loss: 0.0669 - output2_loss: 0.1234 - output3_loss: 0.0689 - output4_loss: 1.1447 - output5_loss: 0.1528 - output6_loss: 0.0892 - output7_loss: 0.0593 - output8_loss: 0.4065 - output9_loss: 0.0835 - output10_loss: 0.1123 - output11_loss: 0.0930 - output12_loss: 0.0816 - output13_loss: 0.0841 - output14_loss: 0.0724 - output15_loss: 0.0782 - output16_loss: 0.0986 - output17_loss: 0.0805 - output18_loss: 0.0686 - output19_loss: 0.0837 - output20_loss: 0.0812 - output21_loss: 0.0611 - output22_loss: 0.0641 - output23_loss: 0.0812 - output24_loss: 0.0718 - output25_loss: 0.0846 - output26_loss: 0.3119 - output27_loss: 0.0834 - output28_loss: 0.0690 - output29_loss: 0.0893 - output30_loss: 0.0945 - output31_loss: 0.0744 - output32_loss: 0.0839 - output33_loss: 0.0656 - output34_loss: 3.6432 - output35_loss: 0.0916 - output36_loss: 0.0861 - output37_loss: 0.0790 - output38_loss: 0.0761 - output39_loss: 0.0721 - output40_loss: 0.0703 - output41_loss: 0.0762 - output42_loss: 0.0901 - output43_loss: 0.1345 - output44_loss: 0.0751 - output45_loss: 0.0728 - output46_loss: 0.0784     \n",
      "Epoch 48/100\n",
      "578/578 [==============================] - 0s - loss: 9.0437 - output1_loss: 0.0704 - output2_loss: 0.1238 - output3_loss: 0.0712 - output4_loss: 1.1435 - output5_loss: 0.1528 - output6_loss: 0.0895 - output7_loss: 0.0618 - output8_loss: 0.4088 - output9_loss: 0.0832 - output10_loss: 0.1133 - output11_loss: 0.0941 - output12_loss: 0.0836 - output13_loss: 0.0848 - output14_loss: 0.0738 - output15_loss: 0.0797 - output16_loss: 0.0992 - output17_loss: 0.0833 - output18_loss: 0.0687 - output19_loss: 0.0849 - output20_loss: 0.0799 - output21_loss: 0.0626 - output22_loss: 0.0628 - output23_loss: 0.0792 - output24_loss: 0.0728 - output25_loss: 0.0850 - output26_loss: 0.3114 - output27_loss: 0.0827 - output28_loss: 0.0687 - output29_loss: 0.0868 - output30_loss: 0.0951 - output31_loss: 0.0769 - output32_loss: 0.0847 - output33_loss: 0.0661 - output34_loss: 3.6446 - output35_loss: 0.0902 - output36_loss: 0.0872 - output37_loss: 0.0788 - output38_loss: 0.0771 - output39_loss: 0.0732 - output40_loss: 0.0697 - output41_loss: 0.0786 - output42_loss: 0.0921 - output43_loss: 0.1344 - output44_loss: 0.0751 - output45_loss: 0.0747 - output46_loss: 0.0828     \n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.0157 - output1_loss: 0.0691 - output2_loss: 0.1243 - output3_loss: 0.0715 - output4_loss: 1.1444 - output5_loss: 0.1522 - output6_loss: 0.0889 - output7_loss: 0.0606 - output8_loss: 0.4063 - output9_loss: 0.0828 - output10_loss: 0.1122 - output11_loss: 0.0930 - output12_loss: 0.0805 - output13_loss: 0.0859 - output14_loss: 0.0728 - output15_loss: 0.0795 - output16_loss: 0.0976 - output17_loss: 0.0799 - output18_loss: 0.0677 - output19_loss: 0.0854 - output20_loss: 0.0809 - output21_loss: 0.0618 - output22_loss: 0.0646 - output23_loss: 0.0798 - output24_loss: 0.0712 - output25_loss: 0.0829 - output26_loss: 0.3151 - output27_loss: 0.0836 - output28_loss: 0.0695 - output29_loss: 0.0878 - output30_loss: 0.0948 - output31_loss: 0.0750 - output32_loss: 0.0835 - output33_loss: 0.0652 - output34_loss: 3.6398 - output35_loss: 0.0896 - output36_loss: 0.0843 - output37_loss: 0.0793 - output38_loss: 0.0754 - output39_loss: 0.0736 - output40_loss: 0.0693 - output41_loss: 0.0788 - output42_loss: 0.0907 - output43_loss: 0.1336 - output44_loss: 0.0759 - output45_loss: 0.0728 - output46_loss: 0.0822     \n",
      "Epoch 50/100\n",
      "578/578 [==============================] - 0s - loss: 9.0106 - output1_loss: 0.0692 - output2_loss: 0.1227 - output3_loss: 0.0700 - output4_loss: 1.1419 - output5_loss: 0.1516 - output6_loss: 0.0896 - output7_loss: 0.0609 - output8_loss: 0.4052 - output9_loss: 0.0825 - output10_loss: 0.1117 - output11_loss: 0.0928 - output12_loss: 0.0813 - output13_loss: 0.0844 - output14_loss: 0.0728 - output15_loss: 0.0794 - output16_loss: 0.0982 - output17_loss: 0.0817 - output18_loss: 0.0685 - output19_loss: 0.0840 - output20_loss: 0.0806 - output21_loss: 0.0614 - output22_loss: 0.0633 - output23_loss: 0.0798 - output24_loss: 0.0728 - output25_loss: 0.0854 - output26_loss: 0.3115 - output27_loss: 0.0821 - output28_loss: 0.0703 - output29_loss: 0.0858 - output30_loss: 0.0943 - output31_loss: 0.0734 - output32_loss: 0.0842 - output33_loss: 0.0661 - output34_loss: 3.6432 - output35_loss: 0.0897 - output36_loss: 0.0870 - output37_loss: 0.0806 - output38_loss: 0.0762 - output39_loss: 0.0721 - output40_loss: 0.0700 - output41_loss: 0.0774 - output42_loss: 0.0896 - output43_loss: 0.1350 - output44_loss: 0.0766 - output45_loss: 0.0742 - output46_loss: 0.0795     \n",
      "Epoch 51/100\n",
      "578/578 [==============================] - 0s - loss: 9.0113 - output1_loss: 0.0691 - output2_loss: 0.1234 - output3_loss: 0.0693 - output4_loss: 1.1455 - output5_loss: 0.1500 - output6_loss: 0.0903 - output7_loss: 0.0603 - output8_loss: 0.4051 - output9_loss: 0.0831 - output10_loss: 0.1113 - output11_loss: 0.0919 - output12_loss: 0.0831 - output13_loss: 0.0852 - output14_loss: 0.0723 - output15_loss: 0.0790 - output16_loss: 0.0983 - output17_loss: 0.0813 - output18_loss: 0.0692 - output19_loss: 0.0834 - output20_loss: 0.0834 - output21_loss: 0.0618 - output22_loss: 0.0643 - output23_loss: 0.0805 - output24_loss: 0.0718 - output25_loss: 0.0836 - output26_loss: 0.3117 - output27_loss: 0.0818 - output28_loss: 0.0708 - output29_loss: 0.0882 - output30_loss: 0.0960 - output31_loss: 0.0738 - output32_loss: 0.0856 - output33_loss: 0.0655 - output34_loss: 3.6377 - output35_loss: 0.0892 - output36_loss: 0.0858 - output37_loss: 0.0792 - output38_loss: 0.0772 - output39_loss: 0.0725 - output40_loss: 0.0702 - output41_loss: 0.0773 - output42_loss: 0.0905 - output43_loss: 0.1342 - output44_loss: 0.0752 - output45_loss: 0.0737 - output46_loss: 0.0790     \n",
      "Epoch 52/100\n",
      "578/578 [==============================] - 0s - loss: 9.0497 - output1_loss: 0.0688 - output2_loss: 0.1244 - output3_loss: 0.0712 - output4_loss: 1.1424 - output5_loss: 0.1536 - output6_loss: 0.0920 - output7_loss: 0.0613 - output8_loss: 0.4099 - output9_loss: 0.0847 - output10_loss: 0.1141 - output11_loss: 0.0939 - output12_loss: 0.0826 - output13_loss: 0.0863 - output14_loss: 0.0739 - output15_loss: 0.0797 - output16_loss: 0.0991 - output17_loss: 0.0815 - output18_loss: 0.0697 - output19_loss: 0.0863 - output20_loss: 0.0816 - output21_loss: 0.0627 - output22_loss: 0.0639 - output23_loss: 0.0809 - output24_loss: 0.0732 - output25_loss: 0.0836 - output26_loss: 0.3131 - output27_loss: 0.0820 - output28_loss: 0.0689 - output29_loss: 0.0877 - output30_loss: 0.0950 - output31_loss: 0.0743 - output32_loss: 0.0846 - output33_loss: 0.0668 - output34_loss: 3.6416 - output35_loss: 0.0912 - output36_loss: 0.0902 - output37_loss: 0.0806 - output38_loss: 0.0759 - output39_loss: 0.0723 - output40_loss: 0.0701 - output41_loss: 0.0783 - output42_loss: 0.0912 - output43_loss: 0.1338 - output44_loss: 0.0753 - output45_loss: 0.0744 - output46_loss: 0.0813     \n",
      "Epoch 53/100\n",
      "578/578 [==============================] - 0s - loss: 9.0314 - output1_loss: 0.0696 - output2_loss: 0.1251 - output3_loss: 0.0698 - output4_loss: 1.1452 - output5_loss: 0.1528 - output6_loss: 0.0891 - output7_loss: 0.0608 - output8_loss: 0.4084 - output9_loss: 0.0838 - output10_loss: 0.1128 - output11_loss: 0.0928 - output12_loss: 0.0803 - output13_loss: 0.0858 - output14_loss: 0.0713 - output15_loss: 0.0793 - output16_loss: 0.0984 - output17_loss: 0.0820 - output18_loss: 0.0686 - output19_loss: 0.0833 - output20_loss: 0.0803 - output21_loss: 0.0629 - output22_loss: 0.0654 - output23_loss: 0.0799 - output24_loss: 0.0742 - output25_loss: 0.0850 - output26_loss: 0.3119 - output27_loss: 0.0830 - output28_loss: 0.0707 - output29_loss: 0.0864 - output30_loss: 0.0949 - output31_loss: 0.0738 - output32_loss: 0.0843 - output33_loss: 0.0655 - output34_loss: 3.6393 - output35_loss: 0.0898 - output36_loss: 0.0881 - output37_loss: 0.0778 - output38_loss: 0.0761 - output39_loss: 0.0731 - output40_loss: 0.0702 - output41_loss: 0.0792 - output42_loss: 0.0922 - output43_loss: 0.1354 - output44_loss: 0.0768 - output45_loss: 0.0754 - output46_loss: 0.0809     \n",
      "Epoch 54/100\n",
      "578/578 [==============================] - 0s - loss: 9.0088 - output1_loss: 0.0696 - output2_loss: 0.1242 - output3_loss: 0.0694 - output4_loss: 1.1428 - output5_loss: 0.1535 - output6_loss: 0.0901 - output7_loss: 0.0612 - output8_loss: 0.4057 - output9_loss: 0.0824 - output10_loss: 0.1138 - output11_loss: 0.0926 - output12_loss: 0.0816 - output13_loss: 0.0858 - output14_loss: 0.0734 - output15_loss: 0.0794 - output16_loss: 0.0979 - output17_loss: 0.0818 - output18_loss: 0.0670 - output19_loss: 0.0849 - output20_loss: 0.0791 - output21_loss: 0.0614 - output22_loss: 0.0620 - output23_loss: 0.0799 - output24_loss: 0.0731 - output25_loss: 0.0845 - output26_loss: 0.3111 - output27_loss: 0.0812 - output28_loss: 0.0687 - output29_loss: 0.0892 - output30_loss: 0.0933 - output31_loss: 0.0743 - output32_loss: 0.0825 - output33_loss: 0.0657 - output34_loss: 3.6416 - output35_loss: 0.0901 - output36_loss: 0.0857 - output37_loss: 0.0785 - output38_loss: 0.0761 - output39_loss: 0.0739 - output40_loss: 0.0704 - output41_loss: 0.0772 - output42_loss: 0.0900 - output43_loss: 0.1339 - output44_loss: 0.0746 - output45_loss: 0.0739 - output46_loss: 0.0802     \n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.0203 - output1_loss: 0.0688 - output2_loss: 0.1240 - output3_loss: 0.0694 - output4_loss: 1.1431 - output5_loss: 0.1516 - output6_loss: 0.0917 - output7_loss: 0.0613 - output8_loss: 0.4085 - output9_loss: 0.0823 - output10_loss: 0.1110 - output11_loss: 0.0932 - output12_loss: 0.0800 - output13_loss: 0.0847 - output14_loss: 0.0724 - output15_loss: 0.0800 - output16_loss: 0.0989 - output17_loss: 0.0798 - output18_loss: 0.0686 - output19_loss: 0.0859 - output20_loss: 0.0805 - output21_loss: 0.0629 - output22_loss: 0.0636 - output23_loss: 0.0811 - output24_loss: 0.0730 - output25_loss: 0.0847 - output26_loss: 0.3115 - output27_loss: 0.0817 - output28_loss: 0.0683 - output29_loss: 0.0864 - output30_loss: 0.0944 - output31_loss: 0.0753 - output32_loss: 0.0842 - output33_loss: 0.0646 - output34_loss: 3.6421 - output35_loss: 0.0897 - output36_loss: 0.0880 - output37_loss: 0.0803 - output38_loss: 0.0770 - output39_loss: 0.0725 - output40_loss: 0.0704 - output41_loss: 0.0787 - output42_loss: 0.0910 - output43_loss: 0.1343 - output44_loss: 0.0751 - output45_loss: 0.0734 - output46_loss: 0.0806     \n",
      "Epoch 56/100\n",
      "578/578 [==============================] - 0s - loss: 8.9891 - output1_loss: 0.0695 - output2_loss: 0.1230 - output3_loss: 0.0690 - output4_loss: 1.1419 - output5_loss: 0.1530 - output6_loss: 0.0894 - output7_loss: 0.0596 - output8_loss: 0.4072 - output9_loss: 0.0820 - output10_loss: 0.1093 - output11_loss: 0.0931 - output12_loss: 0.0805 - output13_loss: 0.0852 - output14_loss: 0.0717 - output15_loss: 0.0788 - output16_loss: 0.0968 - output17_loss: 0.0817 - output18_loss: 0.0682 - output19_loss: 0.0819 - output20_loss: 0.0797 - output21_loss: 0.0613 - output22_loss: 0.0628 - output23_loss: 0.0813 - output24_loss: 0.0726 - output25_loss: 0.0836 - output26_loss: 0.3120 - output27_loss: 0.0817 - output28_loss: 0.0693 - output29_loss: 0.0866 - output30_loss: 0.0940 - output31_loss: 0.0745 - output32_loss: 0.0837 - output33_loss: 0.0645 - output34_loss: 3.6398 - output35_loss: 0.0905 - output36_loss: 0.0869 - output37_loss: 0.0776 - output38_loss: 0.0757 - output39_loss: 0.0714 - output40_loss: 0.0678 - output41_loss: 0.0777 - output42_loss: 0.0909 - output43_loss: 0.1346 - output44_loss: 0.0757 - output45_loss: 0.0722 - output46_loss: 0.0787     \n",
      "Epoch 57/100\n",
      "578/578 [==============================] - 0s - loss: 8.9563 - output1_loss: 0.0670 - output2_loss: 0.1227 - output3_loss: 0.0692 - output4_loss: 1.1417 - output5_loss: 0.1527 - output6_loss: 0.0884 - output7_loss: 0.0592 - output8_loss: 0.4073 - output9_loss: 0.0803 - output10_loss: 0.1099 - output11_loss: 0.0925 - output12_loss: 0.0790 - output13_loss: 0.0848 - output14_loss: 0.0723 - output15_loss: 0.0784 - output16_loss: 0.0961 - output17_loss: 0.0796 - output18_loss: 0.0673 - output19_loss: 0.0828 - output20_loss: 0.0799 - output21_loss: 0.0611 - output22_loss: 0.0624 - output23_loss: 0.0782 - output24_loss: 0.0717 - output25_loss: 0.0828 - output26_loss: 0.3118 - output27_loss: 0.0817 - output28_loss: 0.0680 - output29_loss: 0.0871 - output30_loss: 0.0937 - output31_loss: 0.0726 - output32_loss: 0.0823 - output33_loss: 0.0631 - output34_loss: 3.6393 - output35_loss: 0.0888 - output36_loss: 0.0862 - output37_loss: 0.0791 - output38_loss: 0.0741 - output39_loss: 0.0711 - output40_loss: 0.0682 - output41_loss: 0.0751 - output42_loss: 0.0891 - output43_loss: 0.1332 - output44_loss: 0.0752 - output45_loss: 0.0713 - output46_loss: 0.0782     \n",
      "Epoch 58/100\n",
      "578/578 [==============================] - 0s - loss: 8.9848 - output1_loss: 0.0681 - output2_loss: 0.1256 - output3_loss: 0.0696 - output4_loss: 1.1432 - output5_loss: 0.1531 - output6_loss: 0.0886 - output7_loss: 0.0602 - output8_loss: 0.4066 - output9_loss: 0.0819 - output10_loss: 0.1115 - output11_loss: 0.0930 - output12_loss: 0.0803 - output13_loss: 0.0845 - output14_loss: 0.0732 - output15_loss: 0.0778 - output16_loss: 0.0972 - output17_loss: 0.0807 - output18_loss: 0.0688 - output19_loss: 0.0830 - output20_loss: 0.0795 - output21_loss: 0.0617 - output22_loss: 0.0645 - output23_loss: 0.0807 - output24_loss: 0.0721 - output25_loss: 0.0826 - output26_loss: 0.3117 - output27_loss: 0.0807 - output28_loss: 0.0679 - output29_loss: 0.0858 - output30_loss: 0.0937 - output31_loss: 0.0740 - output32_loss: 0.0842 - output33_loss: 0.0643 - output34_loss: 3.6376 - output35_loss: 0.0900 - output36_loss: 0.0882 - output37_loss: 0.0787 - output38_loss: 0.0731 - output39_loss: 0.0712 - output40_loss: 0.0692 - output41_loss: 0.0765 - output42_loss: 0.0893 - output43_loss: 0.1337 - output44_loss: 0.0747 - output45_loss: 0.0718 - output46_loss: 0.0804     \n",
      "Epoch 59/100\n",
      "578/578 [==============================] - 0s - loss: 8.9829 - output1_loss: 0.0701 - output2_loss: 0.1212 - output3_loss: 0.0700 - output4_loss: 1.1422 - output5_loss: 0.1500 - output6_loss: 0.0893 - output7_loss: 0.0607 - output8_loss: 0.4087 - output9_loss: 0.0839 - output10_loss: 0.1095 - output11_loss: 0.0935 - output12_loss: 0.0801 - output13_loss: 0.0841 - output14_loss: 0.0708 - output15_loss: 0.0780 - output16_loss: 0.0978 - output17_loss: 0.0805 - output18_loss: 0.0683 - output19_loss: 0.0836 - output20_loss: 0.0791 - output21_loss: 0.0612 - output22_loss: 0.0630 - output23_loss: 0.0811 - output24_loss: 0.0717 - output25_loss: 0.0818 - output26_loss: 0.3107 - output27_loss: 0.0821 - output28_loss: 0.0683 - output29_loss: 0.0873 - output30_loss: 0.0925 - output31_loss: 0.0758 - output32_loss: 0.0852 - output33_loss: 0.0644 - output34_loss: 3.6404 - output35_loss: 0.0886 - output36_loss: 0.0866 - output37_loss: 0.0791 - output38_loss: 0.0754 - output39_loss: 0.0718 - output40_loss: 0.0676 - output41_loss: 0.0767 - output42_loss: 0.0889 - output43_loss: 0.1344 - output44_loss: 0.0758 - output45_loss: 0.0719 - output46_loss: 0.0793     \n",
      "Epoch 60/100\n",
      "578/578 [==============================] - 0s - loss: 8.9242 - output1_loss: 0.0669 - output2_loss: 0.1204 - output3_loss: 0.0676 - output4_loss: 1.1413 - output5_loss: 0.1483 - output6_loss: 0.0882 - output7_loss: 0.0579 - output8_loss: 0.4072 - output9_loss: 0.0814 - output10_loss: 0.1102 - output11_loss: 0.0902 - output12_loss: 0.0781 - output13_loss: 0.0815 - output14_loss: 0.0697 - output15_loss: 0.0781 - output16_loss: 0.0959 - output17_loss: 0.0810 - output18_loss: 0.0645 - output19_loss: 0.0832 - output20_loss: 0.0787 - output21_loss: 0.0592 - output22_loss: 0.0608 - output23_loss: 0.0794 - output24_loss: 0.0711 - output25_loss: 0.0830 - output26_loss: 0.3110 - output27_loss: 0.0814 - output28_loss: 0.0662 - output29_loss: 0.0888 - output30_loss: 0.0927 - output31_loss: 0.0726 - output32_loss: 0.0816 - output33_loss: 0.0620 - output34_loss: 3.6419 - output35_loss: 0.0860 - output36_loss: 0.0845 - output37_loss: 0.0771 - output38_loss: 0.0738 - output39_loss: 0.0716 - output40_loss: 0.0671 - output41_loss: 0.0769 - output42_loss: 0.0882 - output43_loss: 0.1323 - output44_loss: 0.0754 - output45_loss: 0.0716 - output46_loss: 0.0772     \n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 9.0112 - output1_loss: 0.0676 - output2_loss: 0.1237 - output3_loss: 0.0703 - output4_loss: 1.1426 - output5_loss: 0.1520 - output6_loss: 0.0880 - output7_loss: 0.0600 - output8_loss: 0.4072 - output9_loss: 0.0831 - output10_loss: 0.1102 - output11_loss: 0.0923 - output12_loss: 0.0820 - output13_loss: 0.0837 - output14_loss: 0.0735 - output15_loss: 0.0796 - output16_loss: 0.0984 - output17_loss: 0.0818 - output18_loss: 0.0685 - output19_loss: 0.0843 - output20_loss: 0.0811 - output21_loss: 0.0618 - output22_loss: 0.0626 - output23_loss: 0.0818 - output24_loss: 0.0728 - output25_loss: 0.0834 - output26_loss: 0.3135 - output27_loss: 0.0832 - output28_loss: 0.0686 - output29_loss: 0.0872 - output30_loss: 0.0955 - output31_loss: 0.0747 - output32_loss: 0.0845 - output33_loss: 0.0659 - output34_loss: 3.6389 - output35_loss: 0.0893 - output36_loss: 0.0880 - output37_loss: 0.0787 - output38_loss: 0.0775 - output39_loss: 0.0715 - output40_loss: 0.0687 - output41_loss: 0.0785 - output42_loss: 0.0890 - output43_loss: 0.1349 - output44_loss: 0.0766 - output45_loss: 0.0741 - output46_loss: 0.0805     \n",
      "Epoch 62/100\n",
      "578/578 [==============================] - 0s - loss: 8.9890 - output1_loss: 0.0679 - output2_loss: 0.1246 - output3_loss: 0.0691 - output4_loss: 1.1406 - output5_loss: 0.1523 - output6_loss: 0.0893 - output7_loss: 0.0608 - output8_loss: 0.4068 - output9_loss: 0.0830 - output10_loss: 0.1129 - output11_loss: 0.0921 - output12_loss: 0.0799 - output13_loss: 0.0833 - output14_loss: 0.0725 - output15_loss: 0.0792 - output16_loss: 0.0956 - output17_loss: 0.0816 - output18_loss: 0.0677 - output19_loss: 0.0830 - output20_loss: 0.0804 - output21_loss: 0.0617 - output22_loss: 0.0630 - output23_loss: 0.0802 - output24_loss: 0.0714 - output25_loss: 0.0845 - output26_loss: 0.3141 - output27_loss: 0.0822 - output28_loss: 0.0675 - output29_loss: 0.0858 - output30_loss: 0.0941 - output31_loss: 0.0751 - output32_loss: 0.0828 - output33_loss: 0.0642 - output34_loss: 3.6405 - output35_loss: 0.0886 - output36_loss: 0.0865 - output37_loss: 0.0775 - output38_loss: 0.0769 - output39_loss: 0.0715 - output40_loss: 0.0689 - output41_loss: 0.0761 - output42_loss: 0.0916 - output43_loss: 0.1337 - output44_loss: 0.0750 - output45_loss: 0.0733 - output46_loss: 0.0796     \n",
      "Epoch 63/100\n",
      "578/578 [==============================] - 0s - loss: 9.0088 - output1_loss: 0.0674 - output2_loss: 0.1244 - output3_loss: 0.0706 - output4_loss: 1.1432 - output5_loss: 0.1522 - output6_loss: 0.0899 - output7_loss: 0.0593 - output8_loss: 0.4057 - output9_loss: 0.0831 - output10_loss: 0.1130 - output11_loss: 0.0916 - output12_loss: 0.0808 - output13_loss: 0.0834 - output14_loss: 0.0727 - output15_loss: 0.0785 - output16_loss: 0.0962 - output17_loss: 0.0824 - output18_loss: 0.0696 - output19_loss: 0.0846 - output20_loss: 0.0816 - output21_loss: 0.0633 - output22_loss: 0.0641 - output23_loss: 0.0821 - output24_loss: 0.0724 - output25_loss: 0.0844 - output26_loss: 0.3108 - output27_loss: 0.0828 - output28_loss: 0.0688 - output29_loss: 0.0872 - output30_loss: 0.0929 - output31_loss: 0.0736 - output32_loss: 0.0862 - output33_loss: 0.0646 - output34_loss: 3.6395 - output35_loss: 0.0912 - output36_loss: 0.0863 - output37_loss: 0.0804 - output38_loss: 0.0757 - output39_loss: 0.0734 - output40_loss: 0.0703 - output41_loss: 0.0780 - output42_loss: 0.0888 - output43_loss: 0.1345 - output44_loss: 0.0769 - output45_loss: 0.0726 - output46_loss: 0.0781     \n",
      "Epoch 64/100\n",
      "578/578 [==============================] - 0s - loss: 8.9679 - output1_loss: 0.0683 - output2_loss: 0.1235 - output3_loss: 0.0695 - output4_loss: 1.1421 - output5_loss: 0.1526 - output6_loss: 0.0881 - output7_loss: 0.0592 - output8_loss: 0.4093 - output9_loss: 0.0817 - output10_loss: 0.1111 - output11_loss: 0.0930 - output12_loss: 0.0800 - output13_loss: 0.0846 - output14_loss: 0.0711 - output15_loss: 0.0778 - output16_loss: 0.0987 - output17_loss: 0.0812 - output18_loss: 0.0668 - output19_loss: 0.0821 - output20_loss: 0.0791 - output21_loss: 0.0628 - output22_loss: 0.0616 - output23_loss: 0.0820 - output24_loss: 0.0703 - output25_loss: 0.0835 - output26_loss: 0.3109 - output27_loss: 0.0805 - output28_loss: 0.0668 - output29_loss: 0.0847 - output30_loss: 0.0946 - output31_loss: 0.0737 - output32_loss: 0.0846 - output33_loss: 0.0640 - output34_loss: 3.6398 - output35_loss: 0.0881 - output36_loss: 0.0855 - output37_loss: 0.0774 - output38_loss: 0.0739 - output39_loss: 0.0701 - output40_loss: 0.0683 - output41_loss: 0.0763 - output42_loss: 0.0899 - output43_loss: 0.1335 - output44_loss: 0.0745 - output45_loss: 0.0728 - output46_loss: 0.0783     \n",
      "Epoch 65/100\n",
      "578/578 [==============================] - 0s - loss: 8.9925 - output1_loss: 0.0679 - output2_loss: 0.1257 - output3_loss: 0.0714 - output4_loss: 1.1407 - output5_loss: 0.1520 - output6_loss: 0.0884 - output7_loss: 0.0598 - output8_loss: 0.4070 - output9_loss: 0.0822 - output10_loss: 0.1116 - output11_loss: 0.0934 - output12_loss: 0.0813 - output13_loss: 0.0840 - output14_loss: 0.0725 - output15_loss: 0.0780 - output16_loss: 0.0988 - output17_loss: 0.0811 - output18_loss: 0.0675 - output19_loss: 0.0835 - output20_loss: 0.0812 - output21_loss: 0.0613 - output22_loss: 0.0627 - output23_loss: 0.0814 - output24_loss: 0.0726 - output25_loss: 0.0842 - output26_loss: 0.3126 - output27_loss: 0.0819 - output28_loss: 0.0689 - output29_loss: 0.0864 - output30_loss: 0.0922 - output31_loss: 0.0733 - output32_loss: 0.0841 - output33_loss: 0.0651 - output34_loss: 3.6370 - output35_loss: 0.0915 - output36_loss: 0.0875 - output37_loss: 0.0784 - output38_loss: 0.0754 - output39_loss: 0.0720 - output40_loss: 0.0693 - output41_loss: 0.0775 - output42_loss: 0.0885 - output43_loss: 0.1331 - output44_loss: 0.0748 - output45_loss: 0.0722 - output46_loss: 0.0804     \n",
      "Epoch 66/100\n",
      "578/578 [==============================] - 0s - loss: 8.9444 - output1_loss: 0.0683 - output2_loss: 0.1227 - output3_loss: 0.0694 - output4_loss: 1.1427 - output5_loss: 0.1509 - output6_loss: 0.0880 - output7_loss: 0.0589 - output8_loss: 0.4062 - output9_loss: 0.0820 - output10_loss: 0.1121 - output11_loss: 0.0919 - output12_loss: 0.0798 - output13_loss: 0.0846 - output14_loss: 0.0706 - output15_loss: 0.0786 - output16_loss: 0.0949 - output17_loss: 0.0802 - output18_loss: 0.0668 - output19_loss: 0.0833 - output20_loss: 0.0783 - output21_loss: 0.0601 - output22_loss: 0.0618 - output23_loss: 0.0803 - output24_loss: 0.0716 - output25_loss: 0.0817 - output26_loss: 0.3110 - output27_loss: 0.0797 - output28_loss: 0.0658 - output29_loss: 0.0862 - output30_loss: 0.0930 - output31_loss: 0.0718 - output32_loss: 0.0830 - output33_loss: 0.0633 - output34_loss: 3.6371 - output35_loss: 0.0895 - output36_loss: 0.0847 - output37_loss: 0.0773 - output38_loss: 0.0741 - output39_loss: 0.0710 - output40_loss: 0.0674 - output41_loss: 0.0765 - output42_loss: 0.0900 - output43_loss: 0.1329 - output44_loss: 0.0748 - output45_loss: 0.0713 - output46_loss: 0.0783     \n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.9308 - output1_loss: 0.0667 - output2_loss: 0.1216 - output3_loss: 0.0676 - output4_loss: 1.1414 - output5_loss: 0.1520 - output6_loss: 0.0880 - output7_loss: 0.0584 - output8_loss: 0.4051 - output9_loss: 0.0811 - output10_loss: 0.1100 - output11_loss: 0.0930 - output12_loss: 0.0782 - output13_loss: 0.0827 - output14_loss: 0.0697 - output15_loss: 0.0776 - output16_loss: 0.0962 - output17_loss: 0.0782 - output18_loss: 0.0661 - output19_loss: 0.0814 - output20_loss: 0.0792 - output21_loss: 0.0605 - output22_loss: 0.0622 - output23_loss: 0.0787 - output24_loss: 0.0709 - output25_loss: 0.0832 - output26_loss: 0.3114 - output27_loss: 0.0813 - output28_loss: 0.0672 - output29_loss: 0.0863 - output30_loss: 0.0925 - output31_loss: 0.0720 - output32_loss: 0.0823 - output33_loss: 0.0635 - output34_loss: 3.6394 - output35_loss: 0.0879 - output36_loss: 0.0852 - output37_loss: 0.0773 - output38_loss: 0.0741 - output39_loss: 0.0705 - output40_loss: 0.0683 - output41_loss: 0.0765 - output42_loss: 0.0908 - output43_loss: 0.1322 - output44_loss: 0.0738 - output45_loss: 0.0706 - output46_loss: 0.0780     \n",
      "Epoch 68/100\n",
      "578/578 [==============================] - 0s - loss: 8.9490 - output1_loss: 0.0688 - output2_loss: 0.1237 - output3_loss: 0.0683 - output4_loss: 1.1423 - output5_loss: 0.1499 - output6_loss: 0.0887 - output7_loss: 0.0588 - output8_loss: 0.4073 - output9_loss: 0.0801 - output10_loss: 0.1099 - output11_loss: 0.0910 - output12_loss: 0.0802 - output13_loss: 0.0833 - output14_loss: 0.0711 - output15_loss: 0.0784 - output16_loss: 0.0955 - output17_loss: 0.0797 - output18_loss: 0.0669 - output19_loss: 0.0831 - output20_loss: 0.0794 - output21_loss: 0.0609 - output22_loss: 0.0630 - output23_loss: 0.0802 - output24_loss: 0.0713 - output25_loss: 0.0828 - output26_loss: 0.3101 - output27_loss: 0.0821 - output28_loss: 0.0673 - output29_loss: 0.0853 - output30_loss: 0.0921 - output31_loss: 0.0724 - output32_loss: 0.0829 - output33_loss: 0.0636 - output34_loss: 3.6407 - output35_loss: 0.0886 - output36_loss: 0.0858 - output37_loss: 0.0775 - output38_loss: 0.0735 - output39_loss: 0.0688 - output40_loss: 0.0686 - output41_loss: 0.0768 - output42_loss: 0.0912 - output43_loss: 0.1341 - output44_loss: 0.0738 - output45_loss: 0.0711 - output46_loss: 0.0778     \n",
      "Epoch 69/100\n",
      "578/578 [==============================] - 0s - loss: 8.9142 - output1_loss: 0.0689 - output2_loss: 0.1230 - output3_loss: 0.0673 - output4_loss: 1.1423 - output5_loss: 0.1515 - output6_loss: 0.0875 - output7_loss: 0.0595 - output8_loss: 0.4073 - output9_loss: 0.0803 - output10_loss: 0.1093 - output11_loss: 0.0901 - output12_loss: 0.0781 - output13_loss: 0.0837 - output14_loss: 0.0712 - output15_loss: 0.0767 - output16_loss: 0.0954 - output17_loss: 0.0797 - output18_loss: 0.0655 - output19_loss: 0.0819 - output20_loss: 0.0768 - output21_loss: 0.0583 - output22_loss: 0.0635 - output23_loss: 0.0794 - output24_loss: 0.0702 - output25_loss: 0.0812 - output26_loss: 0.3100 - output27_loss: 0.0784 - output28_loss: 0.0658 - output29_loss: 0.0878 - output30_loss: 0.0920 - output31_loss: 0.0712 - output32_loss: 0.0810 - output33_loss: 0.0614 - output34_loss: 3.6401 - output35_loss: 0.0865 - output36_loss: 0.0855 - output37_loss: 0.0767 - output38_loss: 0.0728 - output39_loss: 0.0715 - output40_loss: 0.0667 - output41_loss: 0.0756 - output42_loss: 0.0872 - output43_loss: 0.1335 - output44_loss: 0.0736 - output45_loss: 0.0705 - output46_loss: 0.0780     \n",
      "Epoch 70/100\n",
      "578/578 [==============================] - 0s - loss: 8.9649 - output1_loss: 0.0685 - output2_loss: 0.1229 - output3_loss: 0.0692 - output4_loss: 1.1431 - output5_loss: 0.1497 - output6_loss: 0.0907 - output7_loss: 0.0601 - output8_loss: 0.4086 - output9_loss: 0.0814 - output10_loss: 0.1102 - output11_loss: 0.0909 - output12_loss: 0.0794 - output13_loss: 0.0828 - output14_loss: 0.0720 - output15_loss: 0.0797 - output16_loss: 0.0967 - output17_loss: 0.0807 - output18_loss: 0.0666 - output19_loss: 0.0822 - output20_loss: 0.0786 - output21_loss: 0.0603 - output22_loss: 0.0627 - output23_loss: 0.0813 - output24_loss: 0.0721 - output25_loss: 0.0858 - output26_loss: 0.3115 - output27_loss: 0.0800 - output28_loss: 0.0678 - output29_loss: 0.0841 - output30_loss: 0.0929 - output31_loss: 0.0744 - output32_loss: 0.0828 - output33_loss: 0.0642 - output34_loss: 3.6390 - output35_loss: 0.0886 - output36_loss: 0.0856 - output37_loss: 0.0783 - output38_loss: 0.0747 - output39_loss: 0.0723 - output40_loss: 0.0681 - output41_loss: 0.0763 - output42_loss: 0.0892 - output43_loss: 0.1331 - output44_loss: 0.0755 - output45_loss: 0.0708 - output46_loss: 0.0796     \n",
      "Epoch 71/100\n",
      "578/578 [==============================] - 0s - loss: 8.9184 - output1_loss: 0.0663 - output2_loss: 0.1212 - output3_loss: 0.0691 - output4_loss: 1.1411 - output5_loss: 0.1510 - output6_loss: 0.0867 - output7_loss: 0.0592 - output8_loss: 0.4063 - output9_loss: 0.0818 - output10_loss: 0.1092 - output11_loss: 0.0896 - output12_loss: 0.0791 - output13_loss: 0.0821 - output14_loss: 0.0698 - output15_loss: 0.0772 - output16_loss: 0.0949 - output17_loss: 0.0802 - output18_loss: 0.0655 - output19_loss: 0.0830 - output20_loss: 0.0779 - output21_loss: 0.0598 - output22_loss: 0.0616 - output23_loss: 0.0794 - output24_loss: 0.0705 - output25_loss: 0.0814 - output26_loss: 0.3112 - output27_loss: 0.0790 - output28_loss: 0.0669 - output29_loss: 0.0885 - output30_loss: 0.0918 - output31_loss: 0.0727 - output32_loss: 0.0838 - output33_loss: 0.0618 - output34_loss: 3.6385 - output35_loss: 0.0861 - output36_loss: 0.0846 - output37_loss: 0.0755 - output38_loss: 0.0756 - output39_loss: 0.0696 - output40_loss: 0.0681 - output41_loss: 0.0759 - output42_loss: 0.0873 - output43_loss: 0.1334 - output44_loss: 0.0751 - output45_loss: 0.0704 - output46_loss: 0.0785     \n",
      "Epoch 72/100\n",
      "578/578 [==============================] - 0s - loss: 8.9084 - output1_loss: 0.0650 - output2_loss: 0.1208 - output3_loss: 0.0678 - output4_loss: 1.1425 - output5_loss: 0.1514 - output6_loss: 0.0875 - output7_loss: 0.0573 - output8_loss: 0.4057 - output9_loss: 0.0811 - output10_loss: 0.1104 - output11_loss: 0.0899 - output12_loss: 0.0766 - output13_loss: 0.0814 - output14_loss: 0.0710 - output15_loss: 0.0767 - output16_loss: 0.0944 - output17_loss: 0.0798 - output18_loss: 0.0654 - output19_loss: 0.0826 - output20_loss: 0.0802 - output21_loss: 0.0600 - output22_loss: 0.0597 - output23_loss: 0.0798 - output24_loss: 0.0722 - output25_loss: 0.0814 - output26_loss: 0.3117 - output27_loss: 0.0813 - output28_loss: 0.0657 - output29_loss: 0.0859 - output30_loss: 0.0925 - output31_loss: 0.0709 - output32_loss: 0.0822 - output33_loss: 0.0621 - output34_loss: 3.6387 - output35_loss: 0.0873 - output36_loss: 0.0835 - output37_loss: 0.0753 - output38_loss: 0.0737 - output39_loss: 0.0703 - output40_loss: 0.0667 - output41_loss: 0.0765 - output42_loss: 0.0884 - output43_loss: 0.1316 - output44_loss: 0.0744 - output45_loss: 0.0722 - output46_loss: 0.0772     \n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.9276 - output1_loss: 0.0674 - output2_loss: 0.1223 - output3_loss: 0.0683 - output4_loss: 1.1408 - output5_loss: 0.1537 - output6_loss: 0.0882 - output7_loss: 0.0580 - output8_loss: 0.4056 - output9_loss: 0.0805 - output10_loss: 0.1121 - output11_loss: 0.0914 - output12_loss: 0.0787 - output13_loss: 0.0831 - output14_loss: 0.0712 - output15_loss: 0.0770 - output16_loss: 0.0964 - output17_loss: 0.0798 - output18_loss: 0.0648 - output19_loss: 0.0816 - output20_loss: 0.0770 - output21_loss: 0.0599 - output22_loss: 0.0630 - output23_loss: 0.0800 - output24_loss: 0.0713 - output25_loss: 0.0800 - output26_loss: 0.3106 - output27_loss: 0.0795 - output28_loss: 0.0685 - output29_loss: 0.0866 - output30_loss: 0.0947 - output31_loss: 0.0715 - output32_loss: 0.0809 - output33_loss: 0.0622 - output34_loss: 3.6376 - output35_loss: 0.0890 - output36_loss: 0.0850 - output37_loss: 0.0755 - output38_loss: 0.0743 - output39_loss: 0.0707 - output40_loss: 0.0668 - output41_loss: 0.0762 - output42_loss: 0.0896 - output43_loss: 0.1331 - output44_loss: 0.0745 - output45_loss: 0.0696 - output46_loss: 0.0792     \n",
      "Epoch 74/100\n",
      "578/578 [==============================] - 0s - loss: 8.9334 - output1_loss: 0.0673 - output2_loss: 0.1209 - output3_loss: 0.0690 - output4_loss: 1.1397 - output5_loss: 0.1515 - output6_loss: 0.0866 - output7_loss: 0.0579 - output8_loss: 0.4072 - output9_loss: 0.0831 - output10_loss: 0.1093 - output11_loss: 0.0907 - output12_loss: 0.0784 - output13_loss: 0.0839 - output14_loss: 0.0701 - output15_loss: 0.0775 - output16_loss: 0.0968 - output17_loss: 0.0794 - output18_loss: 0.0671 - output19_loss: 0.0812 - output20_loss: 0.0788 - output21_loss: 0.0611 - output22_loss: 0.0624 - output23_loss: 0.0791 - output24_loss: 0.0704 - output25_loss: 0.0854 - output26_loss: 0.3090 - output27_loss: 0.0808 - output28_loss: 0.0670 - output29_loss: 0.0864 - output30_loss: 0.0936 - output31_loss: 0.0722 - output32_loss: 0.0823 - output33_loss: 0.0638 - output34_loss: 3.6399 - output35_loss: 0.0876 - output36_loss: 0.0870 - output37_loss: 0.0760 - output38_loss: 0.0755 - output39_loss: 0.0681 - output40_loss: 0.0678 - output41_loss: 0.0771 - output42_loss: 0.0886 - output43_loss: 0.1323 - output44_loss: 0.0751 - output45_loss: 0.0713 - output46_loss: 0.0773     \n",
      "Epoch 75/100\n",
      "578/578 [==============================] - 0s - loss: 8.9175 - output1_loss: 0.0649 - output2_loss: 0.1237 - output3_loss: 0.0677 - output4_loss: 1.1418 - output5_loss: 0.1515 - output6_loss: 0.0857 - output7_loss: 0.0584 - output8_loss: 0.4060 - output9_loss: 0.0813 - output10_loss: 0.1111 - output11_loss: 0.0912 - output12_loss: 0.0773 - output13_loss: 0.0801 - output14_loss: 0.0709 - output15_loss: 0.0771 - output16_loss: 0.0935 - output17_loss: 0.0804 - output18_loss: 0.0657 - output19_loss: 0.0808 - output20_loss: 0.0780 - output21_loss: 0.0593 - output22_loss: 0.0613 - output23_loss: 0.0780 - output24_loss: 0.0695 - output25_loss: 0.0834 - output26_loss: 0.3134 - output27_loss: 0.0798 - output28_loss: 0.0676 - output29_loss: 0.0861 - output30_loss: 0.0932 - output31_loss: 0.0719 - output32_loss: 0.0825 - output33_loss: 0.0624 - output34_loss: 3.6385 - output35_loss: 0.0881 - output36_loss: 0.0857 - output37_loss: 0.0773 - output38_loss: 0.0737 - output39_loss: 0.0678 - output40_loss: 0.0669 - output41_loss: 0.0752 - output42_loss: 0.0909 - output43_loss: 0.1341 - output44_loss: 0.0764 - output45_loss: 0.0695 - output46_loss: 0.0779     \n",
      "Epoch 76/100\n",
      "578/578 [==============================] - 0s - loss: 8.9271 - output1_loss: 0.0663 - output2_loss: 0.1220 - output3_loss: 0.0678 - output4_loss: 1.1414 - output5_loss: 0.1506 - output6_loss: 0.0867 - output7_loss: 0.0577 - output8_loss: 0.4067 - output9_loss: 0.0798 - output10_loss: 0.1088 - output11_loss: 0.0908 - output12_loss: 0.0788 - output13_loss: 0.0811 - output14_loss: 0.0718 - output15_loss: 0.0773 - output16_loss: 0.0941 - output17_loss: 0.0789 - output18_loss: 0.0662 - output19_loss: 0.0829 - output20_loss: 0.0771 - output21_loss: 0.0592 - output22_loss: 0.0642 - output23_loss: 0.0792 - output24_loss: 0.0701 - output25_loss: 0.0843 - output26_loss: 0.3113 - output27_loss: 0.0819 - output28_loss: 0.0665 - output29_loss: 0.0863 - output30_loss: 0.0922 - output31_loss: 0.0735 - output32_loss: 0.0810 - output33_loss: 0.0626 - output34_loss: 3.6390 - output35_loss: 0.0881 - output36_loss: 0.0864 - output37_loss: 0.0790 - output38_loss: 0.0738 - output39_loss: 0.0710 - output40_loss: 0.0691 - output41_loss: 0.0762 - output42_loss: 0.0887 - output43_loss: 0.1331 - output44_loss: 0.0756 - output45_loss: 0.0713 - output46_loss: 0.0767     \n",
      "Epoch 77/100\n",
      "578/578 [==============================] - 0s - loss: 8.8974 - output1_loss: 0.0655 - output2_loss: 0.1222 - output3_loss: 0.0682 - output4_loss: 1.1397 - output5_loss: 0.1506 - output6_loss: 0.0888 - output7_loss: 0.0572 - output8_loss: 0.4070 - output9_loss: 0.0785 - output10_loss: 0.1086 - output11_loss: 0.0905 - output12_loss: 0.0775 - output13_loss: 0.0815 - output14_loss: 0.0703 - output15_loss: 0.0770 - output16_loss: 0.0938 - output17_loss: 0.0785 - output18_loss: 0.0664 - output19_loss: 0.0813 - output20_loss: 0.0794 - output21_loss: 0.0594 - output22_loss: 0.0624 - output23_loss: 0.0802 - output24_loss: 0.0702 - output25_loss: 0.0814 - output26_loss: 0.3115 - output27_loss: 0.0789 - output28_loss: 0.0667 - output29_loss: 0.0843 - output30_loss: 0.0921 - output31_loss: 0.0720 - output32_loss: 0.0821 - output33_loss: 0.0631 - output34_loss: 3.6382 - output35_loss: 0.0892 - output36_loss: 0.0820 - output37_loss: 0.0772 - output38_loss: 0.0727 - output39_loss: 0.0690 - output40_loss: 0.0667 - output41_loss: 0.0739 - output42_loss: 0.0891 - output43_loss: 0.1327 - output44_loss: 0.0737 - output45_loss: 0.0697 - output46_loss: 0.0763     \n",
      "Epoch 78/100\n",
      "578/578 [==============================] - 0s - loss: 8.8925 - output1_loss: 0.0654 - output2_loss: 0.1211 - output3_loss: 0.0681 - output4_loss: 1.1414 - output5_loss: 0.1514 - output6_loss: 0.0863 - output7_loss: 0.0575 - output8_loss: 0.4062 - output9_loss: 0.0798 - output10_loss: 0.1086 - output11_loss: 0.0909 - output12_loss: 0.0786 - output13_loss: 0.0842 - output14_loss: 0.0683 - output15_loss: 0.0773 - output16_loss: 0.0959 - output17_loss: 0.0782 - output18_loss: 0.0656 - output19_loss: 0.0816 - output20_loss: 0.0774 - output21_loss: 0.0594 - output22_loss: 0.0602 - output23_loss: 0.0785 - output24_loss: 0.0697 - output25_loss: 0.0824 - output26_loss: 0.3105 - output27_loss: 0.0801 - output28_loss: 0.0652 - output29_loss: 0.0852 - output30_loss: 0.0920 - output31_loss: 0.0709 - output32_loss: 0.0810 - output33_loss: 0.0620 - output34_loss: 3.6392 - output35_loss: 0.0887 - output36_loss: 0.0838 - output37_loss: 0.0753 - output38_loss: 0.0728 - output39_loss: 0.0691 - output40_loss: 0.0674 - output41_loss: 0.0750 - output42_loss: 0.0891 - output43_loss: 0.1327 - output44_loss: 0.0730 - output45_loss: 0.0698 - output46_loss: 0.0756     \n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.8978 - output1_loss: 0.0653 - output2_loss: 0.1232 - output3_loss: 0.0672 - output4_loss: 1.1415 - output5_loss: 0.1521 - output6_loss: 0.0863 - output7_loss: 0.0579 - output8_loss: 0.4066 - output9_loss: 0.0817 - output10_loss: 0.1085 - output11_loss: 0.0910 - output12_loss: 0.0777 - output13_loss: 0.0832 - output14_loss: 0.0699 - output15_loss: 0.0771 - output16_loss: 0.0941 - output17_loss: 0.0801 - output18_loss: 0.0653 - output19_loss: 0.0802 - output20_loss: 0.0772 - output21_loss: 0.0592 - output22_loss: 0.0609 - output23_loss: 0.0777 - output24_loss: 0.0693 - output25_loss: 0.0808 - output26_loss: 0.3108 - output27_loss: 0.0790 - output28_loss: 0.0665 - output29_loss: 0.0871 - output30_loss: 0.0930 - output31_loss: 0.0711 - output32_loss: 0.0807 - output33_loss: 0.0620 - output34_loss: 3.6385 - output35_loss: 0.0882 - output36_loss: 0.0863 - output37_loss: 0.0761 - output38_loss: 0.0725 - output39_loss: 0.0686 - output40_loss: 0.0684 - output41_loss: 0.0737 - output42_loss: 0.0885 - output43_loss: 0.1329 - output44_loss: 0.0737 - output45_loss: 0.0689 - output46_loss: 0.0771     \n",
      "Epoch 80/100\n",
      "578/578 [==============================] - 0s - loss: 8.9284 - output1_loss: 0.0653 - output2_loss: 0.1221 - output3_loss: 0.0672 - output4_loss: 1.1427 - output5_loss: 0.1524 - output6_loss: 0.0865 - output7_loss: 0.0586 - output8_loss: 0.4061 - output9_loss: 0.0807 - output10_loss: 0.1107 - output11_loss: 0.0908 - output12_loss: 0.0784 - output13_loss: 0.0841 - output14_loss: 0.0705 - output15_loss: 0.0779 - output16_loss: 0.0951 - output17_loss: 0.0810 - output18_loss: 0.0669 - output19_loss: 0.0818 - output20_loss: 0.0795 - output21_loss: 0.0606 - output22_loss: 0.0601 - output23_loss: 0.0798 - output24_loss: 0.0713 - output25_loss: 0.0826 - output26_loss: 0.3106 - output27_loss: 0.0821 - output28_loss: 0.0665 - output29_loss: 0.0861 - output30_loss: 0.0939 - output31_loss: 0.0718 - output32_loss: 0.0814 - output33_loss: 0.0642 - output34_loss: 3.6384 - output35_loss: 0.0871 - output36_loss: 0.0839 - output37_loss: 0.0768 - output38_loss: 0.0723 - output39_loss: 0.0705 - output40_loss: 0.0676 - output41_loss: 0.0776 - output42_loss: 0.0891 - output43_loss: 0.1322 - output44_loss: 0.0743 - output45_loss: 0.0712 - output46_loss: 0.0779     \n",
      "Epoch 81/100\n",
      "578/578 [==============================] - 0s - loss: 8.8316 - output1_loss: 0.0639 - output2_loss: 0.1206 - output3_loss: 0.0675 - output4_loss: 1.1411 - output5_loss: 0.1482 - output6_loss: 0.0857 - output7_loss: 0.0563 - output8_loss: 0.4070 - output9_loss: 0.0792 - output10_loss: 0.1080 - output11_loss: 0.0899 - output12_loss: 0.0756 - output13_loss: 0.0812 - output14_loss: 0.0688 - output15_loss: 0.0768 - output16_loss: 0.0929 - output17_loss: 0.0781 - output18_loss: 0.0628 - output19_loss: 0.0777 - output20_loss: 0.0768 - output21_loss: 0.0571 - output22_loss: 0.0574 - output23_loss: 0.0768 - output24_loss: 0.0698 - output25_loss: 0.0801 - output26_loss: 0.3098 - output27_loss: 0.0784 - output28_loss: 0.0633 - output29_loss: 0.0856 - output30_loss: 0.0914 - output31_loss: 0.0685 - output32_loss: 0.0774 - output33_loss: 0.0604 - output34_loss: 3.6376 - output35_loss: 0.0864 - output36_loss: 0.0839 - output37_loss: 0.0756 - output38_loss: 0.0708 - output39_loss: 0.0667 - output40_loss: 0.0639 - output41_loss: 0.0743 - output42_loss: 0.0878 - output43_loss: 0.1335 - output44_loss: 0.0738 - output45_loss: 0.0671 - output46_loss: 0.0762     \n",
      "Epoch 82/100\n",
      "578/578 [==============================] - 0s - loss: 8.9058 - output1_loss: 0.0664 - output2_loss: 0.1213 - output3_loss: 0.0668 - output4_loss: 1.1406 - output5_loss: 0.1503 - output6_loss: 0.0870 - output7_loss: 0.0588 - output8_loss: 0.4067 - output9_loss: 0.0800 - output10_loss: 0.1095 - output11_loss: 0.0899 - output12_loss: 0.0798 - output13_loss: 0.0819 - output14_loss: 0.0698 - output15_loss: 0.0781 - output16_loss: 0.0940 - output17_loss: 0.0796 - output18_loss: 0.0660 - output19_loss: 0.0826 - output20_loss: 0.0755 - output21_loss: 0.0595 - output22_loss: 0.0618 - output23_loss: 0.0790 - output24_loss: 0.0712 - output25_loss: 0.0836 - output26_loss: 0.3100 - output27_loss: 0.0796 - output28_loss: 0.0671 - output29_loss: 0.0861 - output30_loss: 0.0922 - output31_loss: 0.0732 - output32_loss: 0.0819 - output33_loss: 0.0621 - output34_loss: 3.6375 - output35_loss: 0.0874 - output36_loss: 0.0856 - output37_loss: 0.0768 - output38_loss: 0.0724 - output39_loss: 0.0683 - output40_loss: 0.0683 - output41_loss: 0.0756 - output42_loss: 0.0886 - output43_loss: 0.1326 - output44_loss: 0.0733 - output45_loss: 0.0699 - output46_loss: 0.0775     \n",
      "Epoch 83/100\n",
      "578/578 [==============================] - 0s - loss: 8.8570 - output1_loss: 0.0646 - output2_loss: 0.1225 - output3_loss: 0.0680 - output4_loss: 1.1415 - output5_loss: 0.1493 - output6_loss: 0.0849 - output7_loss: 0.0575 - output8_loss: 0.4059 - output9_loss: 0.0801 - output10_loss: 0.1072 - output11_loss: 0.0900 - output12_loss: 0.0763 - output13_loss: 0.0821 - output14_loss: 0.0684 - output15_loss: 0.0759 - output16_loss: 0.0933 - output17_loss: 0.0783 - output18_loss: 0.0660 - output19_loss: 0.0800 - output20_loss: 0.0754 - output21_loss: 0.0589 - output22_loss: 0.0588 - output23_loss: 0.0765 - output24_loss: 0.0699 - output25_loss: 0.0811 - output26_loss: 0.3095 - output27_loss: 0.0794 - output28_loss: 0.0647 - output29_loss: 0.0844 - output30_loss: 0.0914 - output31_loss: 0.0700 - output32_loss: 0.0798 - output33_loss: 0.0619 - output34_loss: 3.6369 - output35_loss: 0.0862 - output36_loss: 0.0822 - output37_loss: 0.0753 - output38_loss: 0.0707 - output39_loss: 0.0688 - output40_loss: 0.0662 - output41_loss: 0.0746 - output42_loss: 0.0890 - output43_loss: 0.1332 - output44_loss: 0.0741 - output45_loss: 0.0697 - output46_loss: 0.0767     \n",
      "Epoch 84/100\n",
      "578/578 [==============================] - 0s - loss: 8.8652 - output1_loss: 0.0644 - output2_loss: 0.1223 - output3_loss: 0.0668 - output4_loss: 1.1408 - output5_loss: 0.1511 - output6_loss: 0.0875 - output7_loss: 0.0572 - output8_loss: 0.4063 - output9_loss: 0.0792 - output10_loss: 0.1103 - output11_loss: 0.0898 - output12_loss: 0.0757 - output13_loss: 0.0819 - output14_loss: 0.0690 - output15_loss: 0.0778 - output16_loss: 0.0946 - output17_loss: 0.0785 - output18_loss: 0.0641 - output19_loss: 0.0810 - output20_loss: 0.0754 - output21_loss: 0.0570 - output22_loss: 0.0604 - output23_loss: 0.0765 - output24_loss: 0.0710 - output25_loss: 0.0814 - output26_loss: 0.3101 - output27_loss: 0.0783 - output28_loss: 0.0643 - output29_loss: 0.0867 - output30_loss: 0.0925 - output31_loss: 0.0682 - output32_loss: 0.0802 - output33_loss: 0.0618 - output34_loss: 3.6368 - output35_loss: 0.0828 - output36_loss: 0.0851 - output37_loss: 0.0766 - output38_loss: 0.0717 - output39_loss: 0.0678 - output40_loss: 0.0662 - output41_loss: 0.0750 - output42_loss: 0.0887 - output43_loss: 0.1329 - output44_loss: 0.0739 - output45_loss: 0.0702 - output46_loss: 0.0755     \n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.8550 - output1_loss: 0.0650 - output2_loss: 0.1203 - output3_loss: 0.0675 - output4_loss: 1.1406 - output5_loss: 0.1490 - output6_loss: 0.0863 - output7_loss: 0.0567 - output8_loss: 0.4051 - output9_loss: 0.0797 - output10_loss: 0.1071 - output11_loss: 0.0884 - output12_loss: 0.0780 - output13_loss: 0.0816 - output14_loss: 0.0684 - output15_loss: 0.0765 - output16_loss: 0.0920 - output17_loss: 0.0791 - output18_loss: 0.0648 - output19_loss: 0.0817 - output20_loss: 0.0782 - output21_loss: 0.0585 - output22_loss: 0.0597 - output23_loss: 0.0766 - output24_loss: 0.0685 - output25_loss: 0.0805 - output26_loss: 0.3090 - output27_loss: 0.0799 - output28_loss: 0.0655 - output29_loss: 0.0861 - output30_loss: 0.0925 - output31_loss: 0.0704 - output32_loss: 0.0811 - output33_loss: 0.0612 - output34_loss: 3.6367 - output35_loss: 0.0859 - output36_loss: 0.0832 - output37_loss: 0.0749 - output38_loss: 0.0710 - output39_loss: 0.0674 - output40_loss: 0.0641 - output41_loss: 0.0741 - output42_loss: 0.0881 - output43_loss: 0.1335 - output44_loss: 0.0750 - output45_loss: 0.0702 - output46_loss: 0.0753     \n",
      "Epoch 86/100\n",
      "578/578 [==============================] - 0s - loss: 8.9045 - output1_loss: 0.0657 - output2_loss: 0.1207 - output3_loss: 0.0681 - output4_loss: 1.1426 - output5_loss: 0.1509 - output6_loss: 0.0868 - output7_loss: 0.0581 - output8_loss: 0.4069 - output9_loss: 0.0811 - output10_loss: 0.1104 - output11_loss: 0.0900 - output12_loss: 0.0774 - output13_loss: 0.0838 - output14_loss: 0.0712 - output15_loss: 0.0778 - output16_loss: 0.0962 - output17_loss: 0.0797 - output18_loss: 0.0654 - output19_loss: 0.0815 - output20_loss: 0.0776 - output21_loss: 0.0598 - output22_loss: 0.0607 - output23_loss: 0.0756 - output24_loss: 0.0698 - output25_loss: 0.0808 - output26_loss: 0.3119 - output27_loss: 0.0805 - output28_loss: 0.0670 - output29_loss: 0.0855 - output30_loss: 0.0929 - output31_loss: 0.0704 - output32_loss: 0.0805 - output33_loss: 0.0622 - output34_loss: 3.6366 - output35_loss: 0.0864 - output36_loss: 0.0856 - output37_loss: 0.0756 - output38_loss: 0.0723 - output39_loss: 0.0688 - output40_loss: 0.0680 - output41_loss: 0.0758 - output42_loss: 0.0891 - output43_loss: 0.1339 - output44_loss: 0.0739 - output45_loss: 0.0715 - output46_loss: 0.0774     \n",
      "Epoch 87/100\n",
      "578/578 [==============================] - 0s - loss: 8.8497 - output1_loss: 0.0663 - output2_loss: 0.1219 - output3_loss: 0.0674 - output4_loss: 1.1408 - output5_loss: 0.1492 - output6_loss: 0.0874 - output7_loss: 0.0571 - output8_loss: 0.4067 - output9_loss: 0.0793 - output10_loss: 0.1097 - output11_loss: 0.0902 - output12_loss: 0.0769 - output13_loss: 0.0805 - output14_loss: 0.0691 - output15_loss: 0.0766 - output16_loss: 0.0952 - output17_loss: 0.0777 - output18_loss: 0.0639 - output19_loss: 0.0799 - output20_loss: 0.0759 - output21_loss: 0.0574 - output22_loss: 0.0603 - output23_loss: 0.0757 - output24_loss: 0.0695 - output25_loss: 0.0807 - output26_loss: 0.3079 - output27_loss: 0.0787 - output28_loss: 0.0631 - output29_loss: 0.0840 - output30_loss: 0.0919 - output31_loss: 0.0698 - output32_loss: 0.0794 - output33_loss: 0.0595 - output34_loss: 3.6380 - output35_loss: 0.0860 - output36_loss: 0.0825 - output37_loss: 0.0747 - output38_loss: 0.0718 - output39_loss: 0.0680 - output40_loss: 0.0653 - output41_loss: 0.0730 - output42_loss: 0.0895 - output43_loss: 0.1337 - output44_loss: 0.0731 - output45_loss: 0.0696 - output46_loss: 0.0752     \n",
      "Epoch 88/100\n",
      "578/578 [==============================] - 0s - loss: 8.8656 - output1_loss: 0.0636 - output2_loss: 0.1217 - output3_loss: 0.0658 - output4_loss: 1.1429 - output5_loss: 0.1497 - output6_loss: 0.0865 - output7_loss: 0.0566 - output8_loss: 0.4083 - output9_loss: 0.0805 - output10_loss: 0.1088 - output11_loss: 0.0909 - output12_loss: 0.0757 - output13_loss: 0.0825 - output14_loss: 0.0681 - output15_loss: 0.0755 - output16_loss: 0.0942 - output17_loss: 0.0788 - output18_loss: 0.0654 - output19_loss: 0.0787 - output20_loss: 0.0775 - output21_loss: 0.0588 - output22_loss: 0.0598 - output23_loss: 0.0770 - output24_loss: 0.0691 - output25_loss: 0.0817 - output26_loss: 0.3096 - output27_loss: 0.0782 - output28_loss: 0.0643 - output29_loss: 0.0853 - output30_loss: 0.0924 - output31_loss: 0.0706 - output32_loss: 0.0810 - output33_loss: 0.0606 - output34_loss: 3.6414 - output35_loss: 0.0856 - output36_loss: 0.0829 - output37_loss: 0.0754 - output38_loss: 0.0742 - output39_loss: 0.0676 - output40_loss: 0.0661 - output41_loss: 0.0732 - output42_loss: 0.0893 - output43_loss: 0.1326 - output44_loss: 0.0723 - output45_loss: 0.0682 - output46_loss: 0.0766     \n",
      "Epoch 89/100\n",
      "578/578 [==============================] - 0s - loss: 8.8233 - output1_loss: 0.0632 - output2_loss: 0.1200 - output3_loss: 0.0681 - output4_loss: 1.1403 - output5_loss: 0.1497 - output6_loss: 0.0834 - output7_loss: 0.0563 - output8_loss: 0.4064 - output9_loss: 0.0786 - output10_loss: 0.1084 - output11_loss: 0.0889 - output12_loss: 0.0771 - output13_loss: 0.0804 - output14_loss: 0.0679 - output15_loss: 0.0754 - output16_loss: 0.0924 - output17_loss: 0.0776 - output18_loss: 0.0651 - output19_loss: 0.0789 - output20_loss: 0.0770 - output21_loss: 0.0569 - output22_loss: 0.0568 - output23_loss: 0.0768 - output24_loss: 0.0679 - output25_loss: 0.0806 - output26_loss: 0.3100 - output27_loss: 0.0779 - output28_loss: 0.0621 - output29_loss: 0.0845 - output30_loss: 0.0907 - output31_loss: 0.0695 - output32_loss: 0.0795 - output33_loss: 0.0601 - output34_loss: 3.6383 - output35_loss: 0.0852 - output36_loss: 0.0846 - output37_loss: 0.0726 - output38_loss: 0.0705 - output39_loss: 0.0687 - output40_loss: 0.0650 - output41_loss: 0.0732 - output42_loss: 0.0873 - output43_loss: 0.1316 - output44_loss: 0.0746 - output45_loss: 0.0679 - output46_loss: 0.0753     \n",
      "Epoch 90/100\n",
      "578/578 [==============================] - 0s - loss: 8.8275 - output1_loss: 0.0627 - output2_loss: 0.1218 - output3_loss: 0.0657 - output4_loss: 1.1410 - output5_loss: 0.1492 - output6_loss: 0.0839 - output7_loss: 0.0566 - output8_loss: 0.4073 - output9_loss: 0.0794 - output10_loss: 0.1070 - output11_loss: 0.0894 - output12_loss: 0.0747 - output13_loss: 0.0806 - output14_loss: 0.0672 - output15_loss: 0.0744 - output16_loss: 0.0931 - output17_loss: 0.0803 - output18_loss: 0.0642 - output19_loss: 0.0804 - output20_loss: 0.0757 - output21_loss: 0.0591 - output22_loss: 0.0593 - output23_loss: 0.0758 - output24_loss: 0.0689 - output25_loss: 0.0821 - output26_loss: 0.3085 - output27_loss: 0.0772 - output28_loss: 0.0640 - output29_loss: 0.0844 - output30_loss: 0.0928 - output31_loss: 0.0697 - output32_loss: 0.0779 - output33_loss: 0.0608 - output34_loss: 3.6371 - output35_loss: 0.0865 - output36_loss: 0.0819 - output37_loss: 0.0736 - output38_loss: 0.0712 - output39_loss: 0.0672 - output40_loss: 0.0652 - output41_loss: 0.0720 - output42_loss: 0.0878 - output43_loss: 0.1319 - output44_loss: 0.0734 - output45_loss: 0.0704 - output46_loss: 0.0744     \n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.8628 - output1_loss: 0.0673 - output2_loss: 0.1202 - output3_loss: 0.0686 - output4_loss: 1.1406 - output5_loss: 0.1512 - output6_loss: 0.0869 - output7_loss: 0.0580 - output8_loss: 0.4042 - output9_loss: 0.0784 - output10_loss: 0.1079 - output11_loss: 0.0880 - output12_loss: 0.0776 - output13_loss: 0.0832 - output14_loss: 0.0696 - output15_loss: 0.0769 - output16_loss: 0.0951 - output17_loss: 0.0797 - output18_loss: 0.0660 - output19_loss: 0.0797 - output20_loss: 0.0779 - output21_loss: 0.0581 - output22_loss: 0.0604 - output23_loss: 0.0761 - output24_loss: 0.0692 - output25_loss: 0.0814 - output26_loss: 0.3085 - output27_loss: 0.0808 - output28_loss: 0.0649 - output29_loss: 0.0860 - output30_loss: 0.0930 - output31_loss: 0.0698 - output32_loss: 0.0802 - output33_loss: 0.0619 - output34_loss: 3.6359 - output35_loss: 0.0858 - output36_loss: 0.0826 - output37_loss: 0.0735 - output38_loss: 0.0726 - output39_loss: 0.0665 - output40_loss: 0.0669 - output41_loss: 0.0748 - output42_loss: 0.0883 - output43_loss: 0.1316 - output44_loss: 0.0731 - output45_loss: 0.0683 - output46_loss: 0.0757     \n",
      "Epoch 92/100\n",
      "578/578 [==============================] - 0s - loss: 8.8353 - output1_loss: 0.0633 - output2_loss: 0.1220 - output3_loss: 0.0663 - output4_loss: 1.1403 - output5_loss: 0.1484 - output6_loss: 0.0855 - output7_loss: 0.0569 - output8_loss: 0.4049 - output9_loss: 0.0775 - output10_loss: 0.1067 - output11_loss: 0.0922 - output12_loss: 0.0752 - output13_loss: 0.0804 - output14_loss: 0.0689 - output15_loss: 0.0764 - output16_loss: 0.0938 - output17_loss: 0.0781 - output18_loss: 0.0637 - output19_loss: 0.0817 - output20_loss: 0.0762 - output21_loss: 0.0571 - output22_loss: 0.0596 - output23_loss: 0.0752 - output24_loss: 0.0680 - output25_loss: 0.0818 - output26_loss: 0.3110 - output27_loss: 0.0776 - output28_loss: 0.0639 - output29_loss: 0.0850 - output30_loss: 0.0934 - output31_loss: 0.0698 - output32_loss: 0.0787 - output33_loss: 0.0623 - output34_loss: 3.6389 - output35_loss: 0.0865 - output36_loss: 0.0802 - output37_loss: 0.0743 - output38_loss: 0.0707 - output39_loss: 0.0682 - output40_loss: 0.0664 - output41_loss: 0.0727 - output42_loss: 0.0881 - output43_loss: 0.1299 - output44_loss: 0.0732 - output45_loss: 0.0688 - output46_loss: 0.0756     \n",
      "Epoch 93/100\n",
      "578/578 [==============================] - 0s - loss: 8.8386 - output1_loss: 0.0649 - output2_loss: 0.1196 - output3_loss: 0.0669 - output4_loss: 1.1405 - output5_loss: 0.1514 - output6_loss: 0.0840 - output7_loss: 0.0554 - output8_loss: 0.4072 - output9_loss: 0.0801 - output10_loss: 0.1097 - output11_loss: 0.0895 - output12_loss: 0.0757 - output13_loss: 0.0815 - output14_loss: 0.0694 - output15_loss: 0.0775 - output16_loss: 0.0920 - output17_loss: 0.0795 - output18_loss: 0.0635 - output19_loss: 0.0783 - output20_loss: 0.0756 - output21_loss: 0.0578 - output22_loss: 0.0583 - output23_loss: 0.0754 - output24_loss: 0.0675 - output25_loss: 0.0812 - output26_loss: 0.3085 - output27_loss: 0.0793 - output28_loss: 0.0632 - output29_loss: 0.0846 - output30_loss: 0.0919 - output31_loss: 0.0704 - output32_loss: 0.0800 - output33_loss: 0.0612 - output34_loss: 3.6397 - output35_loss: 0.0852 - output36_loss: 0.0826 - output37_loss: 0.0757 - output38_loss: 0.0735 - output39_loss: 0.0654 - output40_loss: 0.0648 - output41_loss: 0.0745 - output42_loss: 0.0876 - output43_loss: 0.1320 - output44_loss: 0.0726 - output45_loss: 0.0683 - output46_loss: 0.0756     \n",
      "Epoch 94/100\n",
      "578/578 [==============================] - 0s - loss: 8.8100 - output1_loss: 0.0632 - output2_loss: 0.1197 - output3_loss: 0.0653 - output4_loss: 1.1412 - output5_loss: 0.1469 - output6_loss: 0.0819 - output7_loss: 0.0562 - output8_loss: 0.4051 - output9_loss: 0.0795 - output10_loss: 0.1073 - output11_loss: 0.0888 - output12_loss: 0.0749 - output13_loss: 0.0817 - output14_loss: 0.0679 - output15_loss: 0.0745 - output16_loss: 0.0932 - output17_loss: 0.0763 - output18_loss: 0.0634 - output19_loss: 0.0786 - output20_loss: 0.0758 - output21_loss: 0.0570 - output22_loss: 0.0576 - output23_loss: 0.0755 - output24_loss: 0.0664 - output25_loss: 0.0814 - output26_loss: 0.3103 - output27_loss: 0.0788 - output28_loss: 0.0626 - output29_loss: 0.0844 - output30_loss: 0.0918 - output31_loss: 0.0694 - output32_loss: 0.0790 - output33_loss: 0.0605 - output34_loss: 3.6384 - output35_loss: 0.0860 - output36_loss: 0.0841 - output37_loss: 0.0741 - output38_loss: 0.0710 - output39_loss: 0.0668 - output40_loss: 0.0658 - output41_loss: 0.0732 - output42_loss: 0.0869 - output43_loss: 0.1311 - output44_loss: 0.0734 - output45_loss: 0.0700 - output46_loss: 0.0732     \n",
      "Epoch 95/100\n",
      "578/578 [==============================] - 0s - loss: 8.8333 - output1_loss: 0.0646 - output2_loss: 0.1206 - output3_loss: 0.0672 - output4_loss: 1.1416 - output5_loss: 0.1511 - output6_loss: 0.0857 - output7_loss: 0.0572 - output8_loss: 0.4059 - output9_loss: 0.0797 - output10_loss: 0.1079 - output11_loss: 0.0894 - output12_loss: 0.0748 - output13_loss: 0.0802 - output14_loss: 0.0670 - output15_loss: 0.0760 - output16_loss: 0.0952 - output17_loss: 0.0780 - output18_loss: 0.0641 - output19_loss: 0.0806 - output20_loss: 0.0774 - output21_loss: 0.0566 - output22_loss: 0.0597 - output23_loss: 0.0761 - output24_loss: 0.0691 - output25_loss: 0.0790 - output26_loss: 0.3095 - output27_loss: 0.0775 - output28_loss: 0.0628 - output29_loss: 0.0846 - output30_loss: 0.0927 - output31_loss: 0.0682 - output32_loss: 0.0785 - output33_loss: 0.0599 - output34_loss: 3.6374 - output35_loss: 0.0850 - output36_loss: 0.0825 - output37_loss: 0.0739 - output38_loss: 0.0715 - output39_loss: 0.0679 - output40_loss: 0.0658 - output41_loss: 0.0751 - output42_loss: 0.0886 - output43_loss: 0.1316 - output44_loss: 0.0730 - output45_loss: 0.0677 - output46_loss: 0.0751     \n",
      "Epoch 96/100\n",
      "578/578 [==============================] - 0s - loss: 8.8476 - output1_loss: 0.0651 - output2_loss: 0.1210 - output3_loss: 0.0659 - output4_loss: 1.1413 - output5_loss: 0.1513 - output6_loss: 0.0857 - output7_loss: 0.0564 - output8_loss: 0.4058 - output9_loss: 0.0782 - output10_loss: 0.1075 - output11_loss: 0.0889 - output12_loss: 0.0752 - output13_loss: 0.0804 - output14_loss: 0.0701 - output15_loss: 0.0757 - output16_loss: 0.0955 - output17_loss: 0.0779 - output18_loss: 0.0650 - output19_loss: 0.0801 - output20_loss: 0.0762 - output21_loss: 0.0582 - output22_loss: 0.0612 - output23_loss: 0.0773 - output24_loss: 0.0690 - output25_loss: 0.0798 - output26_loss: 0.3077 - output27_loss: 0.0785 - output28_loss: 0.0626 - output29_loss: 0.0848 - output30_loss: 0.0915 - output31_loss: 0.0693 - output32_loss: 0.0794 - output33_loss: 0.0622 - output34_loss: 3.6377 - output35_loss: 0.0861 - output36_loss: 0.0847 - output37_loss: 0.0747 - output38_loss: 0.0722 - output39_loss: 0.0671 - output40_loss: 0.0675 - output41_loss: 0.0745 - output42_loss: 0.0881 - output43_loss: 0.1326 - output44_loss: 0.0727 - output45_loss: 0.0697 - output46_loss: 0.0754     \n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 0s - loss: 8.8160 - output1_loss: 0.0647 - output2_loss: 0.1198 - output3_loss: 0.0671 - output4_loss: 1.1412 - output5_loss: 0.1495 - output6_loss: 0.0835 - output7_loss: 0.0554 - output8_loss: 0.4054 - output9_loss: 0.0786 - output10_loss: 0.1081 - output11_loss: 0.0895 - output12_loss: 0.0756 - output13_loss: 0.0805 - output14_loss: 0.0694 - output15_loss: 0.0758 - output16_loss: 0.0935 - output17_loss: 0.0785 - output18_loss: 0.0629 - output19_loss: 0.0802 - output20_loss: 0.0746 - output21_loss: 0.0561 - output22_loss: 0.0592 - output23_loss: 0.0741 - output24_loss: 0.0675 - output25_loss: 0.0808 - output26_loss: 0.3097 - output27_loss: 0.0794 - output28_loss: 0.0614 - output29_loss: 0.0838 - output30_loss: 0.0921 - output31_loss: 0.0679 - output32_loss: 0.0785 - output33_loss: 0.0615 - output34_loss: 3.6367 - output35_loss: 0.0849 - output36_loss: 0.0838 - output37_loss: 0.0747 - output38_loss: 0.0707 - output39_loss: 0.0641 - output40_loss: 0.0652 - output41_loss: 0.0736 - output42_loss: 0.0858 - output43_loss: 0.1325 - output44_loss: 0.0730 - output45_loss: 0.0697 - output46_loss: 0.0754     \n",
      "Epoch 98/100\n",
      "578/578 [==============================] - 0s - loss: 8.8213 - output1_loss: 0.0641 - output2_loss: 0.1206 - output3_loss: 0.0669 - output4_loss: 1.1414 - output5_loss: 0.1516 - output6_loss: 0.0834 - output7_loss: 0.0553 - output8_loss: 0.4059 - output9_loss: 0.0780 - output10_loss: 0.1087 - output11_loss: 0.0897 - output12_loss: 0.0746 - output13_loss: 0.0807 - output14_loss: 0.0678 - output15_loss: 0.0765 - output16_loss: 0.0929 - output17_loss: 0.0783 - output18_loss: 0.0633 - output19_loss: 0.0794 - output20_loss: 0.0753 - output21_loss: 0.0567 - output22_loss: 0.0589 - output23_loss: 0.0763 - output24_loss: 0.0691 - output25_loss: 0.0805 - output26_loss: 0.3065 - output27_loss: 0.0802 - output28_loss: 0.0636 - output29_loss: 0.0858 - output30_loss: 0.0909 - output31_loss: 0.0707 - output32_loss: 0.0792 - output33_loss: 0.0609 - output34_loss: 3.6389 - output35_loss: 0.0841 - output36_loss: 0.0811 - output37_loss: 0.0754 - output38_loss: 0.0689 - output39_loss: 0.0672 - output40_loss: 0.0661 - output41_loss: 0.0737 - output42_loss: 0.0872 - output43_loss: 0.1322 - output44_loss: 0.0727 - output45_loss: 0.0663 - output46_loss: 0.0740     \n",
      "Epoch 99/100\n",
      "578/578 [==============================] - 0s - loss: 8.8556 - output1_loss: 0.0652 - output2_loss: 0.1212 - output3_loss: 0.0654 - output4_loss: 1.1428 - output5_loss: 0.1516 - output6_loss: 0.0868 - output7_loss: 0.0583 - output8_loss: 0.4071 - output9_loss: 0.0803 - output10_loss: 0.1085 - output11_loss: 0.0909 - output12_loss: 0.0755 - output13_loss: 0.0818 - output14_loss: 0.0682 - output15_loss: 0.0768 - output16_loss: 0.0936 - output17_loss: 0.0776 - output18_loss: 0.0640 - output19_loss: 0.0792 - output20_loss: 0.0762 - output21_loss: 0.0591 - output22_loss: 0.0582 - output23_loss: 0.0761 - output24_loss: 0.0695 - output25_loss: 0.0812 - output26_loss: 0.3096 - output27_loss: 0.0788 - output28_loss: 0.0638 - output29_loss: 0.0841 - output30_loss: 0.0936 - output31_loss: 0.0716 - output32_loss: 0.0798 - output33_loss: 0.0603 - output34_loss: 3.6364 - output35_loss: 0.0859 - output36_loss: 0.0830 - output37_loss: 0.0741 - output38_loss: 0.0708 - output39_loss: 0.0692 - output40_loss: 0.0659 - output41_loss: 0.0732 - output42_loss: 0.0880 - output43_loss: 0.1330 - output44_loss: 0.0743 - output45_loss: 0.0696 - output46_loss: 0.0751     \n",
      "Epoch 100/100\n",
      "578/578 [==============================] - 0s - loss: 8.8550 - output1_loss: 0.0660 - output2_loss: 0.1221 - output3_loss: 0.0663 - output4_loss: 1.1412 - output5_loss: 0.1510 - output6_loss: 0.0845 - output7_loss: 0.0565 - output8_loss: 0.4052 - output9_loss: 0.0794 - output10_loss: 0.1086 - output11_loss: 0.0891 - output12_loss: 0.0761 - output13_loss: 0.0804 - output14_loss: 0.0684 - output15_loss: 0.0771 - output16_loss: 0.0921 - output17_loss: 0.0783 - output18_loss: 0.0641 - output19_loss: 0.0821 - output20_loss: 0.0792 - output21_loss: 0.0597 - output22_loss: 0.0615 - output23_loss: 0.0759 - output24_loss: 0.0668 - output25_loss: 0.0815 - output26_loss: 0.3098 - output27_loss: 0.0805 - output28_loss: 0.0646 - output29_loss: 0.0847 - output30_loss: 0.0918 - output31_loss: 0.0690 - output32_loss: 0.0800 - output33_loss: 0.0618 - output34_loss: 3.6397 - output35_loss: 0.0859 - output36_loss: 0.0833 - output37_loss: 0.0741 - output38_loss: 0.0718 - output39_loss: 0.0668 - output40_loss: 0.0661 - output41_loss: 0.0769 - output42_loss: 0.0876 - output43_loss: 0.1320 - output44_loss: 0.0735 - output45_loss: 0.0690 - output46_loss: 0.0731     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1cc4eea10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build shared layers and individual layers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# First, build the shared layers\n",
    "main_input = Input(shape=(118,), name='main_input')\n",
    "\n",
    "shared_layer1 = Dense(64, activation='relu')(main_input)\n",
    "shared_layer1 = Dropout(0.5)(shared_layer1)\n",
    "\n",
    "shared_layer2 = Dense(32, activation='relu')(shared_layer1)\n",
    "shared_layer2 = Dropout(0.3)(shared_layer2)\n",
    "    \n",
    "shared_layer = Dense(15, activation='relu')(shared_layer2)\n",
    "shared_layer = Dropout(0.3)(shared_layer)\n",
    "\n",
    "# Second, build the individual layers for each site.\n",
    "Output = []\n",
    "for i in range(1, 47):\n",
    "    layer = Dense(15, activation='relu')(shared_layer)\n",
    "    layer = Dense(1, name='output' + str(i))(layer)\n",
    "    Output.append(layer)\n",
    "    \n",
    "\n",
    "# Third, build the model for the 46 sites.\n",
    "model = Model(inputs=[main_input], outputs=Output)\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Fourth, feed the data into the model\n",
    "model.fit([np_features], target_matrix, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: We can see that for observation site 1, the error rate decrease to 6.6% in the multitask learning comparing with the single task learning.\n",
    "\n",
    "### Problem: here I have a problem for the validation. I'm not sure if the sklearn can do the multitasking cross validation part but it seems very computational expensive. Please give me some feedback on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: predict for all the 46 test observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the prediction result, I can see it has all the prediction results for the 46 observation sites.\n",
    "\n",
    "train_predict_result = model.predict(np_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: We need to preprocess the test data in order to predict on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>humidity</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.204204</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9   ...     108  109  110  111  112  113  114  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "1  0  0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "2  0  0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "3  0  0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "4  0  0  0  0  0  0  0  0  0  0   ...       0    0    0    0    0    0    0   \n",
       "\n",
       "   115  humidity  holiday  \n",
       "0    0  0.204204        0  \n",
       "1    0  0.258258        0  \n",
       "2    0  0.096096        1  \n",
       "3    0  0.333333        1  \n",
       "4    0  0.297297        0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = np.asarray(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the prediction result for the test data of all the observation sites: prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get the prediction result in the ratio format. We need to convert the ratio to the origial scale. For each site, we have the ratio base which is the first value of that site. We need to use the training prediction of the first value as the base for mapping to the real value of the first target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420.25733572994153"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the first site\n",
    "train_base_ratio = 0.22859439 \n",
    "test_base_ratio = 0.10661314\n",
    "train_base = 901.094080\n",
    "\n",
    "test_base = train_base * test_base_ratio / train_base_ratio\n",
    "test_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Convert the ratio into the normal power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "scaled_result = []\n",
    "for base, data in zip(all_data.iloc[0], prediction):\n",
    "    train_base_ratio = train_predict_result[i][0]\n",
    "    test_base_ratio = data[0]\n",
    "    train_base = base\n",
    "    test_base = train_base * test_base_ratio / train_base_ratio\n",
    "    i += 1\n",
    "    \n",
    "    site_result = [test_base]\n",
    "    j = 1\n",
    "    while j < len(data): \n",
    "        site_val = test_base * data[j] / data[j - 1]\n",
    "        site_result.append(site_val)\n",
    "        test_base = site_val\n",
    "        j += 1\n",
    "    \n",
    "    scaled_result.append(site_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>31.9</td>\n",
       "      <td>20.4</td>\n",
       "      <td>26.2375</td>\n",
       "      <td>65.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>26.2000</td>\n",
       "      <td>67.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>34.3</td>\n",
       "      <td>18.6</td>\n",
       "      <td>27.1375</td>\n",
       "      <td>61.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>25.8125</td>\n",
       "      <td>70.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>31.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>25.3500</td>\n",
       "      <td>69.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  high   low  average  humidity\n",
       "0  2016      9    1  31.9  20.4  26.2375    65.500\n",
       "1  2016      9    2  34.3  19.3  26.2000    67.750\n",
       "2  2016      9    3  34.3  18.6  27.1375    61.000\n",
       "3  2016      9    4  30.4  21.9  25.8125    70.875\n",
       "4  2016      9    5  31.9  19.4  25.3500    69.375"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the year, month, day\n",
    "ymd = read_csv('data/1_result.csv')\n",
    "ymd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day\n",
       "0  2016      9    1\n",
       "1  2016      9    2\n",
       "2  2016      9    3\n",
       "3  2016      9    4\n",
       "4  2016      9    5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ymd['high']\n",
    "del ymd['low']\n",
    "del ymd['average']\n",
    "del ymd['humidity']\n",
    "\n",
    "ymd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.3: Finally, Creat the Results Files for 46 Observation Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[0], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/1.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[1], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/2.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[2], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/3.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[3], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/4.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[4], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/7.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[5], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/8.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[6], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/11.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[7], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/12.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[8], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/14.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[9], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/15.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[10], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/16.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[11], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/17.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[12], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/19.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[13], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/20.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[14], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/22.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[15], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/23.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[16], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/29.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[17], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/33.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[18], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/36.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[19], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/37.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[20], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/38.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[21], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/39.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[22], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/40.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[23], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/41.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[24], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/42.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[25], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/43.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[26], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/45.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[27], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/47.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[28], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/48.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[29], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/49.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[30], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/50.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[31], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/51.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[32], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/52.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[33], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/53.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[34], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/54.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[35], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/55.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[36], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/60.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[37], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/62.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[38], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/63.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[39], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/64.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[40], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/69.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([ymd, DataFrame(scaled_result[41], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/71.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[42], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/72.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[43], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/73.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[44], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/74.csv', header=True, index=False)\n",
    "\n",
    "final_df = pd.concat([ymd, DataFrame(scaled_result[45], columns=['prediction'])], axis=1)\n",
    "final_df.to_csv('result/76.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
